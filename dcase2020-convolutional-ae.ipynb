{"cells":[{"metadata":{},"cell_type":"markdown","source":"# IMPORT AND DATA LOADING"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import necessari\nimport librosa\nimport numpy\nimport sys\nimport os\nimport glob\nimport itertools\nimport re\nimport pickle\nimport keras\nimport tensorflow as tf\nimport tensorflow.keras.models\nimport tensorflow.keras.backend as K\nimport keras.optimizers\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, BatchNormalization, Activation, Flatten, Multiply, Add, Reshape\nfrom tqdm import tqdm\nfrom itertools import groupby\nfrom keras.utils import to_categorical\nfrom sklearn import metrics\n\n# costanti \nALPHA = 0.75\nN_MELS = 128\nHOP_LENGTH = 512\nN_FFT = 1024\nPOWER = 2.0\nFRAME_NUMS = 313\nNUM_FILES = 3349\nFRAMES = 10\n\n# FEATURES EXTRACTION\n\n# Loading da Google Drive\ntrain_data = numpy.load(\"../input/dcase2020-slider-sequential/training_slider.npy\")\ngrouped_list_by_machine_id = pickle.load( open( \"../input/dcase2020-slider-sequential/training_slider_grouped_list.npy\", \"rb\" ) )","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# FEATURE EXTRACTION"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# GENERAZIONE DELLE LABELS\n# One-hot encoding\nlabel = []\nchoices = []\nfor i in range(0, len(grouped_list_by_machine_id)):\n  for j in range(0, len(grouped_list_by_machine_id[i])):\n    machine_id = grouped_list_by_machine_id[i][j].split('/')[7].split('_')[2]\n    #print(grouped_list_by_machine_id[i][j].split('/')[7])\n    random_choice = numpy.random.choice([\"match\", \"non_match\"], p = [ALPHA, 1-ALPHA]) \n\n    if machine_id == '00':\n      if random_choice == \"match\":\n        to_append = [1,0,0,0]\n      else: \n        not_match_label = numpy.random.choice([1, 2, 3]) \n        if not_match_label == 1:\n          to_append = [0,1,0,0]\n        elif not_match_label == 2:\n          to_append = [0,0,1,0]\n        else: \n          to_append = [0,0,0,1]\n\n    elif machine_id == '02': \n\n      if random_choice == \"match\":\n        to_append = [0,1,0,0]\n      else: \n        not_match_label = numpy.random.choice( [ 1, 2, 3] ) \n        if not_match_label == 1:\n          to_append = [1,0,0,0]\n        elif not_match_label == 2:\n          to_append = [0,0,1,0]\n        else: \n          to_append = [0,0,0,1]\n\n    elif machine_id == \"04\":\n      \n      if random_choice == \"match\":\n        to_append = [0,0,1,0]\n      else: \n        not_match_label = numpy.random.choice( [ 1, 2, 3] ) \n        if not_match_label == 1:\n          to_append = [1,0,0,0]\n        elif not_match_label == 2:\n          to_append = [0,1,0,0]\n        else: \n          to_append = [0,0,0,1]\n\n    elif machine_id == \"06\":\n      if random_choice == \"match\":\n        to_append = [0,0,0,1]\n      else: \n        not_match_label = numpy.random.choice( [ 1, 2, 3] ) \n        if not_match_label == 1:\n          to_append = [1,0,0,0]\n        elif not_match_label == 2:\n          to_append = [0,1,0,0]\n        else: \n          to_append = [0,0,1,0]\n    \n    label.append(to_append) # Append della label associata a ciascuno spettrogramma\n    choices.append(random_choice) # Append della choice utilizzata per associare la label\n                                  # La choice sarà utile in fase di addestramento per capire che tipo di loss calcolare\n\n# Trasformazione in numpy.array     \nlabel_training = numpy.asarray(label)\nchoices_training = numpy.asarray(choices)\nprint(label_training.shape)\nprint(choices_training.shape)","execution_count":2,"outputs":[{"output_type":"stream","text":"(2804, 4)\n(2804,)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Estrazione spettrogrammi divisi per ID\nid_00 = train_data[0:968]\nid_02 = train_data[968:1936]\nid_04 = train_data[1936:2370]\nid_06 = train_data[2370:2804]\n\n# Z-Score Normalization ID_00\nid_00_norm = numpy.empty_like(id_00)\nmean_00 = numpy.mean(id_00)\nstd_00 = numpy.std(id_00)\nid_00_norm = (id_00 - mean_00) / std_00\n\n# Z-Score Normalization ID_02\nid_02_norm = numpy.empty_like(id_02)\nmean_02 = numpy.mean(id_02)\nstd_02 = numpy.std(id_02)\nid_02_norm = (id_02 - mean_02) / std_02\n\n# Z-Score Normalization ID_04\nid_04_norm = numpy.empty_like(id_04)\nmean_04 = numpy.mean(id_04)\nstd_04 = numpy.std(id_04)\nid_04_norm = (id_04 - mean_04) / std_04\n\n# Z-Score Normalization ID_06\nid_06_norm = numpy.empty_like(id_06)\nmean_06 = numpy.mean(id_06)\nstd_06 = numpy.std(id_06)\nid_06_norm = (id_06 - mean_06) / std_06\n\nprint(\"Mean: {m}\".format(m=mean_00))\nprint(\"Dev.Std: {d}\".format(d=std_00))\nprint(id_00_norm.shape)\n\nprint(\"Mean: {m}\".format(m=mean_02))\nprint(\"Dev.Std: {d}\".format(d=std_02))\nprint(id_02_norm.shape)\n\nprint(\"Mean: {m}\".format(m=mean_04))\nprint(\"Dev.Std: {d}\".format(d=std_04))\nprint(id_04_norm.shape)\n\nprint(\"Mean: {m}\".format(m=mean_06))\nprint(\"Dev.Std: {d}\".format(d=std_06))\nprint(id_06_norm.shape)\n\ntrain_data_norm = numpy.concatenate([id_00_norm, id_02_norm, id_04_norm, id_06_norm])\n'''\ni=0\nwhile i<=313:\n    print(i+32)\n    i=i+12\n'''\ntraining = numpy.zeros((len(train_data_norm)*21, 128, 32)) # Dataset utilizzato per il training\nindex = 0\nfor vector_array in train_data_norm:\n  i = 0\n  while i <= 280:\n    vector_i = numpy.zeros((128,32))\n    for j in range(0,128):\n      vector_i[j] = vector_array[j][i:i+32]\n    training[index] = vector_i\n    index += 1\n    i = i+14\n    \n\n# Associazione della label associata a ciascun spettrogramma a ciascuno dei frame estratto da esso.\ntraining_labels = []\nfor elem in label_training:\n  if numpy.array_equal(elem, numpy.asarray([1,0,0,0])) :\n    for i in range(21):\n      training_labels.append([1,0,0,0])\n  elif numpy.array_equal(elem, numpy.asarray([0,1,0,0])):\n    for i in range(21):\n      training_labels.append([0,1,0,0])\n  elif numpy.array_equal(elem, numpy.asarray([0,0,1,0])):\n    for i in range(21):\n      training_labels.append([0,0,1,0])\n  elif numpy.array_equal(elem, numpy.asarray([0,0,0,1])):\n    for i in range(21):\n      training_labels.append([0,0,0,1])\n\n# Associazione della choice associata a ciascun spettrogramma a ciascuno dei frame estratto da esso. \ntraining_choices = []\nfor elem in choices_training:\n  if numpy.array_equal(elem, numpy.asarray(\"match\")) :\n    for i in range(21):\n      training_choices.append(\"match\")\n  elif numpy.array_equal(elem, numpy.asarray(\"non_match\")):\n    for i in range(21):\n      training_choices.append(\"non_match\")\n\ntraining_labels = numpy.asarray(training_labels) # Dataset utilizzato per il training\ntraining_choices = numpy.asarray(training_choices) # Dataset utilizzato per il training\n\n\n# Shuffling\nsplit_validation = int(len(train_data_norm)*29*0.1)\nsplit_train = int(len(train_data_norm)*29 - split_validation)\nprint(split_train)\nrandomize = numpy.arange(len(training))\nnumpy.random.shuffle(randomize)\ntraining_tot_shuffle = training[randomize]\ntraining_tot_labels_shuffle = training_labels[randomize]\ntraining_tot_choices_shuffle = training_choices[randomize]\n\ntraining_shuffle = training_tot_shuffle[:split_train]\nvalidation_shuffle = training_tot_shuffle[-split_validation:]\n\ntraining_labels_shuffle = training_tot_labels_shuffle[:split_train]\nvalidation_labels_shuffle = training_tot_labels_shuffle[- split_validation:]\n\ntraining_choices_shuffle = training_tot_choices_shuffle[:split_train]\nvalidation_choices_shuffle = training_tot_choices_shuffle[- split_validation:]\n\nprint(training_shuffle.shape)\nprint(training_labels_shuffle.shape)\nprint(training_choices_shuffle.shape)\nprint(validation_shuffle.shape)\nprint(validation_labels_shuffle.shape)\nprint(validation_choices_shuffle.shape)\n","execution_count":3,"outputs":[{"output_type":"stream","text":"Mean: -31.345697511820628\nDev.Std: 9.13871134460958\n(968, 128, 313)\nMean: -31.081645362049624\nDev.Std: 9.16533205673763\n(968, 128, 313)\nMean: -29.99784170308134\nDev.Std: 8.453785768550317\n(434, 128, 313)\nMean: -30.20977459826198\nDev.Std: 8.457158727189187\n(434, 128, 313)\n73185\n(58884, 128, 32)\n(58884, 4)\n(58884,)\n(8131, 128, 32)\n(8131, 4)\n(8131,)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# KERAS MODEL"},{"metadata":{"trusted":true},"cell_type":"code","source":"# KERAS MODEL\n\ninput_img = keras.Input(shape=(128, 32, 1))  # adapt this if using 'channels_first' image data format\ninput_Label = keras.Input(shape = [4,])\n\n# encoder\nx = keras.layers.Conv2D(32, (5, 5),strides=(2,1), padding='same')(input_img)   #32x128 -> 32x64\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Activation('relu')(x)\nx = keras.layers.Conv2D(64, (5, 5),strides=(2,1), padding='same')(x)           #32x32\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Activation('relu')(x)\nx = keras.layers.Conv2D(128, (5, 5),strides=(2,2), padding='same')(x)          #16x16\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Activation('relu')(x)\nx = keras.layers.Conv2D(256, (3, 3),strides=(2,2), padding='same')(x)          #8x8\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Activation('relu')(x)\nx = keras.layers.Conv2D(512, (3, 3),strides=(2,2), padding='same')(x)          #4x4\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Activation('relu')(x)\n\nvolumeSize = keras.backend.int_shape(x)\n# at this point the representation size is latentDim i.e. latentDim-dimensional\nx = keras.layers.Conv2D(40, (4,4), strides=(1,1), padding='valid')(x)\nencoded = keras.layers.Flatten()(x)\n\n# Second Branch - Conditioning Feed Forward Neural Network\nc = keras.layers.Dense(40)(input_Label)\nc = keras.layers.Activation('sigmoid')(c)\nq = keras.layers.Dense(40)(input_Label)\n\nm = keras.layers.Multiply()([c,encoded])\nencoded_input_conditioned = keras.layers.Add()([q, m]) # Input da passare al decoder\n    \n# decoder\nx = keras.layers.Dense(volumeSize[1] * volumeSize[2] * volumeSize[3])(encoded_input_conditioned) \nx = keras.layers.Reshape((volumeSize[1], volumeSize[2], 512))(x)                #4x4\n\nx = keras.layers.Conv2DTranspose(256, (3, 3),strides=(2,2), padding='same')(x)  #8x8\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Activation('relu')(x)\nx = keras.layers.Conv2DTranspose(128, (3, 3),strides=(2,2), padding='same')(x)  #16x16   \nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Activation('relu')(x)\nx = keras.layers.Conv2DTranspose(64, (5, 5),strides=(2,2), padding='same')(x)   #32x32\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Activation('relu')(x)\nx = keras.layers.Conv2DTranspose(32, (5, 5),strides=(2,1), padding='same')(x)   #32x64\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Activation('relu')(x)\n    \ndecoded = keras.layers.Conv2DTranspose(1, (5, 5),strides=(2,1), padding='same')(x) \ndecoded_reshaped = keras.layers.Reshape((1, 128, 32))(decoded)  \n\nloss_tracker = keras.metrics.Mean(name=\"loss\")\nmse_metric = keras.metrics.MeanSquaredError(name=\"mse\")\n\nclass CustomModel(keras.Model):\n    @property\n    def metrics(self):\n        return [loss_tracker, mse_metric]\n\n    def test_step(self, data):\n        # Unpack the data\n        x, y = data\n        # Compute predictions\n        y_pred = self([x[0],x[1]], training=False)\n        # Indici match\n        match = tf.where ( tf.equal(x[2][:], \"match\") )\n        # Dati match\n        data_match = K.gather(y, match)\n        # Separazione dei dati PREDETTI sulla base degli indici relativi a match/non_match\n        # Dati match\n        pred_match = K.gather(y_pred, match)\n\n        # Update metrica\n        mse_metric.update_state(data_match, pred_match)\n\n        return {\"mse\": mse_metric.result()}\n    \n    def train_step(self, data):\n          # Unpack the data. Its structure depends on your model and on what you pass to `fit()`.\n          x, y = data\n\n          # Vettore C utilizzato per il calcolo della loss in caso di non_match\n          C = 5 \n          # Valore di probabilità utilizzato come peso\n          ALPHA = 0.75 \n\n          # Indici match\n          match = tf.where ( tf.equal(x[2][:], \"match\") )\n\n          # Indici non_match\n          not_match = tf.where ( tf.equal(x[2][:], \"non_match\") )\n\n          # Dati match\n          data_match = K.gather(y, match)\n\n          with tf.GradientTape() as tape:\n              y_pred = self([x[0],x[1]], training=True)  # Forward pass\n\n              # Separazione dei dati PREDETTI sulla base degli indici relativi a match/non_match\n              # Dati match\n              pred_match = K.gather(y_pred, match)\n              # Dati non match\n              pred_not_match = K.gather(y_pred, not_match) \n\n              loss_m = K.mean(keras.losses.mean_squared_error(data_match, pred_match)) + 1e-6  # Calcolo Loss Match\n              loss_nm = K.mean(keras.losses.mean_squared_error(C,pred_not_match)) + 1e-6     # Calcolo Loss Non_Match\n\n              loss = ALPHA * loss_m + (1 - ALPHA) * loss_nm     # loss utilizzata per l'update dei pesi\n\n          # Compute gradients\n          trainable_vars = self.trainable_variables\n          gradients = tape.gradient(loss, trainable_vars)\n\n          # Update weights\n          self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n\n          # Compute our own metrics\n          loss_tracker.update_state(loss)\n          mse_metric.update_state(y, y_pred)\n          return {\"loss\": loss_tracker.result(), \"mse\": mse_metric.result()}\n\n\nautoencoder = CustomModel(inputs=(input_img, input_Label), outputs = decoded_reshaped)\n\ndef get_lr_metric(optimizer):\n    def lr(y_true, y_pred):\n        return optimizer._decayed_lr(tf.float32) # I use ._decayed_lr method instead of .lr\n    return lr\n\nopt = keras.optimizers.Adam(\n    learning_rate = 0.0001,\n    beta_1=0.95,\n    beta_2=0.999\n)\n\nlr_metric = get_lr_metric(opt)\nautoencoder.compile(optimizer = opt, metrics=[\"mse\", lr_metric])\nautoencoder.summary()\n#os.makedirs('3')\n#keras.utils.plot_model(autoencoder,to_file='./3/model.png')","execution_count":4,"outputs":[{"output_type":"stream","text":"Model: \"custom_model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 128, 32, 1)] 0                                            \n__________________________________________________________________________________________________\nconv2d (Conv2D)                 (None, 64, 32, 32)   832         input_1[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization (BatchNorma (None, 64, 32, 32)   128         conv2d[0][0]                     \n__________________________________________________________________________________________________\nactivation (Activation)         (None, 64, 32, 32)   0           batch_normalization[0][0]        \n__________________________________________________________________________________________________\nconv2d_1 (Conv2D)               (None, 32, 32, 64)   51264       activation[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_1 (BatchNor (None, 32, 32, 64)   256         conv2d_1[0][0]                   \n__________________________________________________________________________________________________\nactivation_1 (Activation)       (None, 32, 32, 64)   0           batch_normalization_1[0][0]      \n__________________________________________________________________________________________________\nconv2d_2 (Conv2D)               (None, 16, 16, 128)  204928      activation_1[0][0]               \n__________________________________________________________________________________________________\nbatch_normalization_2 (BatchNor (None, 16, 16, 128)  512         conv2d_2[0][0]                   \n__________________________________________________________________________________________________\nactivation_2 (Activation)       (None, 16, 16, 128)  0           batch_normalization_2[0][0]      \n__________________________________________________________________________________________________\nconv2d_3 (Conv2D)               (None, 8, 8, 256)    295168      activation_2[0][0]               \n__________________________________________________________________________________________________\nbatch_normalization_3 (BatchNor (None, 8, 8, 256)    1024        conv2d_3[0][0]                   \n__________________________________________________________________________________________________\nactivation_3 (Activation)       (None, 8, 8, 256)    0           batch_normalization_3[0][0]      \n__________________________________________________________________________________________________\nconv2d_4 (Conv2D)               (None, 4, 4, 512)    1180160     activation_3[0][0]               \n__________________________________________________________________________________________________\nbatch_normalization_4 (BatchNor (None, 4, 4, 512)    2048        conv2d_4[0][0]                   \n__________________________________________________________________________________________________\ninput_2 (InputLayer)            [(None, 4)]          0                                            \n__________________________________________________________________________________________________\nactivation_4 (Activation)       (None, 4, 4, 512)    0           batch_normalization_4[0][0]      \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 40)           200         input_2[0][0]                    \n__________________________________________________________________________________________________\nconv2d_5 (Conv2D)               (None, 1, 1, 40)     327720      activation_4[0][0]               \n__________________________________________________________________________________________________\nactivation_5 (Activation)       (None, 40)           0           dense[0][0]                      \n__________________________________________________________________________________________________\nflatten (Flatten)               (None, 40)           0           conv2d_5[0][0]                   \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 40)           200         input_2[0][0]                    \n__________________________________________________________________________________________________\nmultiply (Multiply)             (None, 40)           0           activation_5[0][0]               \n                                                                 flatten[0][0]                    \n__________________________________________________________________________________________________\nadd (Add)                       (None, 40)           0           dense_1[0][0]                    \n                                                                 multiply[0][0]                   \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 8192)         335872      add[0][0]                        \n__________________________________________________________________________________________________\nreshape (Reshape)               (None, 4, 4, 512)    0           dense_2[0][0]                    \n__________________________________________________________________________________________________\nconv2d_transpose (Conv2DTranspo (None, 8, 8, 256)    1179904     reshape[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization_5 (BatchNor (None, 8, 8, 256)    1024        conv2d_transpose[0][0]           \n__________________________________________________________________________________________________\nactivation_6 (Activation)       (None, 8, 8, 256)    0           batch_normalization_5[0][0]      \n__________________________________________________________________________________________________\nconv2d_transpose_1 (Conv2DTrans (None, 16, 16, 128)  295040      activation_6[0][0]               \n__________________________________________________________________________________________________\nbatch_normalization_6 (BatchNor (None, 16, 16, 128)  512         conv2d_transpose_1[0][0]         \n__________________________________________________________________________________________________\nactivation_7 (Activation)       (None, 16, 16, 128)  0           batch_normalization_6[0][0]      \n__________________________________________________________________________________________________\nconv2d_transpose_2 (Conv2DTrans (None, 32, 32, 64)   204864      activation_7[0][0]               \n__________________________________________________________________________________________________\nbatch_normalization_7 (BatchNor (None, 32, 32, 64)   256         conv2d_transpose_2[0][0]         \n__________________________________________________________________________________________________\nactivation_8 (Activation)       (None, 32, 32, 64)   0           batch_normalization_7[0][0]      \n__________________________________________________________________________________________________\nconv2d_transpose_3 (Conv2DTrans (None, 64, 32, 32)   51232       activation_8[0][0]               \n__________________________________________________________________________________________________\nbatch_normalization_8 (BatchNor (None, 64, 32, 32)   128         conv2d_transpose_3[0][0]         \n__________________________________________________________________________________________________\nactivation_9 (Activation)       (None, 64, 32, 32)   0           batch_normalization_8[0][0]      \n__________________________________________________________________________________________________\nconv2d_transpose_4 (Conv2DTrans (None, 128, 32, 1)   801         activation_9[0][0]               \n__________________________________________________________________________________________________\nreshape_1 (Reshape)             (None, 1, 128, 32)   0           conv2d_transpose_4[0][0]         \n==================================================================================================\nTotal params: 4,134,073\nTrainable params: 4,131,129\nNon-trainable params: 2,944\n__________________________________________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = autoencoder.fit([training_shuffle, training_labels_shuffle, training_choices_shuffle], \n                          training_shuffle, \n                          epochs=100,\n                          batch_size=64, \n                          validation_data=([validation_shuffle, validation_labels_shuffle, validation_choices_shuffle], validation_shuffle), shuffle=True)\n","execution_count":5,"outputs":[{"output_type":"stream","text":"Epoch 1/100\n921/921 [==============================] - 41s 41ms/step - loss: nan - mse: 2.6015 - val_mse: 0.5700\nEpoch 2/100\n921/921 [==============================] - 36s 39ms/step - loss: nan - mse: 4.9072 - val_mse: 0.5794\nEpoch 3/100\n921/921 [==============================] - 36s 40ms/step - loss: 0.9345 - mse: 5.7747 - val_mse: 0.5688\nEpoch 4/100\n921/921 [==============================] - 36s 39ms/step - loss: nan - mse: 6.0409 - val_mse: 0.4127\nEpoch 5/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.5059 - mse: 6.1676 - val_mse: 0.4055\nEpoch 6/100\n921/921 [==============================] - 36s 40ms/step - loss: 0.4270 - mse: 6.2396 - val_mse: 0.4211\nEpoch 7/100\n921/921 [==============================] - 37s 40ms/step - loss: 0.3783 - mse: 6.3180 - val_mse: 0.3833\nEpoch 8/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.3508 - mse: 6.3801 - val_mse: 0.3802\nEpoch 9/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.3527 - mse: 6.3860 - val_mse: 0.3716\nEpoch 10/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.3209 - mse: 6.4672 - val_mse: 0.3744\nEpoch 11/100\n921/921 [==============================] - 36s 39ms/step - loss: nan - mse: 6.4795 - val_mse: 0.3672\nEpoch 12/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.3050 - mse: 6.4983 - val_mse: 0.3666\nEpoch 13/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.3043 - mse: 6.5181 - val_mse: 0.3662\nEpoch 14/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.3026 - mse: 6.5301 - val_mse: 0.3744\nEpoch 15/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2964 - mse: 6.5331 - val_mse: 0.3752\nEpoch 16/100\n921/921 [==============================] - 36s 39ms/step - loss: nan - mse: 6.5456 - val_mse: 0.3732\nEpoch 17/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2922 - mse: 6.5537 - val_mse: 0.3735\nEpoch 18/100\n921/921 [==============================] - 36s 40ms/step - loss: 0.2905 - mse: 6.5570 - val_mse: 0.3705\nEpoch 19/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2872 - mse: 6.5674 - val_mse: 0.3683\nEpoch 20/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2868 - mse: 6.5747 - val_mse: 0.3677\nEpoch 21/100\n921/921 [==============================] - 36s 39ms/step - loss: nan - mse: 6.5690 - val_mse: 0.3720\nEpoch 22/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2856 - mse: 6.5793 - val_mse: 0.3747\nEpoch 23/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2846 - mse: 6.5819 - val_mse: 0.3673\nEpoch 24/100\n921/921 [==============================] - 36s 39ms/step - loss: nan - mse: 6.5833 - val_mse: 0.3700\nEpoch 25/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2878 - mse: 6.5809 - val_mse: 0.3670\nEpoch 26/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2810 - mse: 6.5933 - val_mse: 0.3666\nEpoch 27/100\n921/921 [==============================] - 36s 39ms/step - loss: nan - mse: 6.6024 - val_mse: 0.3635\nEpoch 28/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2818 - mse: 6.5876 - val_mse: 0.3632\nEpoch 29/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2795 - mse: 6.5996 - val_mse: 0.3626\nEpoch 30/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2783 - mse: 6.6067 - val_mse: 0.3625\nEpoch 31/100\n921/921 [==============================] - 36s 39ms/step - loss: nan - mse: 6.5991 - val_mse: 0.3661\nEpoch 32/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2807 - mse: 6.5962 - val_mse: 0.3661\nEpoch 33/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2795 - mse: 6.6031 - val_mse: 0.3626\nEpoch 34/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2843 - mse: 6.6014 - val_mse: 0.3625\nEpoch 35/100\n921/921 [==============================] - 36s 39ms/step - loss: nan - mse: 6.6124 - val_mse: 0.3626\nEpoch 36/100\n921/921 [==============================] - 36s 40ms/step - loss: 0.2793 - mse: 6.5978 - val_mse: 0.3628\nEpoch 37/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2771 - mse: 6.6119 - val_mse: 0.3625\nEpoch 38/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2804 - mse: 6.6044 - val_mse: 0.3629\nEpoch 39/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2780 - mse: 6.6101 - val_mse: 0.3624\nEpoch 40/100\n921/921 [==============================] - 36s 40ms/step - loss: 0.2763 - mse: 6.6155 - val_mse: 0.3625\nEpoch 41/100\n921/921 [==============================] - 36s 39ms/step - loss: nan - mse: 6.6104 - val_mse: 0.3623\nEpoch 42/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2799 - mse: 6.6067 - val_mse: 0.3623\nEpoch 43/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2776 - mse: 6.6118 - val_mse: 0.3624\nEpoch 44/100\n921/921 [==============================] - 36s 39ms/step - loss: nan - mse: 6.6168 - val_mse: 0.3591\nEpoch 45/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2782 - mse: 6.6033 - val_mse: 0.3582\nEpoch 46/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2756 - mse: 6.6139 - val_mse: 0.3576\nEpoch 47/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2776 - mse: 6.6169 - val_mse: 0.3587\nEpoch 48/100\n921/921 [==============================] - 36s 39ms/step - loss: nan - mse: 6.6083 - val_mse: 0.3577\nEpoch 49/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2759 - mse: 6.6170 - val_mse: 0.3577\nEpoch 50/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2783 - mse: 6.6110 - val_mse: 0.3580\nEpoch 51/100\n921/921 [==============================] - 36s 39ms/step - loss: nan - mse: 6.6198 - val_mse: 0.3576\nEpoch 52/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2753 - mse: 6.6163 - val_mse: 0.3578\nEpoch 53/100\n921/921 [==============================] - 36s 39ms/step - loss: nan - mse: 6.6186 - val_mse: 0.3576\nEpoch 54/100\n921/921 [==============================] - 36s 39ms/step - loss: nan - mse: 6.6122 - val_mse: 0.3616\nEpoch 55/100\n921/921 [==============================] - 36s 39ms/step - loss: nan - mse: 6.6175 - val_mse: 0.3612\nEpoch 56/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2753 - mse: 6.6167 - val_mse: 0.3577\nEpoch 57/100\n921/921 [==============================] - 36s 40ms/step - loss: 0.2745 - mse: 6.6178 - val_mse: 0.3575\nEpoch 58/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2745 - mse: 6.6203 - val_mse: 0.3577\nEpoch 59/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2813 - mse: 6.6120 - val_mse: 0.3582\nEpoch 60/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2760 - mse: 6.6195 - val_mse: 0.3575\nEpoch 61/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2741 - mse: 6.6218 - val_mse: 0.3577\nEpoch 62/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2743 - mse: 6.6231 - val_mse: 0.3579\nEpoch 63/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2751 - mse: 6.6233 - val_mse: 0.3575\nEpoch 64/100\n921/921 [==============================] - 36s 39ms/step - loss: nan - mse: 6.6223 - val_mse: 0.3618\nEpoch 65/100\n921/921 [==============================] - 36s 39ms/step - loss: nan - mse: 6.6200 - val_mse: 0.3576\nEpoch 66/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2739 - mse: 6.6195 - val_mse: 0.3574\nEpoch 67/100\n921/921 [==============================] - 36s 39ms/step - loss: nan - mse: 6.6210 - val_mse: 0.3575\nEpoch 68/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2745 - mse: 6.6185 - val_mse: 0.3591\nEpoch 69/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2753 - mse: 6.6200 - val_mse: 0.3585\nEpoch 70/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2772 - mse: 6.6151 - val_mse: 0.3577\nEpoch 71/100\n","name":"stdout"},{"output_type":"stream","text":"921/921 [==============================] - 36s 39ms/step - loss: 0.2740 - mse: 6.6240 - val_mse: 0.3574\nEpoch 72/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2764 - mse: 6.6194 - val_mse: 0.3574\nEpoch 73/100\n921/921 [==============================] - 36s 39ms/step - loss: nan - mse: 6.6208 - val_mse: 0.3574\nEpoch 74/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2740 - mse: 6.6215 - val_mse: 0.3579\nEpoch 75/100\n921/921 [==============================] - 36s 39ms/step - loss: nan - mse: 6.6225 - val_mse: 0.3575\nEpoch 76/100\n921/921 [==============================] - 36s 39ms/step - loss: nan - mse: 6.6209 - val_mse: 0.3574\nEpoch 77/100\n921/921 [==============================] - 36s 39ms/step - loss: nan - mse: 6.6242 - val_mse: 0.3578\nEpoch 78/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2751 - mse: 6.6201 - val_mse: 0.3585\nEpoch 79/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2737 - mse: 6.6225 - val_mse: 0.3575\nEpoch 80/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2735 - mse: 6.6195 - val_mse: 0.3575\nEpoch 81/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2733 - mse: 6.6264 - val_mse: 0.3578\nEpoch 82/100\n921/921 [==============================] - 36s 39ms/step - loss: nan - mse: 6.6203 - val_mse: 0.3575\nEpoch 83/100\n921/921 [==============================] - 37s 40ms/step - loss: 0.2738 - mse: 6.6231 - val_mse: 0.3575\nEpoch 84/100\n921/921 [==============================] - 36s 40ms/step - loss: nan - mse: 6.6233 - val_mse: 0.3575\nEpoch 85/100\n921/921 [==============================] - 36s 39ms/step - loss: nan - mse: 6.6215 - val_mse: 0.3574\nEpoch 86/100\n921/921 [==============================] - 36s 40ms/step - loss: 0.2749 - mse: 6.6237 - val_mse: 0.3594\nEpoch 87/100\n921/921 [==============================] - 37s 40ms/step - loss: nan - mse: 6.6200 - val_mse: 0.3573\nEpoch 88/100\n921/921 [==============================] - 36s 39ms/step - loss: nan - mse: 6.6240 - val_mse: 0.3577\nEpoch 89/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2728 - mse: 6.6248 - val_mse: 0.3573\nEpoch 90/100\n921/921 [==============================] - 36s 39ms/step - loss: nan - mse: 6.6271 - val_mse: 0.3573\nEpoch 91/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2737 - mse: 6.6231 - val_mse: 0.3574\nEpoch 92/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2731 - mse: 6.6272 - val_mse: 0.3576\nEpoch 93/100\n921/921 [==============================] - 36s 39ms/step - loss: nan - mse: 6.6268 - val_mse: 0.3576\nEpoch 94/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2737 - mse: 6.6226 - val_mse: 0.3573\nEpoch 95/100\n921/921 [==============================] - 36s 39ms/step - loss: nan - mse: 6.6253 - val_mse: 0.3576\nEpoch 96/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2764 - mse: 6.6188 - val_mse: 0.3575\nEpoch 97/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2747 - mse: 6.6290 - val_mse: 0.3578\nEpoch 98/100\n921/921 [==============================] - 36s 39ms/step - loss: nan - mse: 6.6241 - val_mse: 0.3581\nEpoch 99/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2751 - mse: 6.6217 - val_mse: 0.3574\nEpoch 100/100\n921/921 [==============================] - 36s 39ms/step - loss: 0.2735 - mse: 6.6249 - val_mse: 0.3574\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.makedirs('5')\nautoencoder.save('./5/model_slider_IDCCAE_BS64_E100_LR00001_DATAOVLP14.h5')","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TEST"},{"metadata":{"trusted":true},"cell_type":"code","source":"import csv\n\ndef save_csv(save_file_path,\n             save_data):\n    with open(save_file_path, \"w\", newline=\"\") as f:\n        writer = csv.writer(f, lineterminator='\\n')\n        writer.writerows(save_data)\n\n\n# load dataset\ndef select_dirs(path):\n    dir_path = os.path.abspath(path)\n    dirs = sorted(glob.glob(dir_path))\n    return dirs\n\ndef file_load(wav_name, mono=False):\n    try:\n        return librosa.load(wav_name, sr=None, mono=mono)\n    except:\n        logger.error(\"file_broken or not exists!! : {}\".format(wav_name))\n\ndef file_list_generator(target_dir, dir_name=\"train\", ext=\"wav\"):\n    print(\"target_dir : {}\".format(target_dir))\n\n    # generate training list\n    training_list_path = os.path.abspath(\"{dir}/{dir_name}/*.{ext}\".format(dir=target_dir, dir_name=dir_name, ext=ext))\n    files = sorted(glob.glob(training_list_path))\n    if len(files) == 0:\n      print(\"errore\")\n    return files\n\n\ndef file_to_vector_array(file_name, n_mels=64, n_fft=1024, hop_length=512, power=2.0):\n    # 02 generate melspectrogram using librosa\n    y, sr = file_load(file_name)\n    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, power=power)\n\n    # 03 convert melspectrogram to log mel energy\n    log_mel_spectrogram = 20.0 / power * numpy.log10(mel_spectrogram + sys.float_info.epsilon)\n\n    return log_mel_spectrogram\n\n  \ndef list_to_vector_array(file_list, msg=\"calc...\", n_mels=64, n_fft=1024, hop_length=512, power=2.0, frames=10):\n    # iterate file_to_vector_array()\n    for idx in tqdm(range(len(file_list)), desc=msg):\n        vector_array = file_to_vector_array(file_list[idx], n_mels=n_mels, n_fft=n_fft, hop_length=hop_length, power=power)\n\n        if idx == 0:\n            dataset = numpy.zeros((len(file_list), n_mels, frames), float)\n        dataset[vector_array.shape[0] * idx: vector_array.shape[0] * (idx + 1), :] = vector_array\n    return dataset\n\ndef key_by_id(item):\n  path_splitted = item.split(\"/\")\n  file_name = path_splitted[ len(path_splitted) - 1 ]\n  file_name_splitted = file_name.split(\"_\")\n  machine_id = file_name_splitted = file_name_splitted[2]\n  return machine_id\n\ndef get_machine_id_list_for_test(target_dir,\n                                 dir_name=\"test\",\n                                 ext=\"wav\"):\n\n    # create test files\n    dir_path = os.path.abspath(\"{dir}/{dir_name}/*.{ext}\".format(dir=target_dir, dir_name=dir_name, ext=ext))\n    file_paths = sorted(glob.glob(dir_path))\n    # extract id\n    machine_id_list = sorted(list(set(itertools.chain.from_iterable(\n        [re.findall('id_[0-9][0-9]', ext_id) for ext_id in file_paths]))))\n    return machine_id_list\n\ndef test_file_list_generator(target_dir,\n                             id_name,\n                             dir_name=\"test\",\n                             prefix_normal=\"normal\",\n                             prefix_anomaly=\"anomaly\",\n                             ext=\"wav\"):\n  \n    print(\"target_dir : {}\".format(target_dir+\"_\"+id_name))\n\n    normal_files = sorted(\n    glob.glob(\"{dir}/{dir_name}/{prefix_normal}_{id_name}*.{ext}\".format(dir=target_dir,\n                                                                                 dir_name=dir_name,\n                                                                                 prefix_normal=prefix_normal,\n                                                                                 id_name=id_name,\n                                                                                 ext=ext)))\n    normal_labels = numpy.zeros(len(normal_files))\n    anomaly_files = sorted(\n    glob.glob(\"{dir}/{dir_name}/{prefix_anomaly}_{id_name}*.{ext}\".format(dir=target_dir,\n                                                                                  dir_name=dir_name,\n                                                                                  prefix_anomaly=prefix_anomaly,\n                                                                                  id_name=id_name,\n                                                                                  ext=ext)))\n    anomaly_labels = numpy.ones(len(anomaly_files))\n    files = numpy.concatenate((normal_files, anomaly_files), axis=0)\n    labels = numpy.concatenate((normal_labels, anomaly_labels), axis=0)\n    print(\"test_file  num : {num}\".format(num=len(files)))\n    if len(files) == 0:\n        print(\"no_wav_file!!\")\n    print(\"\\n========================================\")\n\n    return files, labels","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_dir = \"../input/dc2020task2/slider\"\n\nmachine_type = os.path.split(target_dir)[1]\nprint(\"============== MODEL LOAD ==============\")\n# set model path\nmodel_file = \"./5/model_slider_IDCCAE_BS64_E100_LR00001_DATAOVLP14.h5\"\n\n# load model file\nif not os.path.exists(model_file):\n  print(\"{} model not found \".format(machine_type))\n  sys.exit(-1)\nmodel = keras.models.load_model(model_file, custom_objects={'CustomModel': CustomModel, 'mse':mse_metric, 'lr': lr_metric})\n# model.summary()\n\nmachine_id_list = get_machine_id_list_for_test(target_dir)\n\n# initialize lines in csv for AUC and pAUC\ncsv_lines = []\n\ncsv_lines.append([machine_type])\ncsv_lines.append([\"id\", \"AUC\", \"pAUC\"])\nperformance = []\n\nfor id_str in machine_id_list:\n  # load test file\n\n  id_num = id_str.split(\"_\")[1]\n\n  # Definizione della label \"match\" da utilizzare in fase di testing e del min e max da utilizzare per la normalizzazione\n  # i min e max sono stati calcolati a partire dai dati di training.\n  if id_num == \"00\":\n    match_labels = numpy.asarray([1,0,0,0])\n    mean = mean_00\n    std = std_00\n  if id_num == \"02\":\n    match_labels = numpy.asarray([0,1,0,0])\n    mean = mean_02\n    std = std_02\n  if id_num == \"04\":\n    match_labels = numpy.asarray([0,0,1,0])\n    mean = mean_04\n    std = std_04\n  if id_num == \"06\":\n    match_labels = numpy.asarray([0,0,0,1])\n    mean = mean_06\n    std = std_06\n\n  test_files, y_true = test_file_list_generator(target_dir, id_str)\n  #print(\"\\n====== True Labels ======\")\n  #print(y_true)\n  #print(\"==> ====== Match ID Labels ======\")\n  #print(match_labels.shape)\n  #print(\"=================================\\n\")\n\n  # setup anomaly score file path\n  anomaly_score_csv = \"./4/anomaly_score_{machine_type}_{id_str}.csv\".format(machine_type=machine_type, id_str=id_str)\n  anomaly_score_list = []\n\n  print(\"\\n============== BEGIN TEST FOR A MACHINE ID {id} ==============\".format(id=id_num))\n\n  y_pred = [0. for k in test_files]\n\n\n  for file_idx, file_path in tqdm(enumerate(test_files), total=len(test_files)):\n\n    # Estrazione spettrogramma audio test\n    data = file_to_vector_array(file_path, n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH, power=POWER)\n\n    # Normalizzazione spettrogramma di test\n    data = ( data - mean ) / std\n\n    # Estrazione delle frame 128x32\n    data_splitted = numpy.zeros((21, 128, 32))\n    index = 0\n    i = 0\n    while i <= 280:\n      vector_i = numpy.zeros((128,32))\n      for j in range(0,128):\n        vector_i[j] = data[j][i:i+32]\n      data_splitted[index] = vector_i\n      index += 1\n      i = i+14\n\n    # Calcolo dell'errore medio sulle frame estratte dallo spettrogramma\n    elem_error = []\n    for elem in data_splitted:\n      predicted = model.predict([elem.reshape(1,128,32), match_labels.reshape((1,4))])\n      errors = numpy.mean(numpy.square(elem - predicted.reshape(1,128,32)), axis=1)\n      elem_error.append(numpy.mean(errors))\n\n    # Log dell'errore associato all'istanza di test\n    y_pred[file_idx] = numpy.mean(elem_error)\n    anomaly_score_list.append([os.path.basename(file_path), y_pred[file_idx]])\n  \n\n  save_csv(save_file_path=anomaly_score_csv, save_data=anomaly_score_list)\n    \n  # Calcolo AUC e pAUC per i dati con un certo ID_0x\n  auc = metrics.roc_auc_score(y_true,y_pred)\n  p_auc = metrics.roc_auc_score(y_true, y_pred, max_fpr=0.1)\n  csv_lines.append([id_str.split(\"_\", 1)[1], auc, p_auc])\n  performance.append([auc, p_auc])\n  print(\"AUC : {}\".format(auc))\n  print(\"pAUC : {}\".format(p_auc))\n\n  print(\"\\n============ END OF TEST FOR A MACHINE ID ============\")\n\n# Stampa di AUC e pAUC medi su tutti i dati di test (media di AUC e pAUC sui vari ID).\nprint(\"\\n============ AVERAGE PERFORMANCES ============\")\naveraged_performance = numpy.mean(numpy.array(performance, dtype=float), axis=0)\ncsv_lines.append([\"Average\"] + list(averaged_performance))\ncsv_lines.append([])\nprint(averaged_performance)\n\nresult_path = \"./5/anomaly_score_avg_IDCCAE_BS64_E100_LR00001_DATAOVLP14.csv\"\nsave_csv(save_file_path=result_path, save_data=csv_lines)","execution_count":8,"outputs":[{"output_type":"stream","text":"============== MODEL LOAD ==============\n","name":"stdout"},{"output_type":"stream","text":"\r  0%|          | 0/456 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"target_dir : ../input/dc2020task2/slider_id_00\ntest_file  num : 456\n\n========================================\n\n============== BEGIN TEST FOR A MACHINE ID 00 ==============\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 456/456 [07:19<00:00,  1.04it/s]\n  0%|          | 0/367 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"AUC : 0.9910393258426966\npAUC : 0.9625960969840331\n\n============ END OF TEST FOR A MACHINE ID ============\ntarget_dir : ../input/dc2020task2/slider_id_02\ntest_file  num : 367\n\n========================================\n\n============== BEGIN TEST FOR A MACHINE ID 02 ==============\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 367/367 [05:56<00:00,  1.03it/s]\n  0%|          | 0/278 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"AUC : 0.8593258426966291\npAUC : 0.7685787502464025\n\n============ END OF TEST FOR A MACHINE ID ============\ntarget_dir : ../input/dc2020task2/slider_id_04\ntest_file  num : 278\n\n========================================\n\n============== BEGIN TEST FOR A MACHINE ID 04 ==============\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 278/278 [04:28<00:00,  1.03it/s]\n  0%|          | 0/189 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"AUC : 0.9994382022471909\npAUC : 0.997043169722058\n\n============ END OF TEST FOR A MACHINE ID ============\ntarget_dir : ../input/dc2020task2/slider_id_06\ntest_file  num : 189\n\n========================================\n\n============== BEGIN TEST FOR A MACHINE ID 06 ==============\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 189/189 [03:03<00:00,  1.03it/s]","name":"stderr"},{"output_type":"stream","text":"AUC : 0.5901123595505618\npAUC : 0.5257244234180958\n\n============ END OF TEST FOR A MACHINE ID ============\n\n============ AVERAGE PERFORMANCES ============\n[0.85997893 0.81348561]\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('./5/trainHistoryDict', 'wb') as file_pi:\n    pickle.dump(history.history, file_pi)","execution_count":9,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}