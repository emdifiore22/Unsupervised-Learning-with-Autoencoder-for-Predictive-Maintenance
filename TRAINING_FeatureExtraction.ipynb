{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TRAINING - FeatureExtraction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1NauUJXYEZ_-2Zjk8GiRuf5iuRc3FpVd-",
      "authorship_tag": "ABX9TyMElJ0eW0Q17v97AepCfHCs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emdifiore22/Unsupervised-Learning-with-Autoencoder-for-Predictive-Maintenance/blob/main/TRAINING_FeatureExtraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zts-18oLF6LW"
      },
      "source": [
        "# IMPORT NECESSARI\n",
        "import librosa\n",
        "import numpy\n",
        "import sys\n",
        "import os\n",
        "import glob\n",
        "import tensorflow.keras.models\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Activation, Flatten, Multiply, Add, Reshape\n",
        "from tqdm import tqdm\n",
        "from itertools import groupby\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "ALPHA = 0.75\n",
        "N_MELS = 128\n",
        "HOP_LENGTH = 512\n",
        "N_FFT = 1024\n",
        "POWER = 2.0\n",
        "FRAME_NUMS = 313\n",
        "NUM_FILES = 3349\n",
        "\n",
        "# load dataset\n",
        "def select_dirs(path):\n",
        "    dir_path = os.path.abspath(path)\n",
        "    dirs = sorted(glob.glob(dir_path))\n",
        "    return dirs\n",
        "\n",
        "def file_load(wav_name, mono=False):\n",
        "    try:\n",
        "        return librosa.load(wav_name, sr=None, mono=mono)\n",
        "    except:\n",
        "        logger.error(\"file_broken or not exists!! : {}\".format(wav_name))\n",
        "\n",
        "def file_list_generator(target_dir, dir_name=\"train\", ext=\"wav\"):\n",
        "   \n",
        "    print(\"target_dir : {}\".format(target_dir))\n",
        "    # generate training list\n",
        "    training_list_path = os.path.abspath(\"{dir}/{dir_name}/*.{ext}\".format(dir=target_dir, dir_name=dir_name, ext=ext))\n",
        "    files = sorted(glob.glob(training_list_path))\n",
        "    if len(files) == 0:\n",
        "      print(\"errore\")\n",
        "    return files\n",
        "\n",
        "def file_to_vector_array(file_name, n_mels=64, n_fft=1024, hop_length=512, power=2.0):\n",
        "    # 01 generate melspectrogram using librosa\n",
        "    y, sr = file_load(file_name)\n",
        "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, power=power)\n",
        "    # 02 convert melspectrogram to log mel energy\n",
        "    log_mel_spectrogram = 20.0 / power * numpy.log10(mel_spectrogram + sys.float_info.epsilon)\n",
        "\n",
        "    return log_mel_spectrogram\n",
        "\n",
        "  \n",
        "def list_to_vector_array(file_list, msg=\"calc...\", n_mels=64, n_fft=1024, hop_length=512, power=2.0):\n",
        "    # iterate file_to_vector_array()\n",
        "    for idx in tqdm(range(len(file_list)), desc=msg):\n",
        "        vector_array = file_to_vector_array(file_list[idx], n_mels=n_mels, n_fft=n_fft, hop_length=hop_length, power=power)\n",
        "        if idx == 0:\n",
        "            dataset = numpy.zeros((len(file_list), N_MELS, FRAME_NUMS), float)\n",
        "        dataset[vector_array.shape[0] * idx: vector_array.shape[0] * (idx + 1), :] = vector_array\n",
        "    return dataset\n",
        "\n",
        "def key_by_id(item):\n",
        "  path_splitted = item.split(\"/\")\n",
        "  file_name = path_splitted[ len(path_splitted) - 1 ]\n",
        "  file_name_splitted = file_name.split(\"_\")\n",
        "  machine_id = file_name_splitted = file_name_splitted[2]\n",
        "  return machine_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEzeFuLRFmAF",
        "outputId": "fb737ba2-0207-42a4-a007-9563fc26f5ed"
      },
      "source": [
        "# load base_directory list\n",
        "dirs = \"/content/drive/MyDrive/dcase2020_task2_baseline/dev_data/pump\"\n",
        "files = file_list_generator(dirs)\n",
        "train_data = list_to_vector_array(files, msg=\"generate train_dataset\", n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH, power=POWER)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "target_dir : /content/drive/MyDrive/dcase2020_task2_baseline/dev_data/pump\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "generate train_dataset: 100%|██████████| 3349/3349 [27:33<00:00,  2.03it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}