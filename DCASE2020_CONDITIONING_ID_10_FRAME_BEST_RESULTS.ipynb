{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCASE2020_CONDITIONING_ID_10_FRAME_BEST_RESULTS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emdifiore22/Unsupervised-Learning-with-Autoencoder-for-Predictive-Maintenance/blob/main/DCASE2020_CONDITIONING_ID_10_FRAME_BEST_RESULTS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kq5KmwSlEv8v"
      },
      "source": [
        "# import necessari\n",
        "import librosa\n",
        "import numpy\n",
        "import sys\n",
        "import os\n",
        "import glob\n",
        "import itertools\n",
        "import re\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.models\n",
        "import tensorflow.keras.backend as K\n",
        "import keras.optimizers\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, BatchNormalization, Activation, Flatten, Multiply, Add, Reshape\n",
        "from tqdm import tqdm\n",
        "from itertools import groupby\n",
        "from keras.utils import to_categorical\n",
        "from sklearn import metrics\n",
        "from google.colab import drive"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0PLnQAqM3m4",
        "outputId": "fa66eb32-ce41-4ee4-bff7-056a6062c8af"
      },
      "source": [
        "# costanti \n",
        "ALPHA = 0.75\n",
        "N_MELS = 128\n",
        "HOP_LENGTH = 512\n",
        "N_FFT = 1024\n",
        "POWER = 2.0\n",
        "FRAME_NUMS = 313\n",
        "NUM_FILES = 3349\n",
        "FRAMES = 10\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PM9bEsD3Eo2x"
      },
      "source": [
        "# Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHjYenI5_8PM"
      },
      "source": [
        "# load dataset\n",
        "def select_dirs(path):\n",
        "    dir_path = os.path.abspath(path)\n",
        "    dirs = sorted(glob.glob(dir_path))\n",
        "    return dirs\n",
        "\n",
        "def file_load(wav_name, mono=False):\n",
        "    try:\n",
        "        return librosa.load(wav_name, sr=None, mono=mono)\n",
        "    except:\n",
        "        logger.error(\"file_broken or not exists!! : {}\".format(wav_name))\n",
        "\n",
        "def file_list_generator(target_dir, dir_name=\"train\", ext=\"wav\"):\n",
        "    print(\"target_dir : {}\".format(target_dir))\n",
        "\n",
        "    # generate training list\n",
        "    training_list_path = os.path.abspath(\"{dir}/{dir_name}/*.{ext}\".format(dir=target_dir, dir_name=dir_name, ext=ext))\n",
        "    files = sorted(glob.glob(training_list_path))\n",
        "    if len(files) == 0:\n",
        "      print(\"errore\")\n",
        "    return files\n",
        "\n",
        "\n",
        "def file_to_vector_array(file_name, n_mels=64, n_fft=1024, hop_length=512, power=2.0):\n",
        "    # 02 generate melspectrogram using librosa\n",
        "    y, sr = file_load(file_name)\n",
        "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, power=power)\n",
        "\n",
        "    # 03 convert melspectrogram to log mel energy\n",
        "    log_mel_spectrogram = 20.0 / power * numpy.log10(mel_spectrogram + sys.float_info.epsilon)\n",
        "\n",
        "    return log_mel_spectrogram\n",
        "\n",
        "  \n",
        "def list_to_vector_array(file_list, msg=\"calc...\", n_mels=64, n_fft=1024, hop_length=512, power=2.0, frames=10):\n",
        "    # iterate file_to_vector_array()\n",
        "    for idx in tqdm(range(len(file_list)), desc=msg):\n",
        "        vector_array = file_to_vector_array(file_list[idx], n_mels=n_mels, n_fft=n_fft, hop_length=hop_length, power=power)\n",
        "       \n",
        "        # vector_array = numpy.delete(vector_array,[310,311,312], axis=1)\n",
        "        # vector_array = numpy.asarray(numpy.hsplit(vector_array, 31))\n",
        "\n",
        "        if idx == 0:\n",
        "            dataset = numpy.zeros((len(file_list), n_mels, frames), float)\n",
        "        dataset[vector_array.shape[0] * idx: vector_array.shape[0] * (idx + 1), :] = vector_array\n",
        "    return dataset\n",
        "\n",
        "def key_by_id(item):\n",
        "  path_splitted = item.split(\"/\")\n",
        "  file_name = path_splitted[ len(path_splitted) - 1 ]\n",
        "  file_name_splitted = file_name.split(\"_\")\n",
        "  machine_id = file_name_splitted = file_name_splitted[2]\n",
        "  return machine_id"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGMOe52TFiAH",
        "outputId": "c458b3ab-57b5-4dfc-f2bc-5d4d771e6e03"
      },
      "source": [
        "# GENERAZIONE DEI DATI DI TRAINING\n",
        "dirs = \"/content/drive/MyDrive/dcase/pump/\"\n",
        "files = file_list_generator(dirs)\n",
        "\n",
        "# La seguente riga estrae i dati dai file audio\n",
        "# train_data = list_to_vector_array(files, msg=\"generate train_dataset\", n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH, power=POWER, frames=313)\n",
        "\n",
        "# Input 1 (3349, 128, 313): matrici 128x313\n",
        "#   spettrogrammi estratti dai segnali audio\n",
        "# Loading da Google Drive\n",
        "train_data = numpy.load(\"/content/drive/MyDrive/training_not_norm_05_def.npy\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "target_dir : /content/drive/MyDrive/dcase/pump/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1LgXuBycWdr",
        "outputId": "5668a1da-cc51-4a1c-e49f-2049e37ef9da"
      },
      "source": [
        "# GENERAZIONE DELLE LABELS\n",
        "\n",
        "# load base_directory list\n",
        "grouped_list_by_machine_id = [list(v) for k,v in groupby(sorted(files), key_by_id)]\n",
        "\n",
        "# Print summary of the reading\n",
        "print(\"\\n\\n\")\n",
        "print(\"Number of machine ID of type {tipo}: {num_id}\".format(tipo = dirs, num_id = len(grouped_list_by_machine_id)))\n",
        "print(\"======================\")\n",
        "for i in range(0, len(grouped_list_by_machine_id)):\n",
        "  print(\"Number of instances with ID {id}: {num_ist}\".format(id=i, num_ist=len(grouped_list_by_machine_id[i])))\n",
        "print(\"======================\")\n",
        "\n",
        "# One-hot encoding\n",
        "label = []\n",
        "choices = []\n",
        "for i in range(0, len(grouped_list_by_machine_id)):\n",
        "  for j in range(0, len(grouped_list_by_machine_id[i])):\n",
        "    machine_id = grouped_list_by_machine_id[i][j].split('/')[7].split('_')[2]\n",
        "    #print(grouped_list_by_machine_id[i][j].split('/')[7])\n",
        "    random_choice = numpy.random.choice([\"match\", \"non_match\"], p = [ALPHA, 1-ALPHA]) \n",
        "\n",
        "    if machine_id == '00':\n",
        "      if random_choice == \"match\":\n",
        "        to_append = [1,0,0,0]\n",
        "      else: \n",
        "        not_match_label = numpy.random.choice([1, 2, 3]) \n",
        "        if not_match_label == 1:\n",
        "          to_append = [0,1,0,0]\n",
        "        elif not_match_label == 2:\n",
        "          to_append = [0,0,1,0]\n",
        "        else: \n",
        "          to_append = [0,0,0,1]\n",
        "\n",
        "    elif machine_id == '02': \n",
        "\n",
        "      if random_choice == \"match\":\n",
        "        to_append = [0,1,0,0]\n",
        "      else: \n",
        "        not_match_label = numpy.random.choice( [ 1, 2, 3] ) \n",
        "        if not_match_label == 1:\n",
        "          to_append = [1,0,0,0]\n",
        "        elif not_match_label == 2:\n",
        "          to_append = [0,0,1,0]\n",
        "        else: \n",
        "          to_append = [0,0,0,1]\n",
        "\n",
        "    elif machine_id == \"04\":\n",
        "      \n",
        "      if random_choice == \"match\":\n",
        "        to_append = [0,0,1,0]\n",
        "      else: \n",
        "        not_match_label = numpy.random.choice( [ 1, 2, 3] ) \n",
        "        if not_match_label == 1:\n",
        "          to_append = [1,0,0,0]\n",
        "        elif not_match_label == 2:\n",
        "          to_append = [0,1,0,0]\n",
        "        else: \n",
        "          to_append = [0,0,0,1]\n",
        "\n",
        "    elif machine_id == \"06\":\n",
        "      if random_choice == \"match\":\n",
        "        to_append = [0,0,0,1]\n",
        "      else: \n",
        "        not_match_label = numpy.random.choice( [ 1, 2, 3] ) \n",
        "        if not_match_label == 1:\n",
        "          to_append = [1,0,0,0]\n",
        "        elif not_match_label == 2:\n",
        "          to_append = [0,1,0,0]\n",
        "        else: \n",
        "          to_append = [0,0,1,0]\n",
        "    \n",
        "    label.append(to_append) # Append della label associata a ciascuno spettrogramma\n",
        "    choices.append(random_choice) # Append della choice utilizzata per associare la label\n",
        "                                  # La choice sarà utile in fase di addestramento per capire che tipo di loss calcolare\n",
        "\n",
        "# Trasformazione in numpy.array     \n",
        "label_training = numpy.asarray(label)\n",
        "choices_training = numpy.asarray(choices)\n",
        "print(label_training.shape)\n",
        "print(choices_training.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Number of machine ID of type /content/drive/MyDrive/dcase/pump/: 4\n",
            "======================\n",
            "Number of instances with ID 0: 906\n",
            "Number of instances with ID 1: 905\n",
            "Number of instances with ID 2: 602\n",
            "Number of instances with ID 3: 936\n",
            "======================\n",
            "(3349, 4)\n",
            "(3349,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZ2gVY4xUuUD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0501a7c0-9aeb-4205-b352-5057443bf87c"
      },
      "source": [
        "# Estrazione spettrogrammi divisi per ID\n",
        "id_00 = train_data[0:906]\n",
        "id_02 = train_data[906:1811]\n",
        "id_04 = train_data[1811:2413]\n",
        "id_06 = train_data[2413:3349]\n",
        "\n",
        "\n",
        "# MIN-MAX Normalization per ID_00\n",
        "id_00_norm = numpy.empty_like(id_00)\n",
        "max_00 = numpy.max(id_00)\n",
        "min_00 = numpy.min(id_00)\n",
        "index = 0\n",
        "for elem in id_00:\n",
        "  elem = ( elem - min_00 ) / ( max_00 - min_00)\n",
        "  id_00_norm[index] = elem\n",
        "  index = index + 1 \n",
        "\n",
        "# MIN-MAX Normalization per ID_02\n",
        "id_02_norm = numpy.empty_like(id_02)\n",
        "max_02 = numpy.max(id_02)\n",
        "min_02 = numpy.min(id_02)\n",
        "index = 0\n",
        "for elem in id_02:\n",
        "  elem = ( elem - min_02 )/(max_02-min_02)\n",
        "  id_02_norm[index] = elem\n",
        "  index = index + 1 \n",
        "\n",
        "# MIN-MAX Normalization per ID_04\n",
        "id_04_norm = numpy.empty_like(id_04)\n",
        "max_04 = numpy.max(id_04)\n",
        "min_04 = numpy.min(id_04)\n",
        "index = 0\n",
        "for elem in id_04:\n",
        "  elem = (elem-min_04)/(max_04-min_04)\n",
        "  id_04_norm[index] = elem\n",
        "  index = index + 1 \n",
        "\n",
        "# MIN-MAX Normalization per ID_06\n",
        "id_06_norm = numpy.empty_like(id_06)\n",
        "max_06 = numpy.max(id_06)\n",
        "min_06 = numpy.min(id_06)\n",
        "index = 0\n",
        "for elem in id_06:\n",
        "  elem = ( elem - min_06 ) / ( max_06 - min_06 )\n",
        "  id_06_norm[index] = elem\n",
        "  index = index + 1 \n",
        "\n",
        "print(id_00_norm.shape)\n",
        "print(id_02_norm.shape)\n",
        "print(id_04_norm.shape)\n",
        "print(id_06_norm.shape)\n",
        "\n",
        "# Ricreazione di un unico dataset contenente i vari dati opportunamente normalizzati\n",
        "train_data_norm = numpy.concatenate([id_00_norm, id_02_norm, id_04_norm, id_06_norm])\n",
        "\n",
        "print(train_data_norm.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(906, 128, 313)\n",
            "(905, 128, 313)\n",
            "(602, 128, 313)\n",
            "(936, 128, 313)\n",
            "(3349, 128, 313)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qgh3ikjsfKBG",
        "outputId": "92eff560-f093-4a1f-ca83-c2704f655964"
      },
      "source": [
        "# Questi minimi e massimi saranno utilizzati in fase di testing.\n",
        "\n",
        "# Print dei minimi per ciascun ID.\n",
        "print(min_00)\n",
        "print(min_02)\n",
        "print(min_04)\n",
        "print(min_06)\n",
        "\n",
        "# Print dei massimi per ciascun ID.\n",
        "print(max_00)\n",
        "print(max_02)\n",
        "print(max_04)\n",
        "print(max_06)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-61.276222229003906\n",
            "-57.22140884399414\n",
            "-60.722129821777344\n",
            "-68.19204711914062\n",
            "5.679316520690918\n",
            "7.576060771942139\n",
            "7.363907814025879\n",
            "8.988851547241211\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkFwLBYH89tx"
      },
      "source": [
        "# Estrazione frame 128x10 da ciascun spettrogramma\n",
        "training = numpy.zeros((200940, 128, 10)) # Dataset utilizzato per il training\n",
        "index = 0\n",
        "for vector_array in train_data_norm:\n",
        "  i = 0\n",
        "  while (i+5) < 303:\n",
        "    vector_i = numpy.zeros((128,10))\n",
        "    for j in range(0,128):\n",
        "      vector_i[j] = vector_array[j][i:i+10]\n",
        "    training[index] = vector_i\n",
        "    index += 1\n",
        "    i = i+5\n",
        "\n",
        "# Associazione della label associata a ciascun spettrogramma a ciascuno dei frame estratto da esso.\n",
        "training_labels = []\n",
        "for elem in label_training:\n",
        "  if numpy.array_equal(elem, numpy.asarray([1,0,0,0])) :\n",
        "    for i in range(60):\n",
        "      training_labels.append([1,0,0,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,1,0,0])):\n",
        "    for i in range(60):\n",
        "      training_labels.append([0,1,0,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,0,1,0])):\n",
        "    for i in range(60):\n",
        "      training_labels.append([0,0,1,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,0,0,1])):\n",
        "    for i in range(60):\n",
        "      training_labels.append([0,0,0,1])\n",
        "\n",
        "# Associazione della choice associata a ciascun spettrogramma a ciascuno dei frame estratto da esso. \n",
        "training_choices = []\n",
        "for elem in choices_training:\n",
        "  if numpy.array_equal(elem, numpy.asarray(\"match\")) :\n",
        "    for i in range(60):\n",
        "      training_choices.append(\"match\")\n",
        "  elif numpy.array_equal(elem, numpy.asarray(\"non_match\")):\n",
        "    for i in range(60):\n",
        "      training_choices.append(\"non_match\")\n",
        "\n",
        "training_labels = numpy.asarray(training_labels) # Dataset utilizzato per il training\n",
        "training_choices = numpy.asarray(training_choices) # Dataset utilizzato per il training"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuoB8WO3bi2x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d2cb5ed-739c-43d2-8e72-b15267267e10"
      },
      "source": [
        "print(training.shape)\n",
        "print(training_labels.shape)\n",
        "print(training_choices.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(200940, 128, 10)\n",
            "(200940, 4)\n",
            "(200940,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4Qy6uMKuLLV",
        "outputId": "9d5990bf-62b5-49bf-b073-530baf3d8482"
      },
      "source": [
        "# Shuffling\n",
        "randomize = numpy.arange(len(training))\n",
        "numpy.random.shuffle(randomize)\n",
        "training_tot_shuffle = training[randomize]\n",
        "training_tot_labels_shuffle = training_labels[randomize]\n",
        "training_tot_choices_shuffle = training_choices[randomize]\n",
        "\n",
        "training_shuffle = training_tot_shuffle[:180846]\n",
        "validation_shuffle = training_tot_shuffle[-20094:]\n",
        "\n",
        "training_labels_shuffle = training_tot_labels_shuffle[:180846]\n",
        "validation_labels_shuffle = training_tot_labels_shuffle[-20094:]\n",
        "\n",
        "training_choices_shuffle = training_tot_choices_shuffle[:180846]\n",
        "validation_choices_shuffle = training_tot_choices_shuffle[-20094:]\n",
        "\n",
        "print(training_shuffle.shape)\n",
        "print(training_labels_shuffle.shape)\n",
        "print(training_choices_shuffle.shape)\n",
        "print(validation_shuffle.shape)\n",
        "print(validation_labels_shuffle.shape)\n",
        "print(validation_choices_shuffle.shape)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(180846, 128, 10)\n",
            "(180846, 4)\n",
            "(180846,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdtRhIAYXsLn"
      },
      "source": [
        "# Keras Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96e3XmeeX2GS"
      },
      "source": [
        "# LAYER DEFINITION\n",
        "\n",
        "def DenseBlock(input,n):\n",
        "  x = Dense(n)(input)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  return x\n",
        "\n",
        "input_Spect = Input(shape = [128, 10])\n",
        "input_Label = Input(shape = [4,])\n",
        "\n",
        "# First Branch - Encoder\n",
        "m = Flatten(input_shape = [128, 10])(input_Spect)\n",
        "m = DenseBlock(m, 128)\n",
        "m = DenseBlock(m, 64)\n",
        "m = DenseBlock(m, 32)\n",
        "m = DenseBlock(m, 16)\n",
        "\n",
        "# Second Branch - Conditioning Feed Forward Neural Network\n",
        "x = Dense(16)(input_Label)\n",
        "x = Activation('sigmoid')(x)\n",
        "q = Dense(16)(input_Label)\n",
        "\n",
        "# Encoded Input Conditioning\n",
        "m = Multiply()([x,m])\n",
        "encoded_input_conditioned = Add()([q, m]) # Input da passare al decoder\n",
        "\n",
        "# Decoder\n",
        "m = DenseBlock(encoded_input_conditioned, 128)\n",
        "m = DenseBlock(m, 128)\n",
        "m = DenseBlock(m, 128)\n",
        "m = DenseBlock(m, 128)\n",
        "m = Dense(128*10)(m)\n",
        "m = Reshape((128,10),  input_shape=(128*10,))(m) # Output del modello\n",
        "\n",
        "loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "mse_metric = keras.metrics.MeanSquaredError(name=\"mse\")\n",
        "\n",
        "class CustomModel(tensorflow.keras.Model):\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [loss_tracker, mse_metric]\n",
        "\n",
        "    def test_step(self, data):\n",
        "        # Unpack the data\n",
        "        x, y = data\n",
        "        # Compute predictions\n",
        "        y_pred = self([x[0],x[1]], training=False)\n",
        "        # Indici match\n",
        "        match = tf.where ( tf.equal(x[2][:], \"match\") )\n",
        "        # Dati match\n",
        "        data_match = K.gather(y, match)\n",
        "        # Separazione dei dati PREDETTI sulla base degli indici relativi a match/non_match\n",
        "        # Dati match\n",
        "        pred_match = K.gather(y_pred, match)\n",
        "\n",
        "        # Update metrica\n",
        "        mse_metric.update_state(data_match, pred_match)\n",
        "\n",
        "        return {\"mse\": mse_metric.result()}\n",
        "    \n",
        "    def train_step(self, data):\n",
        "          # Unpack the data. Its structure depends on your model and on what you pass to `fit()`.\n",
        "          x, y = data\n",
        "\n",
        "          # Vettore C utilizzato per il calcolo della loss in caso di non_match\n",
        "          C = 5 \n",
        "          # Valore di probabilità utilizzato come peso\n",
        "          ALPHA = 0.75 \n",
        "\n",
        "          # Indici match\n",
        "          match = tf.where ( tf.equal(x[2][:], \"match\") )\n",
        "\n",
        "          # Indici non_match\n",
        "          not_match = tf.where ( tf.equal(x[2][:], \"non_match\") )\n",
        "\n",
        "          # Dati match\n",
        "          data_match = K.gather(y, match)\n",
        "\n",
        "          with tf.GradientTape() as tape:\n",
        "              y_pred = self([x[0],x[1]], training=True)  # Forward pass\n",
        "\n",
        "              # Separazione dei dati PREDETTI sulla base degli indici relativi a match/non_match\n",
        "              # Dati match\n",
        "              pred_match = K.gather(y_pred, match)\n",
        "              # Dati non match\n",
        "              pred_not_match = K.gather(y_pred, not_match) \n",
        "\n",
        "              loss_m = K.mean(keras.losses.mean_squared_error(data_match, pred_match)) + 1e-6  # Calcolo Loss Match\n",
        "              loss_nm = K.mean(keras.losses.mean_squared_error(C,pred_not_match)) + 1e-6     # Calcolo Loss Non_Match\n",
        "\n",
        "              loss = ALPHA * loss_m + (1 - ALPHA) * loss_nm     # loss utilizzata per l'update dei pesi\n",
        "\n",
        "          # Compute gradients\n",
        "          trainable_vars = self.trainable_variables\n",
        "          gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "          # Update weights\n",
        "          self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "          # Compute our own metrics\n",
        "          loss_tracker.update_state(loss)\n",
        "          mse_metric.update_state(y, y_pred)\n",
        "          return {\"loss\": loss_tracker.result(), \"mse\": mse_metric.result()}\n",
        "\n",
        "def get_lr_metric(optimizer):\n",
        "    def lr(y_true, y_pred):\n",
        "        return optimizer._decayed_lr(tf.float32) # I use ._decayed_lr method instead of .lr\n",
        "    return lr\n",
        "\n",
        "opt = keras.optimizers.Adam(\n",
        "    learning_rate = 0.00001,\n",
        "    beta_1=0.95,\n",
        "    beta_2=0.999\n",
        ")\n",
        "\n",
        "lr_metric = get_lr_metric(opt)\n",
        "model = CustomModel(inputs=(input_Spect, input_Label), outputs = m)\n",
        "model.compile(optimizer = opt, metrics=[\"mse\", lr_metric])"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HjXJIU4bvUH",
        "outputId": "3fcfcd35-a2d5-44ff-dc64-5032445c6e8c"
      },
      "source": [
        "history = model.fit([training_shuffle, training_labels_shuffle, training_choices_shuffle], \n",
        "          training_shuffle, \n",
        "          epochs=100, \n",
        "          batch_size=128, \n",
        "          validation_data=([validation_shuffle, validation_labels_shuffle, validation_choices_shuffle],validation_shuffle))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1413/1413 [==============================] - 16s 10ms/step - loss: 4.8582 - mse: 0.3592 - val_mse: 0.2672\n",
            "Epoch 2/100\n",
            "1413/1413 [==============================] - 13s 10ms/step - loss: 1.6759 - mse: 2.3767 - val_mse: 0.3116\n",
            "Epoch 3/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.8615 - mse: 4.0232 - val_mse: 0.3197\n",
            "Epoch 4/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.6623 - mse: 4.4639 - val_mse: 0.2993\n",
            "Epoch 5/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.5540 - mse: 4.6040 - val_mse: 0.2470\n",
            "Epoch 6/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.4823 - mse: 4.6697 - val_mse: 0.2093\n",
            "Epoch 7/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.4276 - mse: 4.7245 - val_mse: 0.2076\n",
            "Epoch 8/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.3900 - mse: 4.7366 - val_mse: 0.1796\n",
            "Epoch 9/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.3465 - mse: 4.7962 - val_mse: 0.1792\n",
            "Epoch 10/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.3174 - mse: 4.8073 - val_mse: 0.1658\n",
            "Epoch 11/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.2945 - mse: 4.8516 - val_mse: 0.2105\n",
            "Epoch 12/100\n",
            "1413/1413 [==============================] - 13s 10ms/step - loss: 0.2701 - mse: 4.8660 - val_mse: 0.1511\n",
            "Epoch 13/100\n",
            "1413/1413 [==============================] - 13s 10ms/step - loss: 0.2500 - mse: 4.8670 - val_mse: 0.1901\n",
            "Epoch 14/100\n",
            "1413/1413 [==============================] - 13s 9ms/step - loss: 0.2346 - mse: 4.9054 - val_mse: 0.1219\n",
            "Epoch 15/100\n",
            "1413/1413 [==============================] - 13s 10ms/step - loss: 0.2196 - mse: 4.9202 - val_mse: 0.1312\n",
            "Epoch 16/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.2047 - mse: 4.9263 - val_mse: 0.1077\n",
            "Epoch 17/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.1905 - mse: 4.9472 - val_mse: 0.1430\n",
            "Epoch 18/100\n",
            "1413/1413 [==============================] - 13s 10ms/step - loss: 0.1786 - mse: 4.9457 - val_mse: 0.1227\n",
            "Epoch 19/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.1711 - mse: 4.9785 - val_mse: 0.1057\n",
            "Epoch 20/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.1593 - mse: 4.9831 - val_mse: 0.1190\n",
            "Epoch 21/100\n",
            "1413/1413 [==============================] - 13s 10ms/step - loss: 0.1513 - mse: 4.9964 - val_mse: 0.1705\n",
            "Epoch 22/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.1414 - mse: 5.0004 - val_mse: 0.1192\n",
            "Epoch 23/100\n",
            "1413/1413 [==============================] - 13s 10ms/step - loss: 0.1377 - mse: 5.0114 - val_mse: 0.0977\n",
            "Epoch 24/100\n",
            "1413/1413 [==============================] - 13s 10ms/step - loss: 0.1284 - mse: 5.0107 - val_mse: 0.0951\n",
            "Epoch 25/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.1219 - mse: 5.0335 - val_mse: 0.1227\n",
            "Epoch 26/100\n",
            "1413/1413 [==============================] - 13s 9ms/step - loss: 0.1163 - mse: 5.0351 - val_mse: 0.1465\n",
            "Epoch 27/100\n",
            "1413/1413 [==============================] - 13s 9ms/step - loss: 0.1085 - mse: 5.0475 - val_mse: 0.1082\n",
            "Epoch 28/100\n",
            "1413/1413 [==============================] - 13s 9ms/step - loss: 0.1054 - mse: 5.0460 - val_mse: 0.0973\n",
            "Epoch 29/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.1018 - mse: 5.0523 - val_mse: 0.1243\n",
            "Epoch 30/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0936 - mse: 5.0683 - val_mse: 0.1149\n",
            "Epoch 31/100\n",
            "1413/1413 [==============================] - 13s 10ms/step - loss: 0.0879 - mse: 5.0705 - val_mse: 0.1107\n",
            "Epoch 32/100\n",
            "1413/1413 [==============================] - 13s 9ms/step - loss: 0.0847 - mse: 5.0877 - val_mse: 0.1243\n",
            "Epoch 33/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0813 - mse: 5.0772 - val_mse: 0.1271\n",
            "Epoch 34/100\n",
            "1413/1413 [==============================] - 13s 10ms/step - loss: 0.0773 - mse: 5.0934 - val_mse: 0.0919\n",
            "Epoch 35/100\n",
            "1413/1413 [==============================] - 13s 10ms/step - loss: 0.0764 - mse: 5.0924 - val_mse: 0.0920\n",
            "Epoch 36/100\n",
            "1413/1413 [==============================] - 13s 9ms/step - loss: 0.0696 - mse: 5.0979 - val_mse: 0.0979\n",
            "Epoch 37/100\n",
            "1413/1413 [==============================] - 13s 9ms/step - loss: 0.0694 - mse: 5.1069 - val_mse: 0.0935\n",
            "Epoch 38/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0646 - mse: 5.1061 - val_mse: 0.1014\n",
            "Epoch 39/100\n",
            "1413/1413 [==============================] - 13s 10ms/step - loss: 0.0620 - mse: 5.1101 - val_mse: 0.0920\n",
            "Epoch 40/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0580 - mse: 5.1212 - val_mse: 0.0940\n",
            "Epoch 41/100\n",
            "1413/1413 [==============================] - 13s 10ms/step - loss: 0.0565 - mse: 5.1202 - val_mse: 0.0907\n",
            "Epoch 42/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0542 - mse: 5.1295 - val_mse: 0.0966\n",
            "Epoch 43/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0531 - mse: 5.1242 - val_mse: 0.0895\n",
            "Epoch 44/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0509 - mse: 5.1284 - val_mse: 0.1102\n",
            "Epoch 45/100\n",
            "1413/1413 [==============================] - 13s 10ms/step - loss: 0.0486 - mse: 5.1367 - val_mse: 0.1081\n",
            "Epoch 46/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0478 - mse: 5.1443 - val_mse: 0.1004\n",
            "Epoch 47/100\n",
            "1413/1413 [==============================] - 13s 9ms/step - loss: 0.0444 - mse: 5.1452 - val_mse: 0.1649\n",
            "Epoch 48/100\n",
            "1413/1413 [==============================] - 13s 9ms/step - loss: 0.0454 - mse: 5.1429 - val_mse: 0.1057\n",
            "Epoch 49/100\n",
            "1413/1413 [==============================] - 13s 9ms/step - loss: 0.0409 - mse: 5.1519 - val_mse: 0.1378\n",
            "Epoch 50/100\n",
            "1413/1413 [==============================] - 13s 9ms/step - loss: 0.0392 - mse: 5.1540 - val_mse: 0.0918\n",
            "Epoch 51/100\n",
            "1413/1413 [==============================] - 13s 10ms/step - loss: 0.0380 - mse: 5.1569 - val_mse: 0.1012\n",
            "Epoch 52/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0374 - mse: 5.1579 - val_mse: 0.0869\n",
            "Epoch 53/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0366 - mse: 5.1599 - val_mse: 0.0928\n",
            "Epoch 54/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0340 - mse: 5.1656 - val_mse: 0.0949\n",
            "Epoch 55/100\n",
            "1413/1413 [==============================] - 13s 9ms/step - loss: 0.0342 - mse: 5.1639 - val_mse: 0.1074\n",
            "Epoch 56/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0318 - mse: 5.1684 - val_mse: 0.0964\n",
            "Epoch 57/100\n",
            "1413/1413 [==============================] - 13s 9ms/step - loss: 0.0317 - mse: 5.1669 - val_mse: 0.1600\n",
            "Epoch 58/100\n",
            "1413/1413 [==============================] - 13s 9ms/step - loss: 0.0307 - mse: 5.1729 - val_mse: 0.1491\n",
            "Epoch 59/100\n",
            "1413/1413 [==============================] - 13s 10ms/step - loss: 0.0307 - mse: 5.1697 - val_mse: 0.1819\n",
            "Epoch 60/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0296 - mse: 5.1693 - val_mse: 0.0875\n",
            "Epoch 61/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0290 - mse: 5.1763 - val_mse: 0.0867\n",
            "Epoch 62/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0297 - mse: 5.1733 - val_mse: 0.1025\n",
            "Epoch 63/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0282 - mse: 5.1768 - val_mse: 0.0996\n",
            "Epoch 64/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0276 - mse: 5.1784 - val_mse: 0.1028\n",
            "Epoch 65/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0271 - mse: 5.1815 - val_mse: 0.1126\n",
            "Epoch 66/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0265 - mse: 5.1781 - val_mse: 0.1349\n",
            "Epoch 67/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0248 - mse: 5.1846 - val_mse: 0.1288\n",
            "Epoch 68/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0237 - mse: 5.1871 - val_mse: 0.1213\n",
            "Epoch 69/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0255 - mse: 5.1837 - val_mse: 0.1038\n",
            "Epoch 70/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0238 - mse: 5.1850 - val_mse: 0.0783\n",
            "Epoch 71/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0224 - mse: 5.1893 - val_mse: 0.1524\n",
            "Epoch 72/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0236 - mse: 5.1902 - val_mse: 0.0813\n",
            "Epoch 73/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0234 - mse: 5.1882 - val_mse: 0.0970\n",
            "Epoch 74/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0218 - mse: 5.1946 - val_mse: 0.1177\n",
            "Epoch 75/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0216 - mse: 5.1910 - val_mse: 0.2305\n",
            "Epoch 76/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0215 - mse: 5.1916 - val_mse: 0.1056\n",
            "Epoch 77/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0210 - mse: 5.1944 - val_mse: 0.0842\n",
            "Epoch 78/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0208 - mse: 5.1950 - val_mse: 0.0976\n",
            "Epoch 79/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0194 - mse: 5.1985 - val_mse: 0.0969\n",
            "Epoch 80/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0194 - mse: 5.1960 - val_mse: 0.1467\n",
            "Epoch 81/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0179 - mse: 5.1992 - val_mse: 0.0918\n",
            "Epoch 82/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0198 - mse: 5.1967 - val_mse: 0.2288\n",
            "Epoch 83/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0189 - mse: 5.1982 - val_mse: 0.1270\n",
            "Epoch 84/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0186 - mse: 5.1996 - val_mse: 0.0865\n",
            "Epoch 85/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0177 - mse: 5.2014 - val_mse: 0.0932\n",
            "Epoch 86/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0184 - mse: 5.2008 - val_mse: 0.1373\n",
            "Epoch 87/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0174 - mse: 5.2020 - val_mse: 0.1321\n",
            "Epoch 88/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0171 - mse: 5.2046 - val_mse: 0.1492\n",
            "Epoch 89/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0175 - mse: 5.1996 - val_mse: 0.1340\n",
            "Epoch 90/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0160 - mse: 5.2059 - val_mse: 0.0841\n",
            "Epoch 91/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0175 - mse: 5.2036 - val_mse: 0.0792\n",
            "Epoch 92/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0158 - mse: 5.2047 - val_mse: 0.1567\n",
            "Epoch 93/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0164 - mse: 5.2033 - val_mse: 0.0898\n",
            "Epoch 94/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0157 - mse: 5.2060 - val_mse: 0.0781\n",
            "Epoch 95/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0156 - mse: 5.2078 - val_mse: 0.1020\n",
            "Epoch 96/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0162 - mse: 5.2026 - val_mse: 0.0807\n",
            "Epoch 97/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0153 - mse: 5.2093 - val_mse: 0.1183\n",
            "Epoch 98/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0153 - mse: 5.2064 - val_mse: 0.1493\n",
            "Epoch 99/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0142 - mse: 5.2095 - val_mse: 0.1365\n",
            "Epoch 100/100\n",
            "1413/1413 [==============================] - 14s 10ms/step - loss: 0.0145 - mse: 5.2094 - val_mse: 0.0945\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktyeNLjDcxJZ"
      },
      "source": [
        "# Salvataggio del modello\n",
        "model.save('/content/drive/MyDrive/models/21_02/model_21_02_4.h5')"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Riv8gAVv8qGq"
      },
      "source": [
        "# Salvataggio history di apprendimento\n",
        "with open('/content/drive/MyDrive/models/21_02/trainHistoryDict_21_02_4', 'wb') as file_pi:\n",
        "    pickle.dump(history.history, file_pi)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sN8yMBbSLYb1"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNh5oOXNLbBM"
      },
      "source": [
        "def get_machine_id_list_for_test(target_dir,\n",
        "                                 dir_name=\"test\",\n",
        "                                 ext=\"wav\"):\n",
        "\n",
        "    # create test files\n",
        "    dir_path = os.path.abspath(\"{dir}/{dir_name}/*.{ext}\".format(dir=target_dir, dir_name=dir_name, ext=ext))\n",
        "    file_paths = sorted(glob.glob(dir_path))\n",
        "    # extract id\n",
        "    machine_id_list = sorted(list(set(itertools.chain.from_iterable(\n",
        "        [re.findall('id_[0-9][0-9]', ext_id) for ext_id in file_paths]))))\n",
        "    return machine_id_list\n",
        "\n",
        "def test_file_list_generator(target_dir,\n",
        "                             id_name,\n",
        "                             dir_name=\"test\",\n",
        "                             prefix_normal=\"normal\",\n",
        "                             prefix_anomaly=\"anomaly\",\n",
        "                             ext=\"wav\"):\n",
        "  \n",
        "    print(\"target_dir : {}\".format(target_dir+\"_\"+id_name))\n",
        "\n",
        "    normal_files = sorted(\n",
        "    glob.glob(\"{dir}/{dir_name}/{prefix_normal}_{id_name}*.{ext}\".format(dir=target_dir,\n",
        "                                                                                 dir_name=dir_name,\n",
        "                                                                                 prefix_normal=prefix_normal,\n",
        "                                                                                 id_name=id_name,\n",
        "                                                                                 ext=ext)))\n",
        "    normal_labels = numpy.zeros(len(normal_files))\n",
        "    anomaly_files = sorted(\n",
        "    glob.glob(\"{dir}/{dir_name}/{prefix_anomaly}_{id_name}*.{ext}\".format(dir=target_dir,\n",
        "                                                                                  dir_name=dir_name,\n",
        "                                                                                  prefix_anomaly=prefix_anomaly,\n",
        "                                                                                  id_name=id_name,\n",
        "                                                                                  ext=ext)))\n",
        "    anomaly_labels = numpy.ones(len(anomaly_files))\n",
        "    files = numpy.concatenate((normal_files, anomaly_files), axis=0)\n",
        "    labels = numpy.concatenate((normal_labels, anomaly_labels), axis=0)\n",
        "    print(\"test_file  num : {num}\".format(num=len(files)))\n",
        "    if len(files) == 0:\n",
        "        print(\"no_wav_file!!\")\n",
        "    print(\"\\n========================================\")\n",
        "\n",
        "    return files, labels"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMP4wV5HL7Ez",
        "outputId": "2c27dced-2a0c-4685-8a35-44f018e9d6e9"
      },
      "source": [
        "target_dir = \"/content/drive/MyDrive/dcase/pump\"\n",
        "\n",
        "performance = []\n",
        "\n",
        "machine_type = os.path.split(target_dir)[1]\n",
        "print(\"============== MODEL LOAD ==============\")\n",
        "# set model path\n",
        "model_file = \"/content/drive/MyDrive/models/21_02/model_21_02_4.h5\"\n",
        "\n",
        "# load model file\n",
        "if not os.path.exists(model_file):\n",
        "  print(\"{} model not found \".format(machine_type))\n",
        "  sys.exit(-1)\n",
        "model = tensorflow.keras.models.load_model(model_file, custom_objects={'CustomModel': CustomModel, 'mse':mse_metric, 'lr': lr_metric})\n",
        "# model.summary()\n",
        "\n",
        "machine_id_list = get_machine_id_list_for_test(target_dir)\n",
        "\n",
        "for id_str in machine_id_list:\n",
        "  # load test file\n",
        "\n",
        "  id_num = id_str.split(\"_\")[1]\n",
        "\n",
        "  # Definizione della label \"match\" da utilizzare in fase di testing e del min e max da utilizzare per la normalizzazione\n",
        "  # i min e max sono stati calcolati a partire dai dati di training.\n",
        "  if id_num == \"00\":\n",
        "    match_labels = numpy.asarray([1,0,0,0])\n",
        "    max = max_00\n",
        "    min = min_00\n",
        "  elif id_num == \"02\":\n",
        "    match_labels = numpy.asarray([0,1,0,0])\n",
        "    max = max_02\n",
        "    min = min_02\n",
        "  elif id_num == \"04\":\n",
        "    match_labels = numpy.asarray([0,0,1,0])\n",
        "    max = max_04\n",
        "    min = min_04\n",
        "  elif id_num == \"06\": \n",
        "    match_labels = numpy.asarray([0,0,0,1])\n",
        "    max = max_06\n",
        "    min = min_06\n",
        "\n",
        "  test_files, y_true = test_file_list_generator(target_dir, id_str)\n",
        "  #print(\"\\n====== True Labels ======\")\n",
        "  #print(y_true)\n",
        "  #print(\"==> ====== Match ID Labels ======\")\n",
        "  #print(match_labels.shape)\n",
        "  #print(\"=================================\\n\")\n",
        "\n",
        "  anomaly_score_list = []\n",
        "\n",
        "  print(\"\\n============== BEGIN TEST FOR A MACHINE ID {id} ==============\".format(id=id_num))\n",
        "\n",
        "  y_pred = [0. for k in test_files]\n",
        "\n",
        "  for file_idx, file_path in tqdm(enumerate(test_files), total=len(test_files)):\n",
        "\n",
        "    # Estrazione spettrogramma audio test\n",
        "    data = file_to_vector_array(file_path, n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH, power=POWER)\n",
        "\n",
        "    # Normalizzazione spettrogramma di test\n",
        "    data = ( data - min ) / ( max - min )\n",
        "\n",
        "    # Estrazione delle frame 128x10\n",
        "    data_splitted = numpy.zeros((60, 128, 10))\n",
        "    index = 0\n",
        "    i = 0\n",
        "    while (i+5) < 303:\n",
        "      vector_i = numpy.zeros((128,10))\n",
        "      for j in range(0,128):\n",
        "        vector_i[j] = data[j][i:i+10]\n",
        "      data_splitted[index] = vector_i\n",
        "      index += 1\n",
        "      i = i+5\n",
        "\n",
        "    # Calcolo dell'errore medio sulle frame estratte dallo spettrogramma\n",
        "    elem_error = []\n",
        "    for elem in data_splitted:\n",
        "      predicted = model.predict([elem.reshape(1,128,10), match_labels.reshape(1,4)])\n",
        "\n",
        "      errors = numpy.mean(numpy.square(elem - predicted), axis=1)\n",
        "      elem_error.append(numpy.mean(errors))\n",
        "    # Log dell'errore associato all'istanza di test\n",
        "    y_pred[file_idx] = numpy.mean(elem_error)\n",
        "    anomaly_score_list.append([os.path.basename(file_path), y_pred[file_idx]])\n",
        "  # Calcolo AUC e pAUC per i dati con un certo ID_0x\n",
        "  auc = metrics.roc_auc_score(y_true,y_pred)\n",
        "  p_auc = metrics.roc_auc_score(y_true, y_pred, max_fpr=0.1)\n",
        "  performance.append([auc, p_auc])\n",
        "  print(\"AUC : {}\".format(auc))\n",
        "  print(\"pAUC : {}\".format(p_auc))\n",
        "\n",
        "  print(\"\\n============ END OF TEST FOR A MACHINE ID ============\")\n",
        "\n",
        "# Stampa di AUC e pAUC medi su tutti i dati di test (media di AUC e pAUC sui vari ID).\n",
        "print(\"\\n============ AVERAGE PERFORMANCES ============\")\n",
        "averaged_performance = numpy.mean(numpy.array(performance, dtype=float), axis=0)\n",
        "print(averaged_performance)\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============== MODEL LOAD ==============\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/243 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "target_dir : /content/drive/MyDrive/dcase/pump_id_00\n",
            "test_file  num : 243\n",
            "\n",
            "========================================\n",
            "\n",
            "====== True Labels ======\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1.]\n",
            "==> ====== Match ID Labels ======\n",
            "\n",
            "============== BEGIN TEST FOR A MACHINE ID 00 ==============\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 243/243 [09:18<00:00,  2.30s/it]\n",
            "  0%|          | 0/211 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AUC : 0.841958041958042\n",
            "pAUC : 0.7471475892528524\n",
            "\n",
            "============ END OF TEST FOR A MACHINE ID ============\n",
            "target_dir : /content/drive/MyDrive/dcase/pump_id_02\n",
            "test_file  num : 211\n",
            "\n",
            "========================================\n",
            "\n",
            "====== True Labels ======\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "==> ====== Match ID Labels ======\n",
            "\n",
            "============== BEGIN TEST FOR A MACHINE ID 02 ==============\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 211/211 [08:02<00:00,  2.29s/it]\n",
            "  0%|          | 0/200 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AUC : 0.6035135135135135\n",
            "pAUC : 0.6353722143195827\n",
            "\n",
            "============ END OF TEST FOR A MACHINE ID ============\n",
            "target_dir : /content/drive/MyDrive/dcase/pump_id_04\n",
            "test_file  num : 200\n",
            "\n",
            "========================================\n",
            "\n",
            "====== True Labels ======\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "==> ====== Match ID Labels ======\n",
            "\n",
            "============== BEGIN TEST FOR A MACHINE ID 04 ==============\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [07:36<00:00,  2.28s/it]\n",
            "  0%|          | 0/202 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AUC : 0.9983000000000001\n",
            "pAUC : 0.9910526315789474\n",
            "\n",
            "============ END OF TEST FOR A MACHINE ID ============\n",
            "target_dir : /content/drive/MyDrive/dcase/pump_id_06\n",
            "test_file  num : 202\n",
            "\n",
            "========================================\n",
            "\n",
            "====== True Labels ======\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "==> ====== Match ID Labels ======\n",
            "\n",
            "============== BEGIN TEST FOR A MACHINE ID 06 ==============\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 202/202 [07:43<00:00,  2.29s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AUC : 0.7883333333333333\n",
            "pAUC : 0.6424148606811145\n",
            "\n",
            "============ END OF TEST FOR A MACHINE ID ============\n",
            "\n",
            "============ AVERAGE PERFORMANCES ============\n",
            "[0.80802622 0.75399682]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}