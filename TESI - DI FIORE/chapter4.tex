\chapter{Experimental Evaluation}
As previously mentioned, this chapter is dedicated to an experimental evaluation of the proposed framework. Firstly, an explanation of how the experiments are conducted is presented, then there is a detailed description of the DCASE dataset, already mentioned in Chapter 2, on which the proposed framework is evaluated. Successively, the chapter shows how data are prepared in order to obtain a proper output from the architecture, with a spotlight on chosen parameters.
In conclusion, performance metrics are described and experimental results are reported.
\section{Experimental protocol}
The experimental part of this thesis shows two instances of the proposed framework, each characterized by the way autoencoder is built and using the DCASE dataset for evaluation. In particular, two different autoencoders are taken in consideration: LSTM encoder-decoder and convolutional autoencoder. The purpose of this part is in fact to show the ID conditioning effects as autoencoder layers varies, to demonstrate its compatibility with different encoding and decoding processes. Using the nature of their autoencoder, the overall architectures are so identified as ID Conditioned LSTM Autoencoder (IDC-LSTM-AE) and ID Conditioned Convolutional Autoencoder (IDCCAE). IDC-LSTM-AE and IDCCAE architectures are implemented and trained on four machines present in DCASE dataset (details reported in following sections).
\section{Dataset and recording procedure}
Previously, in Chapter 2, a brief introduction of DCASE 2020 TASK 2 has been done to introduce some approaches found in literature about the anomaly detection. During the experiments, a part of this dataset is used, in particular the one belonging to MIMII dataset \cite{20MIMIIDataset}.
This dataset contains audio clips recorded from four different machine types: pumps, valves, slide rails and fans. For each machine type there are four different versions. Table \ref{machine-descriptions} shows information about machines operations and possible failures that could occur. Clips are recorded by a circular microphone array so that single-channel-based or multi-channel-based approaches can be evaluated.
\begin{figure}[ht]
\includegraphics[scale=0.8]{TESI DI FIORE/img/micarray.png}
\centering
\caption{Microphones array disposed near machines \cite{DCASE}}
\label{micarray}
\end{figure}
\begin{table}
\small
\centering
\begin{tabularx}{\textwidth}{|c|c|c|} 
\hline
\textbf{Machine Type} & \textbf{Operations} & \textbf{Some Anomalous Conditions} \\ 
\hline
Pump & \begin{tabular}[c]{@{}c@{}}Suction from/ \\discharge to a water pool\end{tabular} & \begin{tabular}[c]{@{}c@{}}Leakage, contamination, \\clogging, etc.\end{tabular} \\ 
\hline
Fan & \begin{tabular}[c]{@{}c@{}}It works to~provide a \\continuous flow or gas \\of air in factories\end{tabular} & \begin{tabular}[c]{@{}c@{}}Unbalanced, voltage change, \\clogging, etc.\end{tabular} \\ 
\hline
Slide rail & \begin{tabular}[c]{@{}c@{}}Slide repeat at \\different speeds\end{tabular} & \begin{tabular}[c]{@{}c@{}}Rail damage, loose belt, \\no grease, etc.\end{tabular} \\ 
\hline
Valve & \begin{tabular}[c]{@{}c@{}}Open/close repeat with\\~different timing\end{tabular} & \begin{tabular}[c]{@{}c@{}}More than two \\kinds of contamination\end{tabular} \\
\hline
\end{tabularx}
\caption{Machine descriptions with some anomalous conditions.}
\label{machine-descriptions}
\end{table}
Figure \ref{micarray} depicts the recording setup with the direction and distance from each machine type (each machine sound was recorded in separate sessions). To simplify the task, DCASE authors used only the first channel of multi-channel recordings and the sampling rate of all signals has been downsampled to 16 kHz. Last important thing to be noticed is the presence of real factory environmental background noise mixed with the target machines sounds.\\
DCASE 2020 authors take this dataset and arrange it in order to create \textit{development}, \textit{additional training} and \textit{evaluation} datasets. The first one contains all the necessary for autoencoder training, such as a training set and a test set, while the last one contains test data without condition labels, used for competition submissions. The development dataset is the one used for experiments presented in next sections. Table \ref{training-test-sets-structure} reports machines and the number of audio clips found in training and test sets.\\

\begin{table}[ht]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|} 
\cline{2-9}
\multicolumn{1}{c|}{} & \multicolumn{2}{c|}{\textbf{ID00}} & \multicolumn{2}{c|}{\textbf{ID02}} & \multicolumn{2}{c|}{\textbf{ID04}} & \multicolumn{2}{c|}{\textbf{ID06}} \\ 
\hline
\textbf{Machine Type} & train & test & train & test & train & test & train & test \\ 
\hline
PUMP & 906 & 243 & 905 & 211 & 602 & 200 & 936 & 202 \\ 
\hline
FAN & 911 & 507 & 916 & 549 & 933 & 448 & 915 & 461 \\ 
\hline
SLIDER & 968 & 456 & 968 & 367 & 434 & 278 & 434 & 189 \\ 
\hline
VALVE & 891 & 219 & 608 & 220 & 900 & 220 & 892 & 220 \\
\hline
\end{tabular}
\caption{Number of training and test samples.}
\label{training-test-sets-structure}
\end{table}
In conclusion, for both instances, four models has been trained, one for each machine type, using training and test sets of all available IDs.
\section{Pre-Processing Phase}
In Chapter 3, mel-spectrogram extractor and all pre-processing blocks have been well described. The goal of this section is to show their details from the experimental point of view. In particular, settings and decisions made for parameters selection regarding mel-spectrogram extraction, normalization, frame generation and IDs pre-processing are presented. Regarding mel-spectrogram extraction, the same parameters are used for all machines: the number of bins ($n\_mels$) is 128, the STFT window ($n\_fft$) is 1024 and the $hop\_length$ is 512. Using Librosa functions \textit{load} and \textit{melspectrogram}, each audio clip is firstly converted into a Numpy ndarray representing the signal, successively it is converted to a mel-spectrogram and then the results are added to a final structure. The final structure contains all the extracted mel-spectrograms needed for training. Because of the duration of each clip (10 seconds) and the just mentioned parameters, each mel-spectrogram has the dimension of 128x313. Frame generation specs are reported in Table \ref{frame-generation}.\\
\begin{table}[ht]
\centering
\begin{tabular}{|c|c|c|c|c|} 
\cline{2-5}
\multicolumn{1}{c|}{} & \multicolumn{2}{c|}{\textbf{IDCCAE}} & \multicolumn{2}{c|}{\textbf{IDC-LSTM-AE}} \\ 
\hline
\textbf{Machine} & \textbf{Num. Frames} & \textbf{Hop-Size} & \textbf{Num. Frames} & \textbf{Hop-Size} \\ 
\hline
Pump & 15 & 20 & 12 & 25  \\ 
\hline
Fan & 15 & 20 & 12 & 25  \\ 
\hline
Valve & 15 & 20 & 16 & 18  \\ 
\hline
Slider & 21 & 14 & 22 & 13  \\
\hline
\end{tabular}
\caption{Frame generation details.}
\label{frame-generation}
\end{table}

In particular, the column \textit{Num. Frames} report, for each machine, the number of frames extracted from each spectrogram, while \textit{Hop-Size} indicates the segmentation time-window shift to extract them. Regarding the normalization, a Z-Score is applied on spectrograms sets extracted for each ID of a particular type, before the frame generation. For both instances of the framework, frames placed in input to autoencoders for training have the size of 128x32. In conclusion, regarding the IDs Pre-processing, the ID strings used to identify each machine, regardless the type, are $00$, $02$, $04$ and $06$ (as can be seen in Table \ref{training-test-sets-structure}) and they are converted respectively to $[0,0,0,1]$, $[0,0,1,0]$, $[0,1,0,0]$ and $[1,0,0,0]$. Definitely, four models per architecture type must be trained to detect eventual anomalies. Moreover, for \textit{match} and \textit{not-match} transformations an $\alpha=0.75$ is chosen, while the vector C is chosen equal to 5, after optimization.

\section{Autoencoder Structure}
In this section, the structures of IDCCAE and IDC-LSTM-AE are described. In both instances, the conditioning is a sequence of mathematical operations, in which encoder output and ID conditioning network outputs are involved, as seen in Chapter 3.

\begin{figure}[ht]
\includegraphics[scale=0.7]{TESI DI FIORE/img/CONDITIONING.png}
\centering
\caption{Conditioning operations. $n$ is the number of latent variables.}
\label{conditioning}
\end{figure}

The encoded ID is passed through a dense layer and an activation layer to produce the ID conditioning network first output, which is multiplied with encoder output. The second output is the output of another dense layer with the same encoded ID provided in input. To produce the final representation, which is decoder input, the second output is added to the result of the multiplication. Figure \ref{conditioning} better explains how the conditioning is done.

\subsection{IDCCAE}

\begin{figure}[ht]
\includegraphics[scale=0.55]{TESI DI FIORE/img/IDCCAE.png}
\centering
\caption{ID Conditioned Convolutional Autoencoder}
\label{IDCCAE}
\end{figure}

The Figure \ref{IDCCAE} reports a detailed view of how encoder and decoder block are composed in the convolutional instance of the framework. The encoder network consists in a stack of five hidden layers with convolutional filters of 32, 64, 128, 256, and 512. In particular, as can be seen at the right part of the image, each component of the encoder is a block composed by a stack of different layers: a convolutional layer, followed by batch normalization and the ReLU activation function.\\
The bottleneck consists of a layer with 40 convolutional filters, reducing the encoder feature maps to a 40-dimensional encoded representation of the input. Regarding the decoder network, first a fully-connected layer reshapes its input to the shape of the last layer of the encoder and then five ConvTransposeBlock (Conv2DTranspose layers followed by batch normalization layers and ReLu activation functions) mirror the encoder. Conditioning operations are those explained in previous sections.

\subsection{IDC-LSTM-AE}
\begin{figure}[ht]
\includegraphics[scale=0.65]{TESI DI FIORE/img/LSTM-AE.png}
\centering
\caption{ID Conditioned LSTM Autoencoder}
\label{LSTM-AE}
\end{figure}
The Figure \ref{LSTM-AE} describes the architecture of the Long-Short Term Memory version of the autoencoder. Here, encoder is composed by three LSTM layers with a decreasing number of units (64,32 and 16), which indicate the dimensionality of their output space. In this architecture, the 128x32 frames placed in input are seen as time-series of 32 timesteps, each characterized by 128 features, which are the frequency amplitudes (the $n\_mels$ bins). The decoder is the reversed version of the encoder, but at the beginning there is a RepeatVector layer, which repeats its input n times. In this case the input is repeated 32 times, such as the number of timesteps. This architecture tries to capture the temporal relationship between sequential frequency amplitudes through time, in order to learn a better function to reconstruct the inputs. In this case, encoder output is a 16-dimensional representation of its input, while the conditioning operations are, again, those explained before.

\section{Other Hyperparameters}
In previous sections, autoencoders used to build the two instances of the proposed framework have been visualized, with an explanation of how layers are built. Obviously, during the training phase other parameters are involved, which should be optimized to get as high as possible performances. In the experimental phase of the study, in addition to those seen for IDCCAE and IDC-LSTM-AE, batch size, number of epochs and learning rate are autoencoder-independent hyperparameters taken in consideration during the tuning process. Regarding batch-size, the values $\{64,128,256,512\}$ have been attempted, while for the initial learning rate choice, values like $\{10^{-2}, 10^{-3}, 10^{-4}, 10^{-5}\}$ are used. Number of epochs has been varied between 50 and 200. Adam is used as optimizer for trainings.\\
The Table \ref{hyperparam-tables} reports the best hyperparameters found during training process.

\begin{table}[ht]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|} 
\cline{2-7}
\multicolumn{1}{c|}{} & \multicolumn{3}{c|}{\textbf{IDCCAE}} & \multicolumn{3}{c|}{\textbf{IDC-LSTM-AE}} \\
\hline
\textbf{Machine} & \textbf{BS} & \textbf{EP} & \textbf{LR} & \textbf{BS} & \textbf{EP} & \textbf{LR} \\ 
\hline
Pump & 256	& 100 & 0.0001 & 512 & 100 & 0.001  \\ 
\hline
Fan & 512 & 100 & 0.0001 & 256 & 100 & 0.001  \\ 
\hline
Valve & 64 & 100 & 0.0001 &	512 & 100 & 0.001  \\ 
\hline
Slider & 12	& 100 & 0.0001 & 512 & 100 & 0.001  \\
\hline
\end{tabular}
\caption{Batch size (BS), learning rate (LR) and number of epochs (EP) chosen. These parameters are used to get final results of the experimental part of this text.}
\label{hyperparam-tables}
\end{table}
Regarding the metrics used for reconstruction errors evaluation, Mean Squared Error (MSE) is chosen for all models.

\section{Evaluation and Performance Metrics}
The goal of this section is to explain how the performances of trained models are evaluated. First of all, in both cases the test sets are pre-processed in the same way seen for training sets. Regarding the metrics used for models evaluation during the experiments, the area under the receiver operating characteristic (ROC) curve (AUC) and the partial-AUC (pAUC) are considered. These metrics are those used for the competition ranking, when the challenge was still in progress. Remembering that the ROC curve shows the trend of the true positive rate (TPR) in function of the false positive rate (FPR) at the variation of a parameter (like the threshold used for binary classification tasks), the pAUC is calculated as the AUC over a low FPR range $[0,p]$, with $p=0.1$. Formulas reported by \cite{DCASE} are:
\[ AUC =\frac{1}{N_-N_+}\sum_{i=1}^{N_-}\sum_{j=1}^{N_+}\mathcal{H}(A_\theta(x_j^+)-A_\theta(x_i^-)) \]
\[AUC =\frac{1}{ \left \lfloor pN_- \right \rfloor N_+}\sum_{i=1}^{ \left \lfloor pN_- \right \rfloor }\sum_{j=1}^{N_+}\mathcal{H}(A_\theta(x_j^+)-A_\theta(x_i^-))\]
where $A_\theta(\cdot)$ is the anomaly score generated by the autoencoder, ⌊⋅⌋ is the flooring function and $\mathcal{H}$ returns 1 when $x>0$ and $0$ otherwise. Here, $\{x^−_i\}^{N_-}_{i=1}$
and $\{x^+_j\}^{N_+}_{j=1}$ are normal and anomalous test samples, respectively, and have been sorted so that their anomaly scores are in descending order. Here, $N_−$ and $N_+$
are the number of normal and anomalous test samples, respectively. According to the above formulas, anomaly scores of normal test samples are used as thresholds. The anomaly score associated to a test sample is calculated taking the reconstruction errors average over all frames extracted from it and, after the application of normalization, placed in input. The pAUC is defined because it is especially important to increase the TPR under low FPR conditions, in that if an ASD system gives false alerts frequently we cannot trust it. \\
In conclusion, because the results produced with a GPU are generally non-deterministic, means and standard deviations are calculated from 10 independent trials (training and testing of models). In particular, once trained, a model is evaluated on test sets, generating for each ID AUC and pAUC values. Moreover, mean values of AUC and pAUC are calculated from those obtained for each ID. Mean values are used to calculate mean and standard deviation of independent trials. The next section reports results tables.
\section{Results}
This section compares the results obtained from the training of the two models just described and the results published by DCASE authors, related to the solutions proposed by participants. The IDCCAE architecture is compared with a similar version of the architecture without the ID conditioning mechanism \cite{15DeepDenseConvAE}. The IDC-LSTM-AE, in the same way, is evaluated using the results obtained by \cite{16LSTMDeepAutoencodersForASDtask}, in which a similar LSTM autoencoder without conditioning is trained. Both are also compared with results generated with baseline model provided by authors. Table \ref{table-conv-results} reports convolutional architectures results. Table \ref{table-lstm-results} reports LSTM architectures results.

%CONV TABLE
\begin{table}[ht]
\small
\centering
\begin{tabular}{|m{1.8cm}|m{1.3cm}|m{1.25cm}|m{1.3cm}|m{1.25cm}|m{1.3cm}|m{1.25cm}|m{1.3cm}|m{1.25cm}|} 
\cline{2-9}
\multicolumn{1}{c|}{} & \multicolumn{4}{c|}{\textbf{Pump}} & \multicolumn{4}{c|}{\textbf{Fan}} \\ 
\hline
\multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c|}{AUC} & \multicolumn{2}{c|}{pAUC} & \multicolumn{2}{c|}{AUC} & \multicolumn{2}{c|}{pAUC} \\ 
\cline{2-9}
 & Mean & Std.Dev & Mean & Std.Dev & Mean & Std.Dev & Mean & Std.Dev \\ 
\hline
Baseline & 72.89\% & 0,70\%~ & 59.99\% & 0.77\% & 65.83\% & 0.53\% & 52.45\% & 0.21\% \\ 
\hline
IDCCAE & \textbf{76.63\%} & 1.87\% & \textbf{67.90\%} & 1.87\% & \textbf{71.05\%} & 0.72\% & \textbf{70.33\%} & 0.55\% \\ 
\hline
CAE \cite{15DeepDenseConvAE} & 72.07\% & - & 60.96\% & - & 66.78\% & - & 52.63\% & - \\ 
\hline
\multicolumn{1}{c|}{} & \multicolumn{4}{c|}{\textbf{Slider}} & \multicolumn{4}{c|}{\textbf{Valve}} \\ 
\hline
\multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c|}{AUC} & \multicolumn{2}{c|}{pAUC} & \multicolumn{2}{c|}{AUC} & \multicolumn{2}{c|}{pAUC} \\ 
\cline{2-9}
 & Mean & Std.Dev & Mean & Std.Dev & Mean & Std.Dev & Mean & Std.Dev \\ 
\hline
Baseline & 84.76\% & 0.29\% & 66.53\% & 0,62\%~ & 66.28\% & 0.49\% & 50.98\% & 0,15\%~ \\ 
\hline
IDCCAE & 90.99\% & 4.30\% & \textbf{84.14\%} & 6.46\% & 74.73\% & 5.00\% & \textbf{61.18\%} & 5.07\% \\ 
\hline
CAE \cite{15DeepDenseConvAE} & \textbf{91.77\%} & - & 76.20\% & - & \textbf{78.83\%} & - & 53.10\% & - \\
\hline
\end{tabular}
\caption{ Mean and std.dev. of AUC and pAUC for convolutional architectures on 10 independent trials. Results found in \cite{15DeepDenseConvAE} are reported for comparison. Best results for each metric are marked in bold.}
\label{table-conv-results}
\end{table}
%LSTM TABLE
\begin{table}[ht]
\small
\centering
\begin{tabular}{|m{2.8cm}|m{1.2cm}|m{1.25cm}|m{1.2cm}|m{1.25cm}|m{1.2cm}|m{1.25cm}|m{1.2cm}|m{1.25cm}|} 
\cline{2-9}
\multicolumn{1}{c|}{} & \multicolumn{4}{c|}{\textbf{Pump}} & \multicolumn{4}{c|}{\textbf{Fan}} \\ 
\hline
\multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c|}{AUC} & \multicolumn{2}{c|}{pAUC} & \multicolumn{2}{c|}{AUC} & \multicolumn{2}{c|}{pAUC} \\ 
\cline{2-9}
 & Mean & Std.Dev & Mean & Std.Dev & Mean & Std.Dev & Mean & Std.Dev \\ 
\hline
Baseline & 72.89\% & 0,70\%~ & 59.99\% & 0.77\% & 65.83\% & 0.53\% & 52.45\% & 0.21\% \\ 
\hline
IDC-LSTM-AE & \textbf{78.29\%} & 2.21\% & \textbf{69.67\%} & 2.44\% & \textbf{67.66\%} & 2.29\% & \textbf{65.83\%} & 1.12\% \\ 
\hline
LSTM \cite{16LSTMDeepAutoencodersForASDtask} & 73.94\% & - & 61.01\% & - & 67.32\% & - & 52.05\% & - \\ 
\hline
\multicolumn{1}{c|}{} & \multicolumn{4}{c|}{\textbf{Slider}} & \multicolumn{4}{c|}{\textbf{Valve}} \\ 
\hline
\multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c|}{AUC} & \multicolumn{2}{c|}{pAUC} & \multicolumn{2}{c|}{AUC} & \multicolumn{2}{c|}{pAUC} \\ 
\cline{2-9}
 & Mean & Std.Dev & Mean & Std.Dev & Mean & Std.Dev & Mean & Std.Dev \\ 
\hline
Baseline & 84.76\% & 0.29\% & 66.53\% & 0,62\%~ & 66.28\% & 0.49\% & 50.98\% & 0,15\%~ \\ 
\hline
IDC-LSTM-AE & 82.62\% & 1.90\% & \textbf{74.48\%} & 2.64\% & 62.98\% & 2.99\% & \textbf{59.71\%} & 1.53\% \\ 
\hline
LSTM \cite{16LSTMDeepAutoencodersForASDtask} & \textbf{84.99\%} & - & 67.47\% & - & \textbf{67.82\%} & - & 51.07\% & - \\
\hline
\end{tabular}
\caption{Mean and std.dev. of AUC and pAUC for convolutional architectures on 10 independent trials. Results found in \cite{16LSTMDeepAutoencodersForASDtask} are reported for comparison. Best results for each metric are marked in bold. }
\label{table-lstm-results}
\end{table}

Analyzing Table \ref{table-conv-results} is evident that IDCCAE, regarding pump and fan,  obtains better results for both metrics, while regarding slider and valve it obtains better results especially in terms of pAUC. Table \ref{table-lstm-results} reports similar results regarding LSTM based models: again conditioned model achieve the best in terms of both metrics for pump and fan, while for slider and valve there are improvements in terms of pAUC. Remarkable are the improvements obtained with conditional autoencoders in terms of pAUC, which means that there are higher values of TPR with lower values of FPR.

\section{Threshold definition}
In Chapter 3, an online version of the proposed framework is shown. It allows a real-time audio classification to establish if a machine is working in a normal state or not. From a technical point of view, the online architecture uses the autoencoder, trained with offline procedures, to reconstruct the spectrograms placed in input. After a reconstruction error evaluation, on the basis of a threshold, audio clips extracted from the stream are classified. In fact, once obtained reconstruction errors, used as anomaly scores, AUC and pAUC are used to compare different solutions and approaches, but the threshold definition step should be done to commission the anomalous sound detector, because without it models can't classify audio inputs. This section tries to explain a possible way to calculate an optimal threshold, but it is not the only one, since different approaches about threshold definition and its optimization process can be found in literature, but this question is beyond the scope of this text. \\
To calculate an optimal threshold, the concept of optimal must be examined. It depends on the particular use case taken in exam. In fact, once defined a threshold $\epsilon$ and a test set is evaluated, a confusion matrix could be calculated, from which true positive rate (TPR, where positive means anomalous), false positive rate (FPR) and other important measures can be extracted. The threshold goodness is related to the weights and the importance associated to these measures. In anomaly detection task is important to have an high TPR and as low as possible value of FPR. To this purpose, the Youden's index J is defined \cite{13RealTimeDetectionUsingSequentialAutoencoder}:
\[ J = Sensitivity + Specificity - 1\]\[ Sensitivity = TPR = \frac{TP}{TP+FN}\]\[ Specificity = TNR = \frac{TN}{TN+FP} = 1-FPR = 1-\frac{FP}{TN+FP}\]\[ J = TPR+(1-FPR)-1 = TPR-FPR \]
The higher J is, better the threshold is, according to this definition of optimum, and to achieve the best, the threshold that corresponds to the max value of J must be found.\\
Practically, the optimal threshold has been calculated using following steps:
\begin{enumerate}
    \item {Reconstruction errors calculated from test set samples (anomaly scores) are collected.}
    \item {Using scikit-learn function \textit{metrics.roc\_curve} FPR, TPR and Thresholds, are calculated and also used to visualize ROC curve. FPR, TPR and Thresholds are three arrays of the same length.}
    \item {The optimal threshold corresponds to the element of Thresholds array at the index on which there is a maximum value of $TPR-FPR$. In other way, a vector of the differences between TPR and FPR can be calculated and then the index of the maximum difference on this vector is the index of the optimal threshold in Threshold array.}
\end{enumerate}
Following, Figure \ref{roc_curves} shows the ROC curves calculated for example from IDCCAE architecture trained on audio clips recorded from pumps. It shows the ROC curves in blue and the bisector lines in red, while black dashed lines indicates the values of J. Moreover, red dot is used to mark the FPR and TPR which correspond to optimal thresholds, also numerically reported.\\
In conclusion, in online detection phases, two alternatives should be taken in consideration:
\begin{itemize}
    \item {Calculate and use a different threshold for each ID string (or machine kind), even if the model used for prediction is one;}
    \item {Calculate and use only one threshold, regardless of the different machine versions (last ROC curve in Figure \ref{roc_curves}).}
\end{itemize}
The first option implies that, during detection, the architecture is able to select the right threshold based on the input.\\
\begin{figure}[ht]
\centering
\begin{subfigure}
    \centering
    \includegraphics[scale=0.45]{TESI DI FIORE/img/roc_id_00.png}
\end{subfigure}
\begin{subfigure}
    \centering
    \includegraphics[scale=0.45]{TESI DI FIORE/img/roc_id_02.png}
\end{subfigure}
\begin{subfigure}
    \centering
    \includegraphics[scale=0.45]{TESI DI FIORE/img/roc_id_04.png}
\end{subfigure}
\begin{subfigure}
    \centering
    \includegraphics[scale=0.45]{TESI DI FIORE/img/roc_id_06.png}
\end{subfigure}
\begin{subfigure}
    \centering
    \includegraphics[scale=0.48]{TESI DI FIORE/img/roc_complessive.png}
\end{subfigure}
\caption{ ROC curves obtained using IDCCAE model on pumps test audio clips. The image shown below is the ROC curve calculated using the predictions of all four kinds of pump, with the threshold calculated.}
\label{roc_curves}
\end{figure}