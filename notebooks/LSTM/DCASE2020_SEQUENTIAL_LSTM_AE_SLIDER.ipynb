{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCASE2020_SEQUENTIAL_LSTM_AE_SLIDER.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i90ezYjxmjFm"
      },
      "source": [
        "# IMPORT AND DATA LOADING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDG8iAV_W2fa",
        "outputId": "b1a78e66-2312-4398-8bab-4bcd9efd6a79"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9uFgr4mYC5v"
      },
      "source": [
        "# import necessari\n",
        "import librosa\n",
        "import numpy\n",
        "import sys\n",
        "import os\n",
        "import glob\n",
        "import itertools\n",
        "import re\n",
        "import pickle\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.models\n",
        "import tensorflow.keras.backend as K\n",
        "import keras.optimizers\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "from keras.layers import LSTM, RepeatVector, TimeDistributed, Dense, Dropout, Multiply, Add, Input, Activation\n",
        "from tqdm import tqdm\n",
        "from itertools import groupby\n",
        "from keras.utils import to_categorical\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# costanti \n",
        "ALPHA = 0.75\n",
        "N_MELS = 128\n",
        "HOP_LENGTH = 512\n",
        "N_FFT = 1024\n",
        "POWER = 2.0\n",
        "FRAME_NUMS = 313\n",
        "FRAMES = 10\n",
        "VAL = 0.05\n",
        "\n",
        "# FEATURES EXTRACTION\n",
        "\n",
        "# Loading da Google Drive\n",
        "train_data = numpy.load(\"/content/drive/MyDrive/DCASE_DATA_EXTRACTED/train/training_slider.npy\")\n",
        "grouped_list_by_machine_id = pickle.load( open( \"/content/drive/MyDrive/DCASE_DATA_EXTRACTED/train/training_slider_grouped_list.npy\", \"rb\" ) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJWVD1YqUDFn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abf4b732-51ca-4b80-d056-b969a79f10d3"
      },
      "source": [
        "# GENERAZIONE DELLE LABELS\n",
        "# One-hot encoding\n",
        "label = []\n",
        "choices = []\n",
        "for i in range(0, len(grouped_list_by_machine_id)):\n",
        "  for j in range(0, len(grouped_list_by_machine_id[i])):\n",
        "    machine_id = grouped_list_by_machine_id[i][j].split('/')[7].split('_')[2]\n",
        "    #print(grouped_list_by_machine_id[i][j].split('/')[7])\n",
        "    random_choice = numpy.random.choice([\"match\", \"non_match\"], p = [ALPHA, 1-ALPHA]) \n",
        "\n",
        "    if machine_id == '00':\n",
        "      if random_choice == \"match\":\n",
        "        to_append = [1,0,0,0]\n",
        "      else: \n",
        "        not_match_label = numpy.random.choice([1, 2, 3]) \n",
        "        if not_match_label == 1:\n",
        "          to_append = [0,1,0,0]\n",
        "        elif not_match_label == 2:\n",
        "          to_append = [0,0,1,0]\n",
        "        else: \n",
        "          to_append = [0,0,0,1]\n",
        "\n",
        "    elif machine_id == '02': \n",
        "      if random_choice == \"match\":\n",
        "        to_append = [0,1,0,0]\n",
        "      else: \n",
        "        not_match_label = numpy.random.choice( [ 1, 2, 3] ) \n",
        "        if not_match_label == 1:\n",
        "          to_append = [1,0,0,0]\n",
        "        elif not_match_label == 2:\n",
        "          to_append = [0,0,1,0]\n",
        "        else: \n",
        "          to_append = [0,0,0,1]\n",
        "\n",
        "    elif machine_id == \"04\":\n",
        "      if random_choice == \"match\":\n",
        "        to_append = [0,0,1,0]\n",
        "      else: \n",
        "        not_match_label = numpy.random.choice( [ 1, 2, 3] ) \n",
        "        if not_match_label == 1:\n",
        "          to_append = [1,0,0,0]\n",
        "        elif not_match_label == 2:\n",
        "          to_append = [0,1,0,0]\n",
        "        else: \n",
        "          to_append = [0,0,0,1]\n",
        "\n",
        "    elif machine_id == \"06\":\n",
        "      if random_choice == \"match\":\n",
        "        to_append = [0,0,0,1]\n",
        "      else: \n",
        "        not_match_label = numpy.random.choice( [ 1, 2, 3] ) \n",
        "        if not_match_label == 1:\n",
        "          to_append = [1,0,0,0]\n",
        "        elif not_match_label == 2:\n",
        "          to_append = [0,1,0,0]\n",
        "        else: \n",
        "          to_append = [0,0,1,0]\n",
        "    \n",
        "    label.append(to_append) # Append della label associata a ciascuno spettrogramma\n",
        "    choices.append(random_choice) # Append della choice utilizzata per associare la label\n",
        "                                  # La choice sarà utile in fase di addestramento per capire che tipo di loss calcolare\n",
        "\n",
        "# Trasformazione in numpy.array     \n",
        "label = numpy.asarray(label)\n",
        "choices = numpy.asarray(choices)\n",
        "print(label.shape)\n",
        "print(choices.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2804, 4)\n",
            "(2804,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQagajQlV65x",
        "outputId": "bd0b3b42-e93a-41bf-e728-7e0c0cccbdab"
      },
      "source": [
        "# Estrazione spettrogrammi divisi per ID\n",
        "id_00 = train_data[0:906]\n",
        "label_00 = label[0:906]\n",
        "choices_00 = choices[0:906]\n",
        "\n",
        "id_02 = train_data[906:1811]\n",
        "label_02 = label[906:1811]\n",
        "choices_02 = choices[906:1811]\n",
        "\n",
        "id_04 = train_data[1811:2413]\n",
        "label_04 = label[1811:2413]\n",
        "choices_04 = choices[1811:2413]\n",
        "\n",
        "id_06 = train_data[2413:3349]\n",
        "label_06 = label[2413:3349]\n",
        "choices_06 = choices[2413:3349]\n",
        "\n",
        "id_00_training, \\\n",
        "id_00_validation, \\\n",
        "label_00_train, \\\n",
        "label_00_validation, \\\n",
        "choices_00_train, \\\n",
        "choices_00_validation = train_test_split(id_00, label_00, choices_00, test_size=VAL, random_state=42)\n",
        "\n",
        "id_02_training, \\\n",
        "id_02_validation, \\\n",
        "label_02_train, \\\n",
        "label_02_validation, \\\n",
        "choices_02_train, \\\n",
        "choices_02_validation = train_test_split(id_02, label_02, choices_02, test_size=VAL, random_state=42)\n",
        "\n",
        "id_04_training, \\\n",
        "id_04_validation, \\\n",
        "label_04_train, \\\n",
        "label_04_validation, \\\n",
        "choices_04_train, \\\n",
        "choices_04_validation = train_test_split(id_04, label_04, choices_04, test_size=VAL, random_state=42)\n",
        "\n",
        "id_06_training, \\\n",
        "id_06_validation, \\\n",
        "label_06_train, \\\n",
        "label_06_validation, \\\n",
        "choices_06_train, \\\n",
        "choices_06_validation = train_test_split(id_06, label_06, choices_06, test_size=VAL, random_state=42)\n",
        "\n",
        "# Normalization ID_00\n",
        "id_00_norm = numpy.empty_like(id_00_training)\n",
        "mean_00 = numpy.mean(id_00_training)\n",
        "std_00 = numpy.std(id_00_training)\n",
        "id_00_norm = (id_00_training - mean_00) / (std_00)\n",
        "id_00_norm_validation = (id_00_validation - mean_00) / (std_00)\n",
        "\n",
        "# Normalization ID_02\n",
        "id_02_norm = numpy.empty_like(id_02_training)\n",
        "mean_02 = numpy.mean(id_02_training)\n",
        "std_02 = numpy.std(id_02_training)\n",
        "id_02_norm = (id_02_training - mean_02) / (std_02)\n",
        "id_02_norm_validation = (id_02_validation - mean_02) / (std_02)\n",
        "\n",
        "# Normalization ID_04\n",
        "id_04_norm = numpy.empty_like(id_04_training)\n",
        "mean_04 = numpy.mean(id_04_training)\n",
        "std_04 = numpy.std(id_04_training)\n",
        "id_04_norm = (id_04_training - mean_04) / (std_04)\n",
        "id_04_norm_validation = (id_04_validation - mean_04) / (std_04)\n",
        "\n",
        "# Normalization ID_06\n",
        "id_06_norm = numpy.empty_like(id_06_training)\n",
        "mean_06 = numpy.mean(id_06_training)\n",
        "std_06 = numpy.std(id_06_training)\n",
        "id_06_norm = (id_06_training - mean_06) / (std_06)\n",
        "id_06_norm_validation = (id_06_validation - mean_06) / (std_06)\n",
        "\n",
        "print(\"==== DATA ====\")\n",
        "total_training = numpy.concatenate([id_00_norm, id_02_norm, id_04_norm, id_06_norm])\n",
        "print(total_training.shape)\n",
        "total_validation = numpy.concatenate([id_00_norm_validation, id_02_norm_validation, id_04_norm_validation, id_06_norm_validation])\n",
        "print(total_validation.shape)\n",
        "\n",
        "print(\"==== LABELS ====\")\n",
        "total_training_label = numpy.concatenate([label_00_train, label_02_train, label_04_train, label_06_train])\n",
        "print(total_training_label.shape)\n",
        "total_validation_label = numpy.concatenate([label_00_validation, label_02_validation, label_04_validation, label_06_validation])\n",
        "print(total_validation_label.shape)\n",
        "\n",
        "print(\"==== CHOICES ====\")\n",
        "total_training_choices = numpy.concatenate([choices_00_train, choices_02_train, choices_04_train, choices_06_train])\n",
        "print(total_training_choices.shape)\n",
        "total_validation_choices = numpy.concatenate([choices_00_validation, choices_02_validation, choices_04_validation, choices_06_validation])\n",
        "print(total_validation_choices.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==== DATA ====\n",
            "(2661, 128, 313)\n",
            "(143, 128, 313)\n",
            "==== LABELS ====\n",
            "(2661, 4)\n",
            "(143, 4)\n",
            "==== CHOICES ====\n",
            "(2661,)\n",
            "(143,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_KQ1vZ2Yh1x",
        "outputId": "c1b2eba3-2b28-4c22-83c2-7aa014dc4f9f"
      },
      "source": [
        "training_aug = numpy.zeros((len(total_training)*22, 128, 32)) # Dataset utilizzato per il training\n",
        "index = 0\n",
        "for vector_array in total_training:\n",
        "  i = 0\n",
        "  while (i+32) <= 313:\n",
        "    vector_i = numpy.zeros((128,32))\n",
        "    for j in range(0,128):\n",
        "      vector_i[j] = vector_array[j][i:i+32]\n",
        "    training_aug[index] = vector_i\n",
        "    index += 1\n",
        "    i = i+13\n",
        "\n",
        "validation_aug = numpy.zeros((len(total_validation)*22, 128, 32)) # Dataset utilizzato per il training\n",
        "index = 0\n",
        "for vector_array in total_validation:\n",
        "  i = 0\n",
        "  while (i+32) <= 313:\n",
        "    vector_i = numpy.zeros((128,32))\n",
        "    for j in range(0,128):\n",
        "      vector_i[j] = vector_array[j][i:i+32]\n",
        "    validation_aug[index] = vector_i\n",
        "    index += 1\n",
        "    i = i+13\n",
        "\n",
        "training_aug_transpose = numpy.zeros((len(training_aug), 32, 128))\n",
        "index = 0\n",
        "for elem in training_aug:\n",
        "  training_aug_transpose[index] = elem.T\n",
        "  index += 1\n",
        "print(training_aug_transpose.shape)\n",
        "\n",
        "validation_aug_transpose = numpy.zeros((len(validation_aug), 32, 128))\n",
        "index = 0\n",
        "for elem in validation_aug:\n",
        "  validation_aug_transpose[index] = elem.T\n",
        "  index += 1\n",
        "print(validation_aug_transpose.shape)\n",
        "\n",
        "\n",
        "####### LABELS ######\n",
        "# Associazione della label associata a ciascun spettrogramma a ciascuno dei frame estratto da esso.\n",
        "training_labels = []\n",
        "for elem in total_training_label:\n",
        "  if numpy.array_equal(elem, numpy.asarray([1,0,0,0])) :\n",
        "    for i in range(22):\n",
        "      training_labels.append([1,0,0,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,1,0,0])):\n",
        "    for i in range(22):\n",
        "      training_labels.append([0,1,0,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,0,1,0])):\n",
        "    for i in range(22):\n",
        "      training_labels.append([0,0,1,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,0,0,1])):\n",
        "    for i in range(22):\n",
        "      training_labels.append([0,0,0,1])\n",
        "\n",
        "validation_labels = []\n",
        "for elem in total_validation_label:\n",
        "  if numpy.array_equal(elem, numpy.asarray([1,0,0,0])) :\n",
        "    for i in range(22):\n",
        "      validation_labels.append([1,0,0,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,1,0,0])):\n",
        "    for i in range(22):\n",
        "      validation_labels.append([0,1,0,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,0,1,0])):\n",
        "    for i in range(22):\n",
        "      validation_labels.append([0,0,1,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,0,0,1])):\n",
        "    for i in range(22):\n",
        "      validation_labels.append([0,0,0,1])\n",
        "\n",
        "training_labels = numpy.asarray(training_labels) # Dataset utilizzato per il training\n",
        "validation_labels = numpy.asarray(validation_labels) # Dataset utilizzato per il training\n",
        "#####################\n",
        "\n",
        "\n",
        "####### CHOICES ######\n",
        "# Associazione della choice associata a ciascun spettrogramma a ciascuno dei frame estratto da esso. \n",
        "training_choices = []\n",
        "for elem in total_training_choices:\n",
        "  if numpy.array_equal(elem, numpy.asarray(\"match\")) :\n",
        "    for i in range(22):\n",
        "      training_choices.append(\"match\")\n",
        "  elif numpy.array_equal(elem, numpy.asarray(\"non_match\")):\n",
        "    for i in range(22):\n",
        "      training_choices.append(\"non_match\")\n",
        "\n",
        "validation_choices = []\n",
        "for elem in total_validation_choices:\n",
        "  if numpy.array_equal(elem, numpy.asarray(\"match\")) :\n",
        "    for i in range(22):\n",
        "      validation_choices.append(\"match\")\n",
        "  elif numpy.array_equal(elem, numpy.asarray(\"non_match\")):\n",
        "    for i in range(22):\n",
        "      validation_choices.append(\"non_match\")\n",
        "\n",
        "training_choices = numpy.asarray(training_choices) # Dataset utilizzato per il training\n",
        "validation_choices = numpy.asarray(validation_choices) # Dataset utilizzato per il training\n",
        "######################\n",
        "\n",
        "print(\"==== DATA ====\")\n",
        "print(training_aug_transpose.shape)\n",
        "print(validation_aug_transpose.shape)\n",
        "\n",
        "print(\"==== LABELS ====\")\n",
        "print(training_labels.shape)\n",
        "print(validation_labels.shape)\n",
        "\n",
        "print(\"==== CHOICES ====\")\n",
        "print(training_choices.shape)\n",
        "print(validation_choices.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(31932, 32, 128)\n",
            "(1716, 32, 128)\n",
            "==== DATA ====\n",
            "(31932, 32, 128)\n",
            "(1716, 32, 128)\n",
            "==== LABELS ====\n",
            "(31932, 4)\n",
            "(1716, 4)\n",
            "==== CHOICES ====\n",
            "(31932,)\n",
            "(1716,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtyHa1v0my9w"
      },
      "source": [
        "# KERAS MODEL DEFINITION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISWY6OjXbZNd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fdea328-6029-4e9a-a9c1-ec4995f15f11"
      },
      "source": [
        "timesteps = 32\n",
        "num_features = 128\n",
        "\n",
        "input_Spect = Input(shape = [timesteps, num_features])\n",
        "input_Label = Input(shape = [4,])\n",
        "\n",
        "x = LSTM(64, \n",
        "        batch_input_shape=(None, timesteps, num_features), \n",
        "        return_sequences=True, name='encoder_1', \n",
        "        kernel_constraint = max_norm(1), \n",
        "        recurrent_constraint = max_norm(1), \n",
        "        bias_constraint = max_norm(1))(input_Spect)\n",
        "\n",
        "x = LSTM(32, \n",
        "        return_sequences=True, name='encoder_2', \n",
        "        kernel_constraint = max_norm(1), \n",
        "        recurrent_constraint = max_norm(1), \n",
        "        bias_constraint = max_norm(1))(x)\n",
        "\n",
        "x = LSTM(16, \n",
        "        return_sequences=False, \n",
        "        name='encoder_3', \n",
        "        kernel_constraint = max_norm(1), \n",
        "        recurrent_constraint = max_norm(1), \n",
        "        bias_constraint = max_norm(1))(x)\n",
        "\n",
        "# Second Branch - Conditioning Feed Forward Neural Network\n",
        "m = Dense(16)(input_Label)\n",
        "m = Activation('sigmoid')(m)\n",
        "q = Dense(16)(input_Label)\n",
        "\n",
        "# Encoded Input Conditioning\n",
        "m = Multiply()([x, m])\n",
        "encoded_input_conditioned = Add()([q, m]) # Input da passare al decoder\n",
        "\n",
        "x = RepeatVector(timesteps, name='encoder_decoder_bridge')(encoded_input_conditioned)\n",
        "\n",
        "x = LSTM(16, \n",
        "        return_sequences=True, name='decoder_1', \n",
        "        kernel_constraint = max_norm(1), \n",
        "        recurrent_constraint = max_norm(1), \n",
        "        bias_constraint = max_norm(1))(x)\n",
        "\n",
        "x = LSTM(32, \n",
        "        return_sequences=True, name='decoder_2', \n",
        "        kernel_constraint = max_norm(1), \n",
        "        recurrent_constraint = max_norm(1), \n",
        "        bias_constraint = max_norm(1))(x)\n",
        "\n",
        "x = LSTM(64, \n",
        "        return_sequences=True, name='decoder_3', \n",
        "        kernel_constraint = max_norm(1), \n",
        "        recurrent_constraint = max_norm(1), \n",
        "        bias_constraint = max_norm(1))(x)\n",
        "\n",
        "x = TimeDistributed(Dense(num_features))(x)\n",
        "\n",
        "loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "mse_metric = keras.metrics.MeanSquaredError(name=\"mse\")\n",
        "\n",
        "class CustomModel(tensorflow.keras.Model):\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [loss_tracker, mse_metric]\n",
        "\n",
        "    def test_step(self, data):\n",
        "        # Unpack the data\n",
        "        x, y = data\n",
        "        # Compute predictions\n",
        "        y_pred = self([x[0],x[1]], training=False)\n",
        "        # Indici match\n",
        "        match = tf.where ( tf.equal(x[2][:], \"match\") )\n",
        "        # Dati match\n",
        "        data_match = K.gather(y, match)\n",
        "        # Separazione dei dati PREDETTI sulla base degli indici relativi a match/non_match\n",
        "        # Dati match\n",
        "        pred_match = K.gather(y_pred, match)\n",
        "\n",
        "        # Update metrica\n",
        "        mse_metric.update_state(data_match, pred_match)\n",
        "\n",
        "        return {\"mse\": mse_metric.result()}\n",
        "    \n",
        "    def train_step(self, data):\n",
        "          # Unpack the data. Its structure depends on your model and on what you pass to `fit()`.\n",
        "          x, y = data\n",
        "\n",
        "          # Vettore C utilizzato per il calcolo della loss in caso di non_match\n",
        "          C = 5 \n",
        "          # Valore di probabilità utilizzato come peso\n",
        "          ALPHA = 0.75 \n",
        "\n",
        "          # Indici match\n",
        "          match = tf.where ( tf.equal(x[2][:], \"match\") )\n",
        "\n",
        "          # Indici non_match\n",
        "          not_match = tf.where ( tf.equal(x[2][:], \"non_match\") )\n",
        "\n",
        "          # Dati match\n",
        "          data_match = K.gather(y, match)\n",
        "\n",
        "          with tf.GradientTape() as tape:\n",
        "              y_pred = self([x[0],x[1]], training=True)  # Forward pass\n",
        "\n",
        "              # Separazione dei dati PREDETTI sulla base degli indici relativi a match/non_match\n",
        "              # Dati match\n",
        "              pred_match = K.gather(y_pred, match)\n",
        "              # Dati non match\n",
        "              pred_not_match = K.gather(y_pred, not_match) \n",
        "\n",
        "              loss_m = K.mean(keras.losses.mean_squared_error(data_match, pred_match)) + 1e-6  # Calcolo Loss Match\n",
        "              loss_nm = K.mean(keras.losses.mean_squared_error(C,pred_not_match)) + 1e-6     # Calcolo Loss Non_Match\n",
        "\n",
        "              loss = ALPHA * loss_m + (1 - ALPHA) * loss_nm     # loss utilizzata per l'update dei pesi\n",
        "\n",
        "          # Compute gradients\n",
        "          trainable_vars = self.trainable_variables\n",
        "          gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "          # Update weights\n",
        "          self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "          # Compute our own metrics\n",
        "          loss_tracker.update_state(loss)\n",
        "          mse_metric.update_state(y, y_pred)\n",
        "          return {\"loss\": loss_tracker.result(), \"mse\": mse_metric.result()}\n",
        "\n",
        "model = CustomModel(inputs=(input_Spect, input_Label), outputs = x)\n",
        "model.compile(metrics=[\"mse\"], optimizer = \"adam\")\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"custom_model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 128)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 4)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder_1 (LSTM)                (None, 32, 64)       49408       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "encoder_2 (LSTM)                (None, 32, 32)       12416       encoder_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 16)           80          input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "encoder_3 (LSTM)                (None, 16)           3136        encoder_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 16)           0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 16)           80          input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "multiply (Multiply)             (None, 16)           0           encoder_3[0][0]                  \n",
            "                                                                 activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 16)           0           dense_1[0][0]                    \n",
            "                                                                 multiply[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "encoder_decoder_bridge (RepeatV (None, 32, 16)       0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "decoder_1 (LSTM)                (None, 32, 16)       2112        encoder_decoder_bridge[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "decoder_2 (LSTM)                (None, 32, 32)       6272        decoder_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "decoder_3 (LSTM)                (None, 32, 64)       24832       decoder_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, 32, 128)      8320        decoder_3[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 106,656\n",
            "Trainable params: 106,656\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbvCY_H6hDZA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeae1e64-931c-45bc-e6b0-1e5068f09129"
      },
      "source": [
        "history = model.fit([training_aug_transpose, training_labels, training_choices],\n",
        "                    training_aug_transpose, \n",
        "                    epochs=100, \n",
        "                    validation_data=([validation_aug_transpose, validation_labels, validation_choices], validation_aug_transpose), \n",
        "                    batch_size=512)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "63/63 [==============================] - 43s 61ms/step - loss: 6.0055 - mse: 1.5233 - val_mse: 2.0456\n",
            "Epoch 2/100\n",
            "63/63 [==============================] - 2s 30ms/step - loss: 5.0419 - mse: 2.0158 - val_mse: 1.9878\n",
            "Epoch 3/100\n",
            "63/63 [==============================] - 2s 29ms/step - loss: 4.9000 - mse: 2.1306 - val_mse: 2.0171\n",
            "Epoch 4/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 4.8806 - mse: 2.1610 - val_mse: 1.9957\n",
            "Epoch 5/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 4.8718 - mse: 2.1583 - val_mse: 1.9534\n",
            "Epoch 6/100\n",
            "63/63 [==============================] - 2s 27ms/step - loss: 4.8643 - mse: 2.1670 - val_mse: 1.9770\n",
            "Epoch 7/100\n",
            "63/63 [==============================] - 2s 27ms/step - loss: 4.8625 - mse: 2.1693 - val_mse: 2.0074\n",
            "Epoch 8/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 4.8587 - mse: 2.1747 - val_mse: 1.9588\n",
            "Epoch 9/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 4.8574 - mse: 2.1773 - val_mse: 2.0148\n",
            "Epoch 10/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 4.8584 - mse: 2.1705 - val_mse: 2.0458\n",
            "Epoch 11/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 4.8566 - mse: 2.1771 - val_mse: 1.9571\n",
            "Epoch 12/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 4.8575 - mse: 2.1808 - val_mse: 1.9545\n",
            "Epoch 13/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 4.8589 - mse: 2.1739 - val_mse: 2.0626\n",
            "Epoch 14/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 4.8551 - mse: 2.1684 - val_mse: 2.0528\n",
            "Epoch 15/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 4.8583 - mse: 2.1856 - val_mse: 1.9762\n",
            "Epoch 16/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 4.8551 - mse: 2.1753 - val_mse: 1.9961\n",
            "Epoch 17/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 4.8534 - mse: 2.1787 - val_mse: 2.0256\n",
            "Epoch 18/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 4.8527 - mse: 2.1861 - val_mse: 1.9302\n",
            "Epoch 19/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 4.8510 - mse: 2.1738 - val_mse: 2.0038\n",
            "Epoch 20/100\n",
            "63/63 [==============================] - 2s 29ms/step - loss: 4.8488 - mse: 2.1763 - val_mse: 2.0625\n",
            "Epoch 21/100\n",
            "63/63 [==============================] - 2s 29ms/step - loss: 4.8526 - mse: 2.1905 - val_mse: 1.9675\n",
            "Epoch 22/100\n",
            "63/63 [==============================] - 2s 29ms/step - loss: 4.8524 - mse: 2.1769 - val_mse: 2.0269\n",
            "Epoch 23/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 4.8508 - mse: 2.1805 - val_mse: 1.9810\n",
            "Epoch 24/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 4.8525 - mse: 2.1806 - val_mse: 2.0431\n",
            "Epoch 25/100\n",
            "63/63 [==============================] - 2s 29ms/step - loss: 4.8263 - mse: 2.2146 - val_mse: 2.0576\n",
            "Epoch 26/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 4.5684 - mse: 2.3611 - val_mse: 2.1661\n",
            "Epoch 27/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 4.5379 - mse: 2.5411 - val_mse: 2.0787\n",
            "Epoch 28/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 4.4793 - mse: 2.5305 - val_mse: 2.0016\n",
            "Epoch 29/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 4.4857 - mse: 2.4697 - val_mse: 1.9911\n",
            "Epoch 30/100\n",
            "63/63 [==============================] - 2s 29ms/step - loss: 4.7416 - mse: 2.2778 - val_mse: 2.0269\n",
            "Epoch 31/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 4.7962 - mse: 2.1828 - val_mse: 1.8957\n",
            "Epoch 32/100\n",
            "63/63 [==============================] - 2s 29ms/step - loss: 4.4980 - mse: 2.4046 - val_mse: 1.9969\n",
            "Epoch 33/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 4.4745 - mse: 2.4447 - val_mse: 1.9696\n",
            "Epoch 34/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 4.3782 - mse: 2.5181 - val_mse: 1.7802\n",
            "Epoch 35/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 3.4515 - mse: 3.3983 - val_mse: 1.3723\n",
            "Epoch 36/100\n",
            "63/63 [==============================] - 2s 29ms/step - loss: 2.8769 - mse: 4.3204 - val_mse: 1.2158\n",
            "Epoch 37/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 2.6224 - mse: 4.4940 - val_mse: 1.2178\n",
            "Epoch 38/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 2.6159 - mse: 4.5743 - val_mse: 1.2710\n",
            "Epoch 39/100\n",
            "63/63 [==============================] - 2s 29ms/step - loss: 2.4350 - mse: 4.8550 - val_mse: 1.1230\n",
            "Epoch 40/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 2.3859 - mse: 4.8818 - val_mse: 1.1256\n",
            "Epoch 41/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 2.3312 - mse: 4.8974 - val_mse: 1.0015\n",
            "Epoch 42/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 2.4145 - mse: 5.0951 - val_mse: 3.1980\n",
            "Epoch 43/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 4.4866 - mse: 2.6182 - val_mse: 1.7522\n",
            "Epoch 44/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 2.6463 - mse: 4.2944 - val_mse: 1.2482\n",
            "Epoch 45/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 2.4464 - mse: 4.8331 - val_mse: 1.1789\n",
            "Epoch 46/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 2.4153 - mse: 4.7740 - val_mse: 1.0359\n",
            "Epoch 47/100\n",
            "63/63 [==============================] - 2s 29ms/step - loss: 2.3546 - mse: 4.8839 - val_mse: 0.9359\n",
            "Epoch 48/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 2.4466 - mse: 4.8679 - val_mse: 1.0933\n",
            "Epoch 49/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 2.3678 - mse: 4.8281 - val_mse: 1.0305\n",
            "Epoch 50/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 2.3731 - mse: 4.9133 - val_mse: 1.1258\n",
            "Epoch 51/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 2.3266 - mse: 4.8906 - val_mse: 1.0278\n",
            "Epoch 52/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 2.3442 - mse: 4.9479 - val_mse: 0.9117\n",
            "Epoch 53/100\n",
            "63/63 [==============================] - 2s 29ms/step - loss: 2.2745 - mse: 5.0008 - val_mse: 1.0438\n",
            "Epoch 54/100\n",
            "63/63 [==============================] - 2s 29ms/step - loss: 2.4312 - mse: 4.7884 - val_mse: 1.1922\n",
            "Epoch 55/100\n",
            "63/63 [==============================] - 2s 29ms/step - loss: 2.3366 - mse: 4.9088 - val_mse: 1.0443\n",
            "Epoch 56/100\n",
            "63/63 [==============================] - 2s 29ms/step - loss: 2.2570 - mse: 5.0118 - val_mse: 0.9621\n",
            "Epoch 57/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 2.2471 - mse: 5.0437 - val_mse: 0.8828\n",
            "Epoch 58/100\n",
            "63/63 [==============================] - 2s 29ms/step - loss: 2.2297 - mse: 5.0359 - val_mse: 0.8896\n",
            "Epoch 59/100\n",
            "63/63 [==============================] - 2s 29ms/step - loss: 2.2246 - mse: 5.0427 - val_mse: 0.8966\n",
            "Epoch 60/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 2.2040 - mse: 5.0553 - val_mse: 0.7734\n",
            "Epoch 61/100\n",
            "63/63 [==============================] - 2s 28ms/step - loss: 2.2113 - mse: 5.0411 - val_mse: 0.8951\n",
            "Epoch 62/100\n",
            "63/63 [==============================] - 2s 29ms/step - loss: 2.1906 - mse: 5.0288 - val_mse: 0.8044\n",
            "Epoch 63/100\n",
            "63/63 [==============================] - 2s 29ms/step - loss: 2.1617 - mse: 5.0596 - val_mse: 0.8086\n",
            "Epoch 64/100\n",
            "63/63 [==============================] - 2s 29ms/step - loss: 2.4148 - mse: 4.8891 - val_mse: 0.7435\n",
            "Epoch 65/100\n",
            "63/63 [==============================] - 2s 29ms/step - loss: 2.2623 - mse: 4.9852 - val_mse: 0.8311\n",
            "Epoch 66/100\n",
            "63/63 [==============================] - 2s 29ms/step - loss: 2.1669 - mse: 5.0581 - val_mse: 0.7363\n",
            "Epoch 67/100\n",
            "63/63 [==============================] - 2s 29ms/step - loss: 2.1422 - mse: 5.0685 - val_mse: 0.7757\n",
            "Epoch 68/100\n",
            "63/63 [==============================] - 2s 29ms/step - loss: 2.1498 - mse: 5.0693 - val_mse: 0.6781\n",
            "Epoch 69/100\n",
            "63/63 [==============================] - 2s 29ms/step - loss: 2.1615 - mse: 5.0714 - val_mse: 0.7379\n",
            "Epoch 70/100\n",
            "63/63 [==============================] - 2s 30ms/step - loss: 2.1425 - mse: 5.0771 - val_mse: 0.6890\n",
            "Epoch 71/100\n",
            "63/63 [==============================] - 2s 29ms/step - loss: 2.1180 - mse: 5.1229 - val_mse: 0.7113\n",
            "Epoch 72/100\n",
            "63/63 [==============================] - 2s 29ms/step - loss: 2.1642 - mse: 5.0451 - val_mse: 0.8343\n",
            "Epoch 73/100\n",
            "63/63 [==============================] - 2s 30ms/step - loss: 2.1338 - mse: 5.0932 - val_mse: 0.7191\n",
            "Epoch 74/100\n",
            "63/63 [==============================] - 2s 29ms/step - loss: 2.1125 - mse: 5.1213 - val_mse: 0.6654\n",
            "Epoch 75/100\n",
            "63/63 [==============================] - 2s 30ms/step - loss: 2.1108 - mse: 5.1217 - val_mse: 0.7233\n",
            "Epoch 76/100\n",
            "63/63 [==============================] - 2s 30ms/step - loss: 2.1007 - mse: 5.1397 - val_mse: 0.6516\n",
            "Epoch 77/100\n",
            "63/63 [==============================] - 2s 30ms/step - loss: 2.0944 - mse: 5.1425 - val_mse: 0.8285\n",
            "Epoch 78/100\n",
            "63/63 [==============================] - 2s 30ms/step - loss: 2.0820 - mse: 5.1639 - val_mse: 0.6550\n",
            "Epoch 79/100\n",
            "63/63 [==============================] - 2s 30ms/step - loss: 2.7705 - mse: 4.3464 - val_mse: 0.9006\n",
            "Epoch 80/100\n",
            "63/63 [==============================] - 2s 30ms/step - loss: 2.3954 - mse: 4.7761 - val_mse: 0.8076\n",
            "Epoch 81/100\n",
            "63/63 [==============================] - 2s 30ms/step - loss: 2.2737 - mse: 4.9945 - val_mse: 0.8004\n",
            "Epoch 82/100\n",
            "63/63 [==============================] - 2s 30ms/step - loss: 2.1726 - mse: 5.0185 - val_mse: 0.7624\n",
            "Epoch 83/100\n",
            "63/63 [==============================] - 2s 30ms/step - loss: 2.1578 - mse: 5.0626 - val_mse: 0.8347\n",
            "Epoch 84/100\n",
            "63/63 [==============================] - 2s 30ms/step - loss: 2.1551 - mse: 5.0556 - val_mse: 0.6433\n",
            "Epoch 85/100\n",
            "63/63 [==============================] - 2s 30ms/step - loss: 2.1342 - mse: 5.0811 - val_mse: 0.7181\n",
            "Epoch 86/100\n",
            "63/63 [==============================] - 2s 30ms/step - loss: 2.1201 - mse: 5.0945 - val_mse: 0.6707\n",
            "Epoch 87/100\n",
            "63/63 [==============================] - 2s 30ms/step - loss: 2.1286 - mse: 5.0946 - val_mse: 0.7274\n",
            "Epoch 88/100\n",
            "63/63 [==============================] - 2s 30ms/step - loss: 2.1280 - mse: 5.1104 - val_mse: 0.7247\n",
            "Epoch 89/100\n",
            "63/63 [==============================] - 2s 29ms/step - loss: 2.1251 - mse: 5.0860 - val_mse: 0.6207\n",
            "Epoch 90/100\n",
            "63/63 [==============================] - 2s 29ms/step - loss: 2.1147 - mse: 5.1123 - val_mse: 0.6108\n",
            "Epoch 91/100\n",
            "63/63 [==============================] - 2s 29ms/step - loss: 2.1144 - mse: 5.1113 - val_mse: 0.6267\n",
            "Epoch 92/100\n",
            "63/63 [==============================] - 2s 30ms/step - loss: 2.1116 - mse: 5.1245 - val_mse: 0.6490\n",
            "Epoch 93/100\n",
            "63/63 [==============================] - 2s 30ms/step - loss: 2.0888 - mse: 5.1249 - val_mse: 0.7933\n",
            "Epoch 94/100\n",
            "63/63 [==============================] - 2s 30ms/step - loss: 2.0819 - mse: 5.1675 - val_mse: 0.7288\n",
            "Epoch 95/100\n",
            "63/63 [==============================] - 2s 30ms/step - loss: 2.0970 - mse: 5.2043 - val_mse: 0.6502\n",
            "Epoch 96/100\n",
            "63/63 [==============================] - 2s 29ms/step - loss: 2.0801 - mse: 5.1771 - val_mse: 0.7259\n",
            "Epoch 97/100\n",
            "63/63 [==============================] - 2s 29ms/step - loss: 2.0326 - mse: 5.2215 - val_mse: 0.6784\n",
            "Epoch 98/100\n",
            "63/63 [==============================] - 2s 29ms/step - loss: 2.0585 - mse: 5.2292 - val_mse: 0.7443\n",
            "Epoch 99/100\n",
            "63/63 [==============================] - 2s 29ms/step - loss: 2.0182 - mse: 5.2474 - val_mse: 0.7678\n",
            "Epoch 100/100\n",
            "63/63 [==============================] - 2s 29ms/step - loss: 1.9130 - mse: 5.3453 - val_mse: 1.1761\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZ8P46b6tq11"
      },
      "source": [
        "model.save('/content/drive/MyDrive/models/IDC-LSTM-AE/slider/1/model_slider.h5') \n",
        "with open('/content/drive/MyDrive/models/IDC-LSTM-AE/slider/1/trainHistoryDict', 'wb') as file_pi:\n",
        "    pickle.dump(history.history, file_pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOxKYCAPmwbO"
      },
      "source": [
        "# TESTING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y42D3ejetbrN"
      },
      "source": [
        "import csv\n",
        "\n",
        "def save_csv(save_file_path,\n",
        "             save_data):\n",
        "    with open(save_file_path, \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f, lineterminator='\\n')\n",
        "        writer.writerows(save_data)\n",
        "\n",
        "\n",
        "# load dataset\n",
        "def select_dirs(path):\n",
        "    dir_path = os.path.abspath(path)\n",
        "    dirs = sorted(glob.glob(dir_path))\n",
        "    return dirs\n",
        "\n",
        "def file_load(wav_name, mono=False):\n",
        "    try:\n",
        "        return librosa.load(wav_name, sr=None, mono=mono)\n",
        "    except:\n",
        "        logger.error(\"file_broken or not exists!! : {}\".format(wav_name))\n",
        "\n",
        "def file_list_generator(target_dir, dir_name=\"train\", ext=\"wav\"):\n",
        "    print(\"target_dir : {}\".format(target_dir))\n",
        "\n",
        "    # generate training list\n",
        "    training_list_path = os.path.abspath(\"{dir}/{dir_name}/*.{ext}\".format(dir=target_dir, dir_name=dir_name, ext=ext))\n",
        "    files = sorted(glob.glob(training_list_path))\n",
        "    if len(files) == 0:\n",
        "      print(\"errore\")\n",
        "    return files\n",
        "\n",
        "\n",
        "def file_to_vector_array(file_name, n_mels=64, n_fft=1024, hop_length=512, power=2.0):\n",
        "    # 02 generate melspectrogram using librosa\n",
        "    y, sr = file_load(file_name)\n",
        "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, power=power)\n",
        "\n",
        "    # 03 convert melspectrogram to log mel energy\n",
        "    log_mel_spectrogram = 20.0 / power * numpy.log10(mel_spectrogram + sys.float_info.epsilon)\n",
        "\n",
        "    return log_mel_spectrogram\n",
        "\n",
        "  \n",
        "def list_to_vector_array(file_list, msg=\"calc...\", n_mels=64, n_fft=1024, hop_length=512, power=2.0, frames=10):\n",
        "    # iterate file_to_vector_array()\n",
        "    for idx in tqdm(range(len(file_list)), desc=msg):\n",
        "        vector_array = file_to_vector_array(file_list[idx], n_mels=n_mels, n_fft=n_fft, hop_length=hop_length, power=power)\n",
        "\n",
        "        if idx == 0:\n",
        "            dataset = numpy.zeros((len(file_list), n_mels, frames), float)\n",
        "        dataset[vector_array.shape[0] * idx: vector_array.shape[0] * (idx + 1), :] = vector_array\n",
        "    return dataset\n",
        "\n",
        "def key_by_id(item):\n",
        "  path_splitted = item.split(\"/\")\n",
        "  file_name = path_splitted[ len(path_splitted) - 1 ]\n",
        "  file_name_splitted = file_name.split(\"_\")\n",
        "  machine_id = file_name_splitted = file_name_splitted[2]\n",
        "  return machine_id\n",
        "\n",
        "def get_machine_id_list_for_test(target_dir,\n",
        "                                 dir_name=\"test\",\n",
        "                                 ext=\"wav\"):\n",
        "\n",
        "    # create test files\n",
        "    dir_path = os.path.abspath(\"{dir}/{dir_name}/*.{ext}\".format(dir=target_dir, dir_name=dir_name, ext=ext))\n",
        "    file_paths = sorted(glob.glob(dir_path))\n",
        "    # extract id\n",
        "    machine_id_list = sorted(list(set(itertools.chain.from_iterable(\n",
        "        [re.findall('id_[0-9][0-9]', ext_id) for ext_id in file_paths]))))\n",
        "    return machine_id_list\n",
        "\n",
        "def test_file_list_generator(target_dir,\n",
        "                             id_name,\n",
        "                             dir_name=\"test\",\n",
        "                             prefix_normal=\"normal\",\n",
        "                             prefix_anomaly=\"anomaly\",\n",
        "                             ext=\"wav\"):\n",
        "  \n",
        "    print(\"target_dir : {}\".format(target_dir+\"_\"+id_name))\n",
        "\n",
        "    normal_files = sorted(\n",
        "    glob.glob(\"{dir}/{dir_name}/{prefix_normal}_{id_name}*.{ext}\".format(dir=target_dir,\n",
        "                                                                                 dir_name=dir_name,\n",
        "                                                                                 prefix_normal=prefix_normal,\n",
        "                                                                                 id_name=id_name,\n",
        "                                                                                 ext=ext)))\n",
        "    normal_labels = numpy.zeros(len(normal_files))\n",
        "    anomaly_files = sorted(\n",
        "    glob.glob(\"{dir}/{dir_name}/{prefix_anomaly}_{id_name}*.{ext}\".format(dir=target_dir,\n",
        "                                                                                  dir_name=dir_name,\n",
        "                                                                                  prefix_anomaly=prefix_anomaly,\n",
        "                                                                                  id_name=id_name,\n",
        "                                                                                  ext=ext)))\n",
        "    anomaly_labels = numpy.ones(len(anomaly_files))\n",
        "    files = numpy.concatenate((normal_files, anomaly_files), axis=0)\n",
        "    labels = numpy.concatenate((normal_labels, anomaly_labels), axis=0)\n",
        "    print(\"test_file  num : {num}\".format(num=len(files)))\n",
        "    if len(files) == 0:\n",
        "        print(\"no_wav_file!!\")\n",
        "    print(\"\\n========================================\")\n",
        "\n",
        "    return files, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQLl5W5n7_T5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "13e67766-c3b6-4111-e284-19ac0ded978f"
      },
      "source": [
        "target_dir = \"/content/drive/MyDrive/test/slider\"\n",
        "\n",
        "machine_type = os.path.split(target_dir)[1]\n",
        "print(\"============== MODEL LOAD ==============\")\n",
        "# set model path\n",
        "model_file = \"/content/drive/MyDrive/models/IDC-LSTM-AE/slider/1/model_slider.h5\"\n",
        "\n",
        "# load model file\n",
        "if not os.path.exists(model_file):\n",
        "  print(\"{} model not found \".format(machine_type))\n",
        "  sys.exit(-1)\n",
        "model = keras.models.load_model(model_file, custom_objects={'CustomModel': CustomModel, 'mse':mse_metric})\n",
        "# model.summary()\n",
        "\n",
        "machine_id_list = get_machine_id_list_for_test(target_dir)\n",
        "\n",
        "# initialize lines in csv for AUC and pAUC\n",
        "csv_lines = []\n",
        "\n",
        "csv_lines.append([machine_type])\n",
        "csv_lines.append([\"id\", \"AUC\", \"pAUC\"])\n",
        "performance = []\n",
        "\n",
        "for id_str in machine_id_list:\n",
        "  # load test file\n",
        "\n",
        "  id_num = id_str.split(\"_\")[1]\n",
        "\n",
        "  # Definizione della label \"match\" da utilizzare in fase di testing e del min e max da utilizzare per la normalizzazione\n",
        "  # i min e max sono stati calcolati a partire dai dati di training.\n",
        "  if id_num == \"00\":\n",
        "    mean = mean_00\n",
        "    std = std_00\n",
        "  if id_num == \"02\":\n",
        "    mean = mean_02\n",
        "    std = std_02\n",
        "  if id_num == \"04\":\n",
        "    mean = mean_04\n",
        "    std = std_04\n",
        "  if id_num == \"06\":\n",
        "    mean = mean_06\n",
        "    std = std_06\n",
        "\n",
        "  test_files, y_true = test_file_list_generator(target_dir, id_str)\n",
        "\n",
        "  # setup anomaly score file path\n",
        "  anomaly_score_csv = \"/content/drive/MyDrive/models/IDC-LSTM-AE/slider/1/anomaly_score_{machine_type}_{id_str}.csv\".format(machine_type=machine_type, id_str=id_str)\n",
        "  anomaly_score_list = []\n",
        "\n",
        "  print(\"\\n============== BEGIN TEST FOR A MACHINE ID {id} ==============\".format(id=id_num))\n",
        "\n",
        "  y_pred = [0. for k in test_files]\n",
        "\n",
        "  for file_idx, file_path in tqdm(enumerate(test_files), total=len(test_files)):\n",
        "\n",
        "    # Estrazione spettrogramma audio test\n",
        "    data = file_to_vector_array(file_path, n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH, power=POWER)\n",
        "    # Normalizzazione spettrogramma di test\n",
        "    data = ( data - mean ) / (std)\n",
        "\n",
        "    #print(data_aug_transpose.shape)\n",
        "\n",
        "    data_aug = numpy.zeros((22, 128, 32))\n",
        "    index = 0\n",
        "    i = 0\n",
        "    while (i+32) <= 313:\n",
        "      vector_i = numpy.zeros((128,32))\n",
        "      for j in range(0,128):\n",
        "        vector_i[j] = data[j][i:i+32]\n",
        "      data_aug[index] = vector_i\n",
        "      index += 1\n",
        "      i = i+13\n",
        "\n",
        "    data_aug_transpose = numpy.zeros((len(data_aug), 32, 128))\n",
        "    index = 0\n",
        "    for elem in data_aug:\n",
        "      data_aug_transpose[index] = elem.T\n",
        "      index += 1\n",
        "    #print(data_aug_transpose.shape)\n",
        "    \n",
        "    # Calcolo dell'errore medio sulle frame estratte dallo spettrogramma\n",
        "    elem_error = []\n",
        "    for elem in data_aug_transpose:\n",
        "      predicted = model.predict(elem.reshape(1,32,128))\n",
        "      errors = numpy.mean(numpy.square(elem - predicted), axis=1)\n",
        "      elem_error.append(numpy.mean(errors))\n",
        "\n",
        "    # Log dell'errore associato all'istanza di test\n",
        "    y_pred[file_idx] = numpy.mean(errors)\n",
        "    anomaly_score_list.append([os.path.basename(file_path), y_pred[file_idx]])\n",
        "  \n",
        "  save_csv(save_file_path=anomaly_score_csv, save_data=anomaly_score_list)\n",
        "    \n",
        "  # Calcolo AUC e pAUC per i dati con un certo ID_0x\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  p_auc = metrics.roc_auc_score(y_true, y_pred, max_fpr=0.1)\n",
        "  csv_lines.append([id_str.split(\"_\", 1)[1], auc, p_auc])\n",
        "  performance.append([auc, p_auc])\n",
        "  print(\"AUC : {}\".format(auc))\n",
        "  print(\"pAUC : {}\".format(p_auc))\n",
        "\n",
        "  print(\"\\n============ END OF TEST FOR A MACHINE ID ============\")\n",
        "\n",
        "# Stampa di AUC e pAUC medi su tutti i dati di test (media di AUC e pAUC sui vari ID).\n",
        "print(\"\\n============ AVERAGE PERFORMANCES ============\")\n",
        "averaged_performance = numpy.mean(numpy.array(performance, dtype=float), axis=0)\n",
        "csv_lines.append([\"Average\"] + list(averaged_performance))\n",
        "csv_lines.append([])\n",
        "print(averaged_performance)\n",
        "\n",
        "result_path = \"/content/drive/MyDrive/models/IDC-LSTM-AE/slider/1/anomaly_score_avg.csv\"\n",
        "save_csv(save_file_path=result_path, save_data=csv_lines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============== MODEL LOAD ==============\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-3d0c3ef528e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} model not found \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmachine_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m# model.summary()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    205\u001b[0m           (isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\n\u001b[1;32m    206\u001b[0m         return hdf5_format.load_model_from_hdf5(filepath, custom_objects,\n\u001b[0;32m--> 207\u001b[0;31m                                                 compile)\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m       \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     model = model_config_lib.model_from_config(model_config,\n\u001b[0;32m--> 184\u001b[0;31m                                                custom_objects=custom_objects)\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# set weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/saving/model_config.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     62\u001b[0m                     '`Sequential.from_config(config)`?')\n\u001b[1;32m     63\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/serialization.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    175\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLOCAL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL_OBJECTS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m       printable_module_name='layer')\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midentifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     (cls, cls_config) = class_and_config_for_serialized_keras_object(\n\u001b[0;32m--> 347\u001b[0;31m         config, module_objects, custom_objects, printable_module_name)\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'from_config'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mclass_and_config_for_serialized_keras_object\u001b[0;34m(config, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    294\u001b[0m   \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_registered_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unknown '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mprintable_module_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m   \u001b[0mcls_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unknown layer: CustomModel"
          ]
        }
      ]
    }
  ]
}