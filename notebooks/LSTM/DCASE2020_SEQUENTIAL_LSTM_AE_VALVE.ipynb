{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCASE2020_SEQUENTIAL_LSTM_AE_VALVE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-61Nns1Qnk5d"
      },
      "source": [
        "# IMPORT AND DATA LOADING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDG8iAV_W2fa",
        "outputId": "16f8b33d-5af4-4583-b104-fb5ea7a8b017"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9uFgr4mYC5v"
      },
      "source": [
        "# import necessari\n",
        "import librosa\n",
        "import numpy\n",
        "import sys\n",
        "import os\n",
        "import glob\n",
        "import itertools\n",
        "import re\n",
        "import pickle\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.models\n",
        "import tensorflow.keras.backend as K\n",
        "import keras.optimizers\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "from keras.layers import LSTM, RepeatVector, TimeDistributed, Dense, Dropout, Activation, Input, Add, Multiply\n",
        "from tqdm import tqdm\n",
        "from itertools import groupby\n",
        "from keras.utils import to_categorical\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# costanti \n",
        "ALPHA = 0.75\n",
        "N_MELS = 128\n",
        "HOP_LENGTH = 512\n",
        "N_FFT = 1024\n",
        "POWER = 2.0\n",
        "FRAME_NUMS = 313\n",
        "FRAMES = 10\n",
        "VAL = 0.05\n",
        "\n",
        "# FEATURES EXTRACTION\n",
        "\n",
        "# Loading da Google Drive\n",
        "train_data = numpy.load(\"/content/drive/MyDrive/DCASE_DATA_EXTRACTED/train/training_valve.npy\")\n",
        "grouped_list_by_machine_id = pickle.load( open( \"/content/drive/MyDrive/DCASE_DATA_EXTRACTED/train/training_valve_grouped_list.npy\", \"rb\" ) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDq3dIgZxVEC",
        "outputId": "9967eaa1-ba64-4971-e9d5-5934369b1cab"
      },
      "source": [
        "# GENERAZIONE DELLE LABELS\n",
        "# One-hot encoding\n",
        "label = []\n",
        "choices = []\n",
        "for i in range(0, len(grouped_list_by_machine_id)):\n",
        "  for j in range(0, len(grouped_list_by_machine_id[i])):\n",
        "    machine_id = grouped_list_by_machine_id[i][j].split('/')[7].split('_')[2]\n",
        "    #print(grouped_list_by_machine_id[i][j].split('/')[7])\n",
        "    random_choice = numpy.random.choice([\"match\", \"non_match\"], p = [ALPHA, 1-ALPHA]) \n",
        "\n",
        "    if machine_id == '00':\n",
        "      if random_choice == \"match\":\n",
        "        to_append = [1,0,0,0]\n",
        "      else: \n",
        "        not_match_label = numpy.random.choice([1, 2, 3]) \n",
        "        if not_match_label == 1:\n",
        "          to_append = [0,1,0,0]\n",
        "        elif not_match_label == 2:\n",
        "          to_append = [0,0,1,0]\n",
        "        else: \n",
        "          to_append = [0,0,0,1]\n",
        "\n",
        "    elif machine_id == '02': \n",
        "      if random_choice == \"match\":\n",
        "        to_append = [0,1,0,0]\n",
        "      else: \n",
        "        not_match_label = numpy.random.choice( [ 1, 2, 3] ) \n",
        "        if not_match_label == 1:\n",
        "          to_append = [1,0,0,0]\n",
        "        elif not_match_label == 2:\n",
        "          to_append = [0,0,1,0]\n",
        "        else: \n",
        "          to_append = [0,0,0,1]\n",
        "\n",
        "    elif machine_id == \"04\":\n",
        "      if random_choice == \"match\":\n",
        "        to_append = [0,0,1,0]\n",
        "      else: \n",
        "        not_match_label = numpy.random.choice( [ 1, 2, 3] ) \n",
        "        if not_match_label == 1:\n",
        "          to_append = [1,0,0,0]\n",
        "        elif not_match_label == 2:\n",
        "          to_append = [0,1,0,0]\n",
        "        else: \n",
        "          to_append = [0,0,0,1]\n",
        "\n",
        "    elif machine_id == \"06\":\n",
        "      if random_choice == \"match\":\n",
        "        to_append = [0,0,0,1]\n",
        "      else: \n",
        "        not_match_label = numpy.random.choice( [ 1, 2, 3] ) \n",
        "        if not_match_label == 1:\n",
        "          to_append = [1,0,0,0]\n",
        "        elif not_match_label == 2:\n",
        "          to_append = [0,1,0,0]\n",
        "        else: \n",
        "          to_append = [0,0,1,0]\n",
        "    \n",
        "    label.append(to_append) # Append della label associata a ciascuno spettrogramma\n",
        "    choices.append(random_choice) # Append della choice utilizzata per associare la label\n",
        "                                  # La choice sar√† utile in fase di addestramento per capire che tipo di loss calcolare\n",
        "\n",
        "# Trasformazione in numpy.array     \n",
        "label = numpy.asarray(label)\n",
        "choices = numpy.asarray(choices)\n",
        "print(label.shape)\n",
        "print(choices.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3291, 4)\n",
            "(3291,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czuXNBN8CelQ",
        "outputId": "97d5730f-cc55-476b-e298-3d44ac437345"
      },
      "source": [
        "print(len(grouped_list_by_machine_id[0]))\n",
        "print(len(grouped_list_by_machine_id[1]))\n",
        "print(len(grouped_list_by_machine_id[2]))\n",
        "print(len(grouped_list_by_machine_id[3]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "891\n",
            "608\n",
            "900\n",
            "892\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQagajQlV65x",
        "outputId": "58396865-f64b-448e-c6ce-01d0559b8087"
      },
      "source": [
        "# Estrazione spettrogrammi divisi per ID\n",
        "id_00 = train_data[0:891]\n",
        "label_00 = label[0:891]\n",
        "choices_00 = choices[0:891]\n",
        "\n",
        "id_02 = train_data[891:1499]\n",
        "label_02 = label[891:1499]\n",
        "choices_02 = choices[891:1499]\n",
        "\n",
        "id_04 = train_data[1499:2399]\n",
        "label_04 = label[1499:2399]\n",
        "choices_04 = choices[1499:2399]\n",
        "\n",
        "id_06 = train_data[2399:3291]\n",
        "label_06 = label[2399:3291]\n",
        "choices_06 = choices[2399:3291]\n",
        "\n",
        "id_00_training, \\\n",
        "id_00_validation, \\\n",
        "label_00_train, \\\n",
        "label_00_validation, \\\n",
        "choices_00_train, \\\n",
        "choices_00_validation = train_test_split(id_00, label_00, choices_00, test_size=VAL, random_state=42)\n",
        "\n",
        "id_02_training, \\\n",
        "id_02_validation, \\\n",
        "label_02_train, \\\n",
        "label_02_validation, \\\n",
        "choices_02_train, \\\n",
        "choices_02_validation = train_test_split(id_02, label_02, choices_02, test_size=VAL, random_state=42)\n",
        "\n",
        "id_04_training, \\\n",
        "id_04_validation, \\\n",
        "label_04_train, \\\n",
        "label_04_validation, \\\n",
        "choices_04_train, \\\n",
        "choices_04_validation = train_test_split(id_04, label_04, choices_04, test_size=VAL, random_state=42)\n",
        "\n",
        "id_06_training, \\\n",
        "id_06_validation, \\\n",
        "label_06_train, \\\n",
        "label_06_validation, \\\n",
        "choices_06_train, \\\n",
        "choices_06_validation = train_test_split(id_06, label_06, choices_06, test_size=VAL, random_state=42)\n",
        "\n",
        "# Normalization ID_00\n",
        "id_00_norm = numpy.empty_like(id_00_training)\n",
        "mean_00 = numpy.mean(id_00_training)\n",
        "std_00 = numpy.std(id_00_training)\n",
        "id_00_norm = (id_00_training - mean_00) / (std_00)\n",
        "id_00_norm_validation = (id_00_validation - mean_00) / (std_00)\n",
        "\n",
        "# Normalization ID_02\n",
        "id_02_norm = numpy.empty_like(id_02_training)\n",
        "mean_02 = numpy.mean(id_02_training)\n",
        "std_02 = numpy.std(id_02_training)\n",
        "id_02_norm = (id_02_training - mean_02) / (std_02)\n",
        "id_02_norm_validation = (id_02_validation - mean_02) / (std_02)\n",
        "\n",
        "# Normalization ID_04\n",
        "id_04_norm = numpy.empty_like(id_04_training)\n",
        "mean_04 = numpy.mean(id_04_training)\n",
        "std_04 = numpy.std(id_04_training)\n",
        "id_04_norm = (id_04_training - mean_04) / (std_04)\n",
        "id_04_norm_validation = (id_04_validation - mean_04) / (std_04)\n",
        "\n",
        "# Normalization ID_06\n",
        "id_06_norm = numpy.empty_like(id_06_training)\n",
        "mean_06 = numpy.mean(id_06_training)\n",
        "std_06 = numpy.std(id_06_training)\n",
        "id_06_norm = (id_06_training - mean_06) / (std_06)\n",
        "id_06_norm_validation = (id_06_validation - mean_06) / (std_06)\n",
        "\n",
        "print(\"==== DATA ====\")\n",
        "total_training = numpy.concatenate([id_00_norm, id_02_norm, id_04_norm, id_06_norm])\n",
        "print(total_training.shape)\n",
        "total_validation = numpy.concatenate([id_00_norm_validation, id_02_norm_validation, id_04_norm_validation, id_06_norm_validation])\n",
        "print(total_validation.shape)\n",
        "\n",
        "print(\"==== LABELS ====\")\n",
        "total_training_label = numpy.concatenate([label_00_train, label_02_train, label_04_train, label_06_train])\n",
        "print(total_training_label.shape)\n",
        "total_validation_label = numpy.concatenate([label_00_validation, label_02_validation, label_04_validation, label_06_validation])\n",
        "print(total_validation_label.shape)\n",
        "\n",
        "print(\"==== CHOICES ====\")\n",
        "total_training_choices = numpy.concatenate([choices_00_train, choices_02_train, choices_04_train, choices_06_train])\n",
        "print(total_training_choices.shape)\n",
        "total_validation_choices = numpy.concatenate([choices_00_validation, choices_02_validation, choices_04_validation, choices_06_validation])\n",
        "print(total_validation_choices.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==== DATA ====\n",
            "(3125, 128, 313)\n",
            "(166, 128, 313)\n",
            "==== LABELS ====\n",
            "(3125, 4)\n",
            "(166, 4)\n",
            "==== CHOICES ====\n",
            "(3125,)\n",
            "(166,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_KQ1vZ2Yh1x",
        "outputId": "91053ffd-a6f8-4550-ea72-54e253565961"
      },
      "source": [
        "training_aug = numpy.zeros((len(total_training)*16, 128, 32)) # Dataset utilizzato per il training\n",
        "index = 0\n",
        "for vector_array in total_training:\n",
        "  i = 0\n",
        "  while (i+32) <= 313:\n",
        "    vector_i = numpy.zeros((128,32))\n",
        "    for j in range(0,128):\n",
        "      vector_i[j] = vector_array[j][i:i+32]\n",
        "    training_aug[index] = vector_i\n",
        "    index += 1\n",
        "    i = i+18\n",
        "\n",
        "validation_aug = numpy.zeros((len(total_validation)*16, 128, 32)) # Dataset utilizzato per il training\n",
        "index = 0\n",
        "for vector_array in total_validation:\n",
        "  i = 0\n",
        "  while (i+32) <= 313:\n",
        "    vector_i = numpy.zeros((128,32))\n",
        "    for j in range(0,128):\n",
        "      vector_i[j] = vector_array[j][i:i+32]\n",
        "    validation_aug[index] = vector_i\n",
        "    index += 1\n",
        "    i = i+18\n",
        "\n",
        "training_aug_transpose = numpy.zeros((len(training_aug), 32, 128))\n",
        "index = 0\n",
        "for elem in training_aug:\n",
        "  training_aug_transpose[index] = elem.T\n",
        "  index += 1\n",
        "print(training_aug_transpose.shape)\n",
        "\n",
        "validation_aug_transpose = numpy.zeros((len(validation_aug), 32, 128))\n",
        "index = 0\n",
        "for elem in validation_aug:\n",
        "  validation_aug_transpose[index] = elem.T\n",
        "  index += 1\n",
        "print(validation_aug_transpose.shape)\n",
        "\n",
        "\n",
        "####### LABELS ######\n",
        "# Associazione della label associata a ciascun spettrogramma a ciascuno dei frame estratto da esso.\n",
        "training_labels = []\n",
        "for elem in total_training_label:\n",
        "  if numpy.array_equal(elem, numpy.asarray([1,0,0,0])) :\n",
        "    for i in range(16):\n",
        "      training_labels.append([1,0,0,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,1,0,0])):\n",
        "    for i in range(16):\n",
        "      training_labels.append([0,1,0,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,0,1,0])):\n",
        "    for i in range(16):\n",
        "      training_labels.append([0,0,1,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,0,0,1])):\n",
        "    for i in range(16):\n",
        "      training_labels.append([0,0,0,1])\n",
        "\n",
        "validation_labels = []\n",
        "for elem in total_validation_label:\n",
        "  if numpy.array_equal(elem, numpy.asarray([1,0,0,0])) :\n",
        "    for i in range(16):\n",
        "      validation_labels.append([1,0,0,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,1,0,0])):\n",
        "    for i in range(16):\n",
        "      validation_labels.append([0,1,0,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,0,1,0])):\n",
        "    for i in range(16):\n",
        "      validation_labels.append([0,0,1,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,0,0,1])):\n",
        "    for i in range(16):\n",
        "      validation_labels.append([0,0,0,1])\n",
        "\n",
        "training_labels = numpy.asarray(training_labels) #¬†Dataset utilizzato per il training\n",
        "validation_labels = numpy.asarray(validation_labels) #¬†Dataset utilizzato per il training\n",
        "#####################\n",
        "\n",
        "\n",
        "####### CHOICES ######\n",
        "# Associazione della choice associata a ciascun spettrogramma a ciascuno dei frame estratto da esso. \n",
        "training_choices = []\n",
        "for elem in total_training_choices:\n",
        "  if numpy.array_equal(elem, numpy.asarray(\"match\")) :\n",
        "    for i in range(16):\n",
        "      training_choices.append(\"match\")\n",
        "  elif numpy.array_equal(elem, numpy.asarray(\"non_match\")):\n",
        "    for i in range(16):\n",
        "      training_choices.append(\"non_match\")\n",
        "\n",
        "validation_choices = []\n",
        "for elem in total_validation_choices:\n",
        "  if numpy.array_equal(elem, numpy.asarray(\"match\")) :\n",
        "    for i in range(16):\n",
        "      validation_choices.append(\"match\")\n",
        "  elif numpy.array_equal(elem, numpy.asarray(\"non_match\")):\n",
        "    for i in range(16):\n",
        "      validation_choices.append(\"non_match\")\n",
        "\n",
        "training_choices = numpy.asarray(training_choices) #¬†Dataset utilizzato per il training\n",
        "validation_choices = numpy.asarray(validation_choices) #¬†Dataset utilizzato per il training\n",
        "######################\n",
        "\n",
        "print(\"==== DATA ====\")\n",
        "print(training_aug_transpose.shape)\n",
        "print(validation_aug_transpose.shape)\n",
        "\n",
        "print(\"==== LABELS ====\")\n",
        "print(training_labels.shape)\n",
        "print(validation_labels.shape)\n",
        "\n",
        "print(\"==== CHOICES ====\")\n",
        "print(training_choices.shape)\n",
        "print(validation_choices.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 32, 128)\n",
            "(2656, 32, 128)\n",
            "==== DATA ====\n",
            "(50000, 32, 128)\n",
            "(2656, 32, 128)\n",
            "==== LABELS ====\n",
            "(50000, 4)\n",
            "(2656, 4)\n",
            "==== CHOICES ====\n",
            "(50000,)\n",
            "(2656,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcCjeEMHnvwG"
      },
      "source": [
        "# KERAS MODEL DEFINITION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISWY6OjXbZNd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8db512c-a6cd-4524-c951-0a7832bc4e8f"
      },
      "source": [
        "timesteps = 32\n",
        "num_features = 128\n",
        "\n",
        "input_Spect = Input(shape = [timesteps, num_features])\n",
        "input_Label = Input(shape = [4,])\n",
        "\n",
        "x = LSTM(64, \n",
        "        batch_input_shape=(None, timesteps, num_features), \n",
        "        return_sequences=True, name='encoder_1', \n",
        "        kernel_constraint = max_norm(1), \n",
        "        recurrent_constraint = max_norm(1), \n",
        "        bias_constraint = max_norm(1))(input_Spect)\n",
        "\n",
        "x = LSTM(32, \n",
        "        return_sequences=True, name='encoder_2', \n",
        "        kernel_constraint = max_norm(1), \n",
        "        recurrent_constraint = max_norm(1), \n",
        "        bias_constraint = max_norm(1))(x)\n",
        "\n",
        "x = LSTM(16, \n",
        "        return_sequences=False, \n",
        "        name='encoder_3', \n",
        "        kernel_constraint = max_norm(1), \n",
        "        recurrent_constraint = max_norm(1), \n",
        "        bias_constraint = max_norm(1))(x)\n",
        "\n",
        "# Second Branch - Conditioning Feed Forward Neural Network\n",
        "m = Dense(16)(input_Label)\n",
        "m = Activation('sigmoid')(m)\n",
        "q = Dense(16)(input_Label)\n",
        "\n",
        "# Encoded Input Conditioning\n",
        "m = Multiply()([x, m])\n",
        "encoded_input_conditioned = Add()([q, m]) # Input da passare al decoder\n",
        "\n",
        "x = RepeatVector(timesteps, name='encoder_decoder_bridge')(encoded_input_conditioned)\n",
        "\n",
        "x = LSTM(16, \n",
        "        return_sequences=True, name='decoder_1', \n",
        "        kernel_constraint = max_norm(1), \n",
        "        recurrent_constraint = max_norm(1), \n",
        "        bias_constraint = max_norm(1))(x)\n",
        "\n",
        "x = LSTM(32, \n",
        "        return_sequences=True, name='decoder_2', \n",
        "        kernel_constraint = max_norm(1), \n",
        "        recurrent_constraint = max_norm(1), \n",
        "        bias_constraint = max_norm(1))(x)\n",
        "\n",
        "x = LSTM(64, \n",
        "        return_sequences=True, name='decoder_3', \n",
        "        kernel_constraint = max_norm(1), \n",
        "        recurrent_constraint = max_norm(1), \n",
        "        bias_constraint = max_norm(1))(x)\n",
        "\n",
        "x = TimeDistributed(Dense(num_features))(x)\n",
        "\n",
        "loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "mse_metric = keras.metrics.MeanSquaredError(name=\"mse\")\n",
        "\n",
        "class CustomModel(tensorflow.keras.Model):\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [loss_tracker, mse_metric]\n",
        "\n",
        "    def test_step(self, data):\n",
        "        # Unpack the data\n",
        "        x, y = data\n",
        "        # Compute predictions\n",
        "        y_pred = self([x[0],x[1]], training=False)\n",
        "        # Indici match\n",
        "        match = tf.where ( tf.equal(x[2][:], \"match\") )\n",
        "        # Dati match\n",
        "        data_match = K.gather(y, match)\n",
        "        # Separazione dei dati PREDETTI sulla base degli indici relativi a match/non_match\n",
        "        # Dati match\n",
        "        pred_match = K.gather(y_pred, match)\n",
        "\n",
        "        # Update metrica\n",
        "        mse_metric.update_state(data_match, pred_match)\n",
        "\n",
        "        return {\"mse\": mse_metric.result()}\n",
        "    \n",
        "    def train_step(self, data):\n",
        "          # Unpack the data. Its structure depends on your model and on what you pass to `fit()`.\n",
        "          x, y = data\n",
        "\n",
        "          # Vettore C utilizzato per il calcolo della loss in caso di non_match\n",
        "          C = 5 \n",
        "          # Valore di probabilit√† utilizzato come peso\n",
        "          ALPHA = 0.75 \n",
        "\n",
        "          # Indici match\n",
        "          match = tf.where ( tf.equal(x[2][:], \"match\") )\n",
        "\n",
        "          # Indici non_match\n",
        "          not_match = tf.where ( tf.equal(x[2][:], \"non_match\") )\n",
        "\n",
        "          # Dati match\n",
        "          data_match = K.gather(y, match)\n",
        "\n",
        "          with tf.GradientTape() as tape:\n",
        "              y_pred = self([x[0],x[1]], training=True)  # Forward pass\n",
        "\n",
        "              # Separazione dei dati PREDETTI sulla base degli indici relativi a match/non_match\n",
        "              # Dati match\n",
        "              pred_match = K.gather(y_pred, match)\n",
        "              # Dati non match\n",
        "              pred_not_match = K.gather(y_pred, not_match) \n",
        "\n",
        "              loss_m = K.mean(keras.losses.mean_squared_error(data_match, pred_match)) + 1e-6  # Calcolo Loss Match\n",
        "              loss_nm = K.mean(keras.losses.mean_squared_error(C,pred_not_match)) + 1e-6     # Calcolo Loss Non_Match\n",
        "\n",
        "              loss = ALPHA * loss_m + (1 - ALPHA) * loss_nm     # loss utilizzata per l'update dei pesi\n",
        "\n",
        "          # Compute gradients\n",
        "          trainable_vars = self.trainable_variables\n",
        "          gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "          # Update weights\n",
        "          self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "          # Compute our own metrics\n",
        "          loss_tracker.update_state(loss)\n",
        "          mse_metric.update_state(y, y_pred)\n",
        "          return {\"loss\": loss_tracker.result(), \"mse\": mse_metric.result()}\n",
        "\n",
        "model = CustomModel(inputs=(input_Spect, input_Label), outputs = x)\n",
        "model.compile(metrics=[\"mse\"], optimizer = \"adam\")\n",
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"custom_model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 128)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 4)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder_1 (LSTM)                (None, 32, 64)       49408       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "encoder_2 (LSTM)                (None, 32, 32)       12416       encoder_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 16)           80          input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "encoder_3 (LSTM)                (None, 16)           3136        encoder_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 16)           0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 16)           80          input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "multiply (Multiply)             (None, 16)           0           encoder_3[0][0]                  \n",
            "                                                                 activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 16)           0           dense_1[0][0]                    \n",
            "                                                                 multiply[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "encoder_decoder_bridge (RepeatV (None, 32, 16)       0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "decoder_1 (LSTM)                (None, 32, 16)       2112        encoder_decoder_bridge[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "decoder_2 (LSTM)                (None, 32, 32)       6272        decoder_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "decoder_3 (LSTM)                (None, 32, 64)       24832       decoder_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, 32, 128)      8320        decoder_3[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 106,656\n",
            "Trainable params: 106,656\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbvCY_H6hDZA",
        "outputId": "d8064601-c736-4c18-e9a6-322ef88496bf"
      },
      "source": [
        "history = model.fit([training_aug_transpose, training_labels, training_choices],\n",
        "                    training_aug_transpose, \n",
        "                    epochs=100, \n",
        "                    validation_data=([validation_aug_transpose, validation_labels, validation_choices], validation_aug_transpose), \n",
        "                    batch_size=512)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "98/98 [==============================] - 47s 71ms/step - loss: 5.6414 - mse: 1.7370 - val_mse: 1.9049\n",
            "Epoch 2/100\n",
            "75/98 [=====================>........] - ETA: 0s - loss: 5.1273 - mse: 1.9681"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZ8P46b6tq11"
      },
      "source": [
        "model.save('/content/drive/MyDrive/models/IDC-LSTM-AE/valve/1/model_valve.h5') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "so_cAj-Ro0Ti"
      },
      "source": [
        "with open('/content/drive/MyDrive/models/IDC-LSTM-AE/valve/1/trainHistoryDict', 'wb') as file_pi:\n",
        "    pickle.dump(history.history, file_pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-Cp082Xn2yj"
      },
      "source": [
        "# TESTING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y42D3ejetbrN"
      },
      "source": [
        "import csv\n",
        "\n",
        "def save_csv(save_file_path,\n",
        "             save_data):\n",
        "    with open(save_file_path, \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f, lineterminator='\\n')\n",
        "        writer.writerows(save_data)\n",
        "\n",
        "\n",
        "# load dataset\n",
        "def select_dirs(path):\n",
        "    dir_path = os.path.abspath(path)\n",
        "    dirs = sorted(glob.glob(dir_path))\n",
        "    return dirs\n",
        "\n",
        "def file_load(wav_name, mono=False):\n",
        "    try:\n",
        "        return librosa.load(wav_name, sr=None, mono=mono)\n",
        "    except:\n",
        "        logger.error(\"file_broken or not exists!! : {}\".format(wav_name))\n",
        "\n",
        "def file_list_generator(target_dir, dir_name=\"train\", ext=\"wav\"):\n",
        "    print(\"target_dir : {}\".format(target_dir))\n",
        "\n",
        "    # generate training list\n",
        "    training_list_path = os.path.abspath(\"{dir}/{dir_name}/*.{ext}\".format(dir=target_dir, dir_name=dir_name, ext=ext))\n",
        "    files = sorted(glob.glob(training_list_path))\n",
        "    if len(files) == 0:\n",
        "      print(\"errore\")\n",
        "    return files\n",
        "\n",
        "\n",
        "def file_to_vector_array(file_name, n_mels=64, n_fft=1024, hop_length=512, power=2.0):\n",
        "    # 02 generate melspectrogram using librosa\n",
        "    y, sr = file_load(file_name)\n",
        "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, power=power)\n",
        "\n",
        "    # 03 convert melspectrogram to log mel energy\n",
        "    log_mel_spectrogram = 20.0 / power * numpy.log10(mel_spectrogram + sys.float_info.epsilon)\n",
        "\n",
        "    return log_mel_spectrogram\n",
        "\n",
        "  \n",
        "def list_to_vector_array(file_list, msg=\"calc...\", n_mels=64, n_fft=1024, hop_length=512, power=2.0, frames=10):\n",
        "    # iterate file_to_vector_array()\n",
        "    for idx in tqdm(range(len(file_list)), desc=msg):\n",
        "        vector_array = file_to_vector_array(file_list[idx], n_mels=n_mels, n_fft=n_fft, hop_length=hop_length, power=power)\n",
        "\n",
        "        if idx == 0:\n",
        "            dataset = numpy.zeros((len(file_list), n_mels, frames), float)\n",
        "        dataset[vector_array.shape[0] * idx: vector_array.shape[0] * (idx + 1), :] = vector_array\n",
        "    return dataset\n",
        "\n",
        "def key_by_id(item):\n",
        "  path_splitted = item.split(\"/\")\n",
        "  file_name = path_splitted[ len(path_splitted) - 1 ]\n",
        "  file_name_splitted = file_name.split(\"_\")\n",
        "  machine_id = file_name_splitted = file_name_splitted[2]\n",
        "  return machine_id\n",
        "\n",
        "def get_machine_id_list_for_test(target_dir,\n",
        "                                 dir_name=\"test\",\n",
        "                                 ext=\"wav\"):\n",
        "\n",
        "    # create test files\n",
        "    dir_path = os.path.abspath(\"{dir}/{dir_name}/*.{ext}\".format(dir=target_dir, dir_name=dir_name, ext=ext))\n",
        "    file_paths = sorted(glob.glob(dir_path))\n",
        "    # extract id\n",
        "    machine_id_list = sorted(list(set(itertools.chain.from_iterable(\n",
        "        [re.findall('id_[0-9][0-9]', ext_id) for ext_id in file_paths]))))\n",
        "    return machine_id_list\n",
        "\n",
        "def test_file_list_generator(target_dir,\n",
        "                             id_name,\n",
        "                             dir_name=\"test\",\n",
        "                             prefix_normal=\"normal\",\n",
        "                             prefix_anomaly=\"anomaly\",\n",
        "                             ext=\"wav\"):\n",
        "  \n",
        "    print(\"target_dir : {}\".format(target_dir+\"_\"+id_name))\n",
        "\n",
        "    normal_files = sorted(\n",
        "    glob.glob(\"{dir}/{dir_name}/{prefix_normal}_{id_name}*.{ext}\".format(dir=target_dir,\n",
        "                                                                                 dir_name=dir_name,\n",
        "                                                                                 prefix_normal=prefix_normal,\n",
        "                                                                                 id_name=id_name,\n",
        "                                                                                 ext=ext)))\n",
        "    normal_labels = numpy.zeros(len(normal_files))\n",
        "    anomaly_files = sorted(\n",
        "    glob.glob(\"{dir}/{dir_name}/{prefix_anomaly}_{id_name}*.{ext}\".format(dir=target_dir,\n",
        "                                                                                  dir_name=dir_name,\n",
        "                                                                                  prefix_anomaly=prefix_anomaly,\n",
        "                                                                                  id_name=id_name,\n",
        "                                                                                  ext=ext)))\n",
        "    anomaly_labels = numpy.ones(len(anomaly_files))\n",
        "    files = numpy.concatenate((normal_files, anomaly_files), axis=0)\n",
        "    labels = numpy.concatenate((normal_labels, anomaly_labels), axis=0)\n",
        "    print(\"test_file  num : {num}\".format(num=len(files)))\n",
        "    if len(files) == 0:\n",
        "        print(\"no_wav_file!!\")\n",
        "    print(\"\\n========================================\")\n",
        "\n",
        "    return files, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQLl5W5n7_T5"
      },
      "source": [
        "target_dir = \"/content/drive/MyDrive/test/valve\"\n",
        "\n",
        "machine_type = os.path.split(target_dir)[1]\n",
        "print(\"============== MODEL LOAD ==============\")\n",
        "# set model path\n",
        "model_file = \"/content/drive/MyDrive/models/IDC-LSTM-AE/valve/1/model_valve.h5\"\n",
        "\n",
        "# load model file\n",
        "if not os.path.exists(model_file):\n",
        "  print(\"{} model not found \".format(machine_type))\n",
        "  sys.exit(-1)\n",
        "model = keras.models.load_model(model_file, custom_objects={'CustomModel': CustomModel, 'mse':mse_metric})\n",
        "# model.summary()\n",
        "\n",
        "machine_id_list = get_machine_id_list_for_test(target_dir)\n",
        "\n",
        "# initialize lines in csv for AUC and pAUC\n",
        "csv_lines = []\n",
        "\n",
        "csv_lines.append([machine_type])\n",
        "csv_lines.append([\"id\", \"AUC\", \"pAUC\"])\n",
        "performance = []\n",
        "\n",
        "for id_str in machine_id_list:\n",
        "  # load test file\n",
        "\n",
        "  id_num = id_str.split(\"_\")[1]\n",
        "\n",
        "  # Definizione della label \"match\" da utilizzare in fase di testing e del min e max da utilizzare per la normalizzazione\n",
        "  # i min e max sono stati calcolati a partire dai dati di training.\n",
        "  if id_num == \"00\":\n",
        "    match_labels = numpy.asarray([1,0,0,0])\n",
        "    mean = mean_00\n",
        "    std = std_00\n",
        "  if id_num == \"02\":\n",
        "    match_labels = numpy.asarray([0,1,0,0])\n",
        "    mean = mean_02\n",
        "    std = std_02\n",
        "  if id_num == \"04\":\n",
        "    match_labels = numpy.asarray([0,0,1,0])\n",
        "    mean = mean_04\n",
        "    std = std_04\n",
        "  if id_num == \"06\":\n",
        "    match_labels = numpy.asarray([0,0,0,1])\n",
        "    mean = mean_06\n",
        "    std = std_06\n",
        "\n",
        "  test_files, y_true = test_file_list_generator(target_dir, id_str)\n",
        "\n",
        "  # setup anomaly score file path\n",
        "  anomaly_score_csv = \"/content/drive/MyDrive/models/IDC-LSTM-AE/valve/1/anomaly_score_{machine_type}_{id_str}.csv\".format(machine_type=machine_type, id_str=id_str)\n",
        "  anomaly_score_list = []\n",
        "\n",
        "  print(\"\\n============== BEGIN TEST FOR A MACHINE ID {id} ==============\".format(id=id_num))\n",
        "\n",
        "  y_pred = [0. for k in test_files]\n",
        "\n",
        "  for file_idx, file_path in tqdm(enumerate(test_files), total=len(test_files)):\n",
        "\n",
        "    # Estrazione spettrogramma audio test\n",
        "    data = file_to_vector_array(file_path, n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH, power=POWER)\n",
        "    # Normalizzazione spettrogramma di test\n",
        "    data = ( data - mean ) / (std)\n",
        "\n",
        "    #print(data_aug_transpose.shape)\n",
        "\n",
        "    data_aug = numpy.zeros((16, 128, 32))\n",
        "    index = 0\n",
        "    i = 0\n",
        "    while (i+32) <= 313:\n",
        "      vector_i = numpy.zeros((128,32))\n",
        "      for j in range(0,128):\n",
        "        vector_i[j] = data[j][i:i+32]\n",
        "      data_aug[index] = vector_i\n",
        "      index += 1\n",
        "      i = i+18\n",
        "\n",
        "    data_aug_transpose = numpy.zeros((len(data_aug), 32, 128))\n",
        "    index = 0\n",
        "    for elem in data_aug:\n",
        "      data_aug_transpose[index] = elem.T\n",
        "      index += 1\n",
        "    #print(data_aug_transpose.shape)\n",
        "    \n",
        "    # Calcolo dell'errore medio sulle frame estratte dallo spettrogramma\n",
        "    elem_error = []\n",
        "    for elem in data_aug_transpose:\n",
        "      predicted = model.predict([elem.reshape(1,32,128), match_labels.reshape(1,4)])\n",
        "      errors = numpy.mean(numpy.square(elem - predicted), axis=1)\n",
        "      elem_error.append(numpy.mean(errors))\n",
        "\n",
        "    # Log dell'errore associato all'istanza di test\n",
        "    y_pred[file_idx] = numpy.mean(errors)\n",
        "    anomaly_score_list.append([os.path.basename(file_path), y_pred[file_idx]])\n",
        "  \n",
        "  save_csv(save_file_path=anomaly_score_csv, save_data=anomaly_score_list)\n",
        "    \n",
        "  # Calcolo AUC e pAUC per i dati con un certo ID_0x\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  p_auc = metrics.roc_auc_score(y_true, y_pred, max_fpr=0.1)\n",
        "  csv_lines.append([id_str.split(\"_\", 1)[1], auc, p_auc])\n",
        "  performance.append([auc, p_auc])\n",
        "  print(\"AUC : {}\".format(auc))\n",
        "  print(\"pAUC : {}\".format(p_auc))\n",
        "\n",
        "  print(\"\\n============ END OF TEST FOR A MACHINE ID ============\")\n",
        "\n",
        "# Stampa di AUC e pAUC medi su tutti i dati di test (media di AUC e pAUC sui vari ID).\n",
        "print(\"\\n============ AVERAGE PERFORMANCES ============\")\n",
        "averaged_performance = numpy.mean(numpy.array(performance, dtype=float), axis=0)\n",
        "csv_lines.append([\"Average\"] + list(averaged_performance))\n",
        "csv_lines.append([])\n",
        "print(averaged_performance)\n",
        "\n",
        "result_path = \"/content/drive/MyDrive/models/IDC-LSTM-AE/valve/1/anomaly_score_avg.csv\"\n",
        "save_csv(save_file_path=result_path, save_data=csv_lines)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}