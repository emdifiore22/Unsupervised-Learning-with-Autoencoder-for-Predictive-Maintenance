{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCASE2020_SEQUENTIAL_LSTM_AE_FAN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "UAsyVUTblaPA",
        "TEtzqLn2liwP"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs9iIa5OlJth"
      },
      "source": [
        "# IMPORT AND DATA LOADING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDG8iAV_W2fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "052157d7-9750-48cb-e17e-2fb10808e464"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9uFgr4mYC5v"
      },
      "source": [
        "# import necessari\n",
        "import librosa\n",
        "import numpy\n",
        "import sys\n",
        "import os\n",
        "import glob\n",
        "import itertools\n",
        "import re\n",
        "import pickle\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.models\n",
        "import tensorflow.keras.backend as K\n",
        "import keras.optimizers\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "from keras import Model\n",
        "from keras.layers import LSTM, RepeatVector, TimeDistributed, Dense, Dropout, Add, Multiply, Activation, Input\n",
        "from tqdm import tqdm\n",
        "from itertools import groupby\n",
        "from keras.utils import to_categorical\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# costanti \n",
        "ALPHA = 0.75\n",
        "N_MELS = 128\n",
        "HOP_LENGTH = 512\n",
        "N_FFT = 1024\n",
        "POWER = 2.0\n",
        "FRAME_NUMS = 313\n",
        "FRAMES = 10\n",
        "VAL = 0.05\n",
        "\n",
        "# FEATURES EXTRACTION\n",
        "\n",
        "# Loading da Google Drive\n",
        "train_data = numpy.load(\"/content/drive/MyDrive/DCASE_DATA_EXTRACTED/train/training_fan.npy\")\n",
        "grouped_list_by_machine_id = pickle.load( open( \"/content/drive/MyDrive/DCASE_DATA_EXTRACTED/train/training_fan_grouped_list.npy\", \"rb\" ) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDq3dIgZxVEC",
        "outputId": "74f59c26-176b-45b4-d9b7-d28115b2be80"
      },
      "source": [
        "# GENERAZIONE DELLE LABELS\n",
        "# One-hot encoding\n",
        "label = []\n",
        "choices = []\n",
        "for i in range(0, len(grouped_list_by_machine_id)):\n",
        "  for j in range(0, len(grouped_list_by_machine_id[i])):\n",
        "    machine_id = grouped_list_by_machine_id[i][j].split('/')[7].split('_')[2]\n",
        "    #print(grouped_list_by_machine_id[i][j].split('/')[7])\n",
        "    random_choice = numpy.random.choice([\"match\", \"non_match\"], p = [ALPHA, 1-ALPHA]) \n",
        "\n",
        "    if machine_id == '00':\n",
        "      if random_choice == \"match\":\n",
        "        to_append = [1,0,0,0]\n",
        "      else: \n",
        "        not_match_label = numpy.random.choice([1, 2, 3]) \n",
        "        if not_match_label == 1:\n",
        "          to_append = [0,1,0,0]\n",
        "        elif not_match_label == 2:\n",
        "          to_append = [0,0,1,0]\n",
        "        else: \n",
        "          to_append = [0,0,0,1]\n",
        "\n",
        "    elif machine_id == '02': \n",
        "      if random_choice == \"match\":\n",
        "        to_append = [0,1,0,0]\n",
        "      else: \n",
        "        not_match_label = numpy.random.choice( [ 1, 2, 3] ) \n",
        "        if not_match_label == 1:\n",
        "          to_append = [1,0,0,0]\n",
        "        elif not_match_label == 2:\n",
        "          to_append = [0,0,1,0]\n",
        "        else: \n",
        "          to_append = [0,0,0,1]\n",
        "\n",
        "    elif machine_id == \"04\":\n",
        "      if random_choice == \"match\":\n",
        "        to_append = [0,0,1,0]\n",
        "      else: \n",
        "        not_match_label = numpy.random.choice( [ 1, 2, 3] ) \n",
        "        if not_match_label == 1:\n",
        "          to_append = [1,0,0,0]\n",
        "        elif not_match_label == 2:\n",
        "          to_append = [0,1,0,0]\n",
        "        else: \n",
        "          to_append = [0,0,0,1]\n",
        "\n",
        "    elif machine_id == \"06\":\n",
        "      if random_choice == \"match\":\n",
        "        to_append = [0,0,0,1]\n",
        "      else: \n",
        "        not_match_label = numpy.random.choice( [ 1, 2, 3] ) \n",
        "        if not_match_label == 1:\n",
        "          to_append = [1,0,0,0]\n",
        "        elif not_match_label == 2:\n",
        "          to_append = [0,1,0,0]\n",
        "        else: \n",
        "          to_append = [0,0,1,0]\n",
        "    \n",
        "    label.append(to_append) # Append della label associata a ciascuno spettrogramma\n",
        "    choices.append(random_choice) # Append della choice utilizzata per associare la label\n",
        "                                  # La choice sarà utile in fase di addestramento per capire che tipo di loss calcolare\n",
        "\n",
        "# Trasformazione in numpy.array     \n",
        "label = numpy.asarray(label)\n",
        "choices = numpy.asarray(choices)\n",
        "print(label.shape)\n",
        "print(choices.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3675, 4)\n",
            "(3675,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czuXNBN8CelQ",
        "outputId": "e4e857ef-cedb-4588-a02e-821966939514"
      },
      "source": [
        "print(len(grouped_list_by_machine_id[0]))\n",
        "print(len(grouped_list_by_machine_id[1]))\n",
        "print(len(grouped_list_by_machine_id[2]))\n",
        "print(len(grouped_list_by_machine_id[3]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "911\n",
            "916\n",
            "933\n",
            "915\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQagajQlV65x",
        "outputId": "32f3f6ab-43be-4298-8603-9b2334675c52"
      },
      "source": [
        "# Estrazione spettrogrammi divisi per ID\n",
        "id_00 = train_data[0:911]\n",
        "label_00 = label[0:911]\n",
        "choices_00 = choices[0:911]\n",
        "\n",
        "id_02 = train_data[911:1827]\n",
        "label_02 = label[911:1827]\n",
        "choices_02 = choices[911:1827]\n",
        "\n",
        "id_04 = train_data[1827:2760]\n",
        "label_04 = label[1827:2760]\n",
        "choices_04 = choices[1827:2760]\n",
        "\n",
        "id_06 = train_data[2760:3675]\n",
        "label_06 = label[2760:3675]\n",
        "choices_06 = choices[2760:3675]\n",
        "\n",
        "id_00_training, \\\n",
        "id_00_validation, \\\n",
        "label_00_train, \\\n",
        "label_00_validation, \\\n",
        "choices_00_train, \\\n",
        "choices_00_validation = train_test_split(id_00, label_00, choices_00, test_size=VAL, random_state=42)\n",
        "\n",
        "id_02_training, \\\n",
        "id_02_validation, \\\n",
        "label_02_train, \\\n",
        "label_02_validation, \\\n",
        "choices_02_train, \\\n",
        "choices_02_validation = train_test_split(id_02, label_02, choices_02, test_size=VAL, random_state=42)\n",
        "\n",
        "id_04_training, \\\n",
        "id_04_validation, \\\n",
        "label_04_train, \\\n",
        "label_04_validation, \\\n",
        "choices_04_train, \\\n",
        "choices_04_validation = train_test_split(id_04, label_04, choices_04, test_size=VAL, random_state=42)\n",
        "\n",
        "id_06_training, \\\n",
        "id_06_validation, \\\n",
        "label_06_train, \\\n",
        "label_06_validation, \\\n",
        "choices_06_train, \\\n",
        "choices_06_validation = train_test_split(id_06, label_06, choices_06, test_size=VAL, random_state=42)\n",
        "\n",
        "# Normalization ID_00\n",
        "id_00_norm = numpy.empty_like(id_00_training)\n",
        "mean_00 = numpy.mean(id_00_training)\n",
        "std_00 = numpy.std(id_00_training)\n",
        "id_00_norm = (id_00_training - mean_00) / (std_00)\n",
        "id_00_norm_validation = (id_00_validation - mean_00) / (std_00)\n",
        "\n",
        "# Normalization ID_02\n",
        "id_02_norm = numpy.empty_like(id_02_training)\n",
        "mean_02 = numpy.mean(id_02_training)\n",
        "std_02 = numpy.std(id_02_training)\n",
        "id_02_norm = (id_02_training - mean_02) / (std_02)\n",
        "id_02_norm_validation = (id_02_validation - mean_02) / (std_02)\n",
        "\n",
        "# Normalization ID_04\n",
        "id_04_norm = numpy.empty_like(id_04_training)\n",
        "mean_04 = numpy.mean(id_04_training)\n",
        "std_04 = numpy.std(id_04_training)\n",
        "id_04_norm = (id_04_training - mean_04) / (std_04)\n",
        "id_04_norm_validation = (id_04_validation - mean_04) / (std_04)\n",
        "\n",
        "# Normalization ID_06\n",
        "id_06_norm = numpy.empty_like(id_06_training)\n",
        "mean_06 = numpy.mean(id_06_training)\n",
        "std_06 = numpy.std(id_06_training)\n",
        "id_06_norm = (id_06_training - mean_06) / (std_06)\n",
        "id_06_norm_validation = (id_06_validation - mean_06) / (std_06)\n",
        "\n",
        "print(\"==== DATA ====\")\n",
        "total_training = numpy.concatenate([id_00_norm, id_02_norm, id_04_norm, id_06_norm])\n",
        "print(total_training.shape)\n",
        "total_validation = numpy.concatenate([id_00_norm_validation, id_02_norm_validation, id_04_norm_validation, id_06_norm_validation])\n",
        "print(total_validation.shape)\n",
        "\n",
        "print(\"==== LABELS ====\")\n",
        "total_training_label = numpy.concatenate([label_00_train, label_02_train, label_04_train, label_06_train])\n",
        "print(total_training_label.shape)\n",
        "total_validation_label = numpy.concatenate([label_00_validation, label_02_validation, label_04_validation, label_06_validation])\n",
        "print(total_validation_label.shape)\n",
        "\n",
        "print(\"==== CHOICES ====\")\n",
        "total_training_choices = numpy.concatenate([choices_00_train, choices_02_train, choices_04_train, choices_06_train])\n",
        "print(total_training_choices.shape)\n",
        "total_validation_choices = numpy.concatenate([choices_00_validation, choices_02_validation, choices_04_validation, choices_06_validation])\n",
        "print(total_validation_choices.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==== DATA ====\n",
            "(3490, 128, 313)\n",
            "(185, 128, 313)\n",
            "==== LABELS ====\n",
            "(3490, 4)\n",
            "(185, 4)\n",
            "==== CHOICES ====\n",
            "(3490,)\n",
            "(185,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_KQ1vZ2Yh1x",
        "outputId": "e5695b83-8c6c-42fd-f110-7211b9426daa"
      },
      "source": [
        "training_aug = numpy.zeros((len(total_training)*12, 128, 32)) # Dataset utilizzato per il training\n",
        "index = 0\n",
        "for vector_array in total_training:\n",
        "  i = 0\n",
        "  while (i+32) <= 313:\n",
        "    vector_i = numpy.zeros((128,32))\n",
        "    for j in range(0,128):\n",
        "      vector_i[j] = vector_array[j][i:i+32]\n",
        "    training_aug[index] = vector_i\n",
        "    index += 1\n",
        "    i = i+25\n",
        "\n",
        "validation_aug = numpy.zeros((len(total_validation)*12, 128, 32)) # Dataset utilizzato per il training\n",
        "index = 0\n",
        "for vector_array in total_validation:\n",
        "  i = 0\n",
        "  while (i+32) <= 313:\n",
        "    vector_i = numpy.zeros((128,32))\n",
        "    for j in range(0,128):\n",
        "      vector_i[j] = vector_array[j][i:i+32]\n",
        "    validation_aug[index] = vector_i\n",
        "    index += 1\n",
        "    i = i+25\n",
        "\n",
        "training_aug_transpose = numpy.zeros((len(training_aug), 32, 128))\n",
        "index = 0\n",
        "for elem in training_aug:\n",
        "  training_aug_transpose[index] = elem.T\n",
        "  index += 1\n",
        "print(training_aug_transpose.shape)\n",
        "\n",
        "validation_aug_transpose = numpy.zeros((len(validation_aug), 32, 128))\n",
        "index = 0\n",
        "for elem in validation_aug:\n",
        "  validation_aug_transpose[index] = elem.T\n",
        "  index += 1\n",
        "print(validation_aug_transpose.shape)\n",
        "\n",
        "\n",
        "####### LABELS ######\n",
        "# Associazione della label associata a ciascun spettrogramma a ciascuno dei frame estratto da esso.\n",
        "training_labels = []\n",
        "for elem in total_training_label:\n",
        "  if numpy.array_equal(elem, numpy.asarray([1,0,0,0])) :\n",
        "    for i in range(12):\n",
        "      training_labels.append([1,0,0,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,1,0,0])):\n",
        "    for i in range(12):\n",
        "      training_labels.append([0,1,0,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,0,1,0])):\n",
        "    for i in range(12):\n",
        "      training_labels.append([0,0,1,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,0,0,1])):\n",
        "    for i in range(12):\n",
        "      training_labels.append([0,0,0,1])\n",
        "\n",
        "validation_labels = []\n",
        "for elem in total_validation_label:\n",
        "  if numpy.array_equal(elem, numpy.asarray([1,0,0,0])) :\n",
        "    for i in range(12):\n",
        "      validation_labels.append([1,0,0,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,1,0,0])):\n",
        "    for i in range(12):\n",
        "      validation_labels.append([0,1,0,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,0,1,0])):\n",
        "    for i in range(12):\n",
        "      validation_labels.append([0,0,1,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,0,0,1])):\n",
        "    for i in range(12):\n",
        "      validation_labels.append([0,0,0,1])\n",
        "\n",
        "training_labels = numpy.asarray(training_labels) # Dataset utilizzato per il training\n",
        "validation_labels = numpy.asarray(validation_labels) # Dataset utilizzato per il training\n",
        "#####################\n",
        "\n",
        "\n",
        "####### CHOICES ######\n",
        "# Associazione della choice associata a ciascun spettrogramma a ciascuno dei frame estratto da esso. \n",
        "training_choices = []\n",
        "for elem in total_training_choices:\n",
        "  if numpy.array_equal(elem, numpy.asarray(\"match\")) :\n",
        "    for i in range(12):\n",
        "      training_choices.append(\"match\")\n",
        "  elif numpy.array_equal(elem, numpy.asarray(\"non_match\")):\n",
        "    for i in range(12):\n",
        "      training_choices.append(\"non_match\")\n",
        "\n",
        "validation_choices = []\n",
        "for elem in total_validation_choices:\n",
        "  if numpy.array_equal(elem, numpy.asarray(\"match\")) :\n",
        "    for i in range(12):\n",
        "      validation_choices.append(\"match\")\n",
        "  elif numpy.array_equal(elem, numpy.asarray(\"non_match\")):\n",
        "    for i in range(12):\n",
        "      validation_choices.append(\"non_match\")\n",
        "\n",
        "training_choices = numpy.asarray(training_choices) # Dataset utilizzato per il training\n",
        "validation_choices = numpy.asarray(validation_choices) # Dataset utilizzato per il training\n",
        "######################\n",
        "\n",
        "print(\"==== DATA ====\")\n",
        "print(training_aug_transpose.shape)\n",
        "print(validation_aug_transpose.shape)\n",
        "\n",
        "print(\"==== LABELS ====\")\n",
        "print(training_labels.shape)\n",
        "print(validation_labels.shape)\n",
        "\n",
        "print(\"==== CHOICES ====\")\n",
        "print(training_choices.shape)\n",
        "print(validation_choices.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(41880, 32, 128)\n",
            "(2220, 32, 128)\n",
            "==== DATA ====\n",
            "(41880, 32, 128)\n",
            "(2220, 32, 128)\n",
            "==== LABELS ====\n",
            "(41880, 4)\n",
            "(2220, 4)\n",
            "==== CHOICES ====\n",
            "(41880,)\n",
            "(2220,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAsyVUTblaPA"
      },
      "source": [
        "# KERAS MODEL DEFINITION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISWY6OjXbZNd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e08fc1c0-382e-4192-b658-db207c4193aa"
      },
      "source": [
        "timesteps = 32\n",
        "num_features = 128\n",
        "\n",
        "input_Spect = Input(shape = [timesteps, num_features])\n",
        "input_Label = Input(shape = [4,])\n",
        "\n",
        "x = LSTM(64, \n",
        "        batch_input_shape=(None, timesteps, num_features), \n",
        "        return_sequences=True, name='encoder_1', \n",
        "        kernel_constraint = max_norm(1), \n",
        "        recurrent_constraint = max_norm(1), \n",
        "        bias_constraint = max_norm(1))(input_Spect)\n",
        "\n",
        "x = LSTM(32, \n",
        "        return_sequences=True, \n",
        "        name='encoder_3', \n",
        "        kernel_constraint = max_norm(1), \n",
        "        recurrent_constraint = max_norm(1), \n",
        "        bias_constraint = max_norm(1))(x)\n",
        "\n",
        "x = LSTM(16, \n",
        "        return_sequences=False, \n",
        "        name='encoder_4', \n",
        "        kernel_constraint = max_norm(1), \n",
        "        recurrent_constraint = max_norm(1), \n",
        "        bias_constraint = max_norm(1))(x)\n",
        "\n",
        "# Second Branch - Conditioning Feed Forward Neural Network\n",
        "m = Dense(16)(input_Label)\n",
        "m = Activation('sigmoid')(m)\n",
        "q = Dense(16)(input_Label)\n",
        "\n",
        "# Encoded Input Conditioning\n",
        "m = Multiply()([x, m])\n",
        "encoded_input_conditioned = Add()([q, m]) # Input da passare al decoder\n",
        "\n",
        "x = RepeatVector(timesteps, name='encoder_decoder_bridge')(encoded_input_conditioned)\n",
        "\n",
        "x = LSTM(16, \n",
        "        return_sequences=True, name='decoder_1', \n",
        "        kernel_constraint = max_norm(1), \n",
        "        recurrent_constraint = max_norm(1), \n",
        "        bias_constraint = max_norm(1))(x)\n",
        "\n",
        "x = LSTM(32, \n",
        "        return_sequences=True, name='decoder_2', \n",
        "        kernel_constraint = max_norm(1), \n",
        "        recurrent_constraint = max_norm(1), \n",
        "        bias_constraint = max_norm(1))(x)\n",
        "\n",
        "x = LSTM(64, \n",
        "        return_sequences=True, name='decoder_3', \n",
        "        kernel_constraint = max_norm(1), \n",
        "        recurrent_constraint = max_norm(1), \n",
        "        bias_constraint = max_norm(1))(x)\n",
        "\n",
        "x = TimeDistributed(Dense(num_features))(x)\n",
        "\n",
        "loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "mse_metric = keras.metrics.MeanSquaredError(name=\"mse\")\n",
        "\n",
        "class CustomModel(tensorflow.keras.Model):\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [loss_tracker, mse_metric]\n",
        "\n",
        "    def test_step(self, data):\n",
        "        # Unpack the data\n",
        "        x, y = data\n",
        "        # Compute predictions\n",
        "        y_pred = self([x[0],x[1]], training=False)\n",
        "        # Indici match\n",
        "        match = tf.where ( tf.equal(x[2][:], \"match\") )\n",
        "        # Dati match\n",
        "        data_match = K.gather(y, match)\n",
        "        # Separazione dei dati PREDETTI sulla base degli indici relativi a match/non_match\n",
        "        # Dati match\n",
        "        pred_match = K.gather(y_pred, match)\n",
        "\n",
        "        # Update metrica\n",
        "        mse_metric.update_state(data_match, pred_match)\n",
        "\n",
        "        return {\"mse\": mse_metric.result()}\n",
        "    \n",
        "    def train_step(self, data):\n",
        "          # Unpack the data. Its structure depends on your model and on what you pass to `fit()`.\n",
        "          x, y = data\n",
        "\n",
        "          # Vettore C utilizzato per il calcolo della loss in caso di non_match\n",
        "          C = 5 \n",
        "          # Valore di probabilità utilizzato come peso\n",
        "          ALPHA = 0.75 \n",
        "\n",
        "          # Indici match\n",
        "          match = tf.where ( tf.equal(x[2][:], \"match\") )\n",
        "\n",
        "          # Indici non_match\n",
        "          not_match = tf.where ( tf.equal(x[2][:], \"non_match\") )\n",
        "\n",
        "          # Dati match\n",
        "          data_match = K.gather(y, match)\n",
        "\n",
        "          with tf.GradientTape() as tape:\n",
        "              y_pred = self([x[0],x[1]], training=True)  # Forward pass\n",
        "\n",
        "              # Separazione dei dati PREDETTI sulla base degli indici relativi a match/non_match\n",
        "              # Dati match\n",
        "              pred_match = K.gather(y_pred, match)\n",
        "              # Dati non match\n",
        "              pred_not_match = K.gather(y_pred, not_match) \n",
        "\n",
        "              loss_m = K.mean(keras.losses.mean_squared_error(data_match, pred_match)) + 1e-6  # Calcolo Loss Match\n",
        "              loss_nm = K.mean(keras.losses.mean_squared_error(C,pred_not_match)) + 1e-6     # Calcolo Loss Non_Match\n",
        "\n",
        "              loss = ALPHA * loss_m + (1 - ALPHA) * loss_nm     # loss utilizzata per l'update dei pesi\n",
        "\n",
        "          # Compute gradients\n",
        "          trainable_vars = self.trainable_variables\n",
        "          gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "          # Update weights\n",
        "          self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "          # Compute our own metrics\n",
        "          loss_tracker.update_state(loss)\n",
        "          mse_metric.update_state(y, y_pred)\n",
        "          return {\"loss\": loss_tracker.result(), \"mse\": mse_metric.result()}\n",
        "\n",
        "model = CustomModel(inputs=(input_Spect, input_Label), outputs = x)\n",
        "model.compile(metrics=[\"mse\"], optimizer = \"adam\")\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"custom_model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 128)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 4)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder_1 (LSTM)                (None, 32, 64)       49408       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "encoder_3 (LSTM)                (None, 32, 64)       33024       encoder_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 16)           80          input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "encoder_4 (LSTM)                (None, 16)           5184        encoder_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 16)           0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 16)           80          input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "multiply (Multiply)             (None, 16)           0           encoder_4[0][0]                  \n",
            "                                                                 activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 16)           0           dense_1[0][0]                    \n",
            "                                                                 multiply[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "encoder_decoder_bridge (RepeatV (None, 32, 16)       0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "decoder_1 (LSTM)                (None, 32, 16)       2112        encoder_decoder_bridge[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "decoder_2 (LSTM)                (None, 32, 64)       20736       decoder_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "decoder_3 (LSTM)                (None, 32, 64)       33024       decoder_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, 32, 128)      8320        decoder_3[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 151,968\n",
            "Trainable params: 151,968\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbvCY_H6hDZA",
        "outputId": "07449c76-7310-4065-e7da-1aea0a6b9584"
      },
      "source": [
        "history = model.fit([training_aug_transpose, training_labels, training_choices],\n",
        "                    training_aug_transpose, \n",
        "                    epochs=100, \n",
        "                    validation_data=([validation_aug_transpose, validation_labels, validation_choices], validation_aug_transpose), \n",
        "                    batch_size=256)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "164/164 [==============================] - 14s 35ms/step - loss: 5.3589 - mse: 1.7408 - val_mse: 1.8753\n",
            "Epoch 2/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 5.0370 - mse: 1.8775 - val_mse: 1.9059\n",
            "Epoch 3/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 5.0322 - mse: 1.8849 - val_mse: 1.8945\n",
            "Epoch 4/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 5.0312 - mse: 1.8869 - val_mse: 1.8808\n",
            "Epoch 5/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 5.0323 - mse: 1.8853 - val_mse: 1.9172\n",
            "Epoch 6/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 4.9978 - mse: 1.9302 - val_mse: 1.9892\n",
            "Epoch 7/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 4.9794 - mse: 1.9479 - val_mse: 1.8632\n",
            "Epoch 8/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 4.9229 - mse: 1.9905 - val_mse: 1.9318\n",
            "Epoch 9/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 4.0650 - mse: 2.7035 - val_mse: 1.0338\n",
            "Epoch 10/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 3.0344 - mse: 3.7759 - val_mse: 0.8275\n",
            "Epoch 11/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 3.1765 - mse: 3.6469 - val_mse: 1.0180\n",
            "Epoch 12/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 2.9880 - mse: 3.8589 - val_mse: 1.0175\n",
            "Epoch 13/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 2.9615 - mse: 3.8650 - val_mse: 0.9953\n",
            "Epoch 14/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 2.8719 - mse: 3.9324 - val_mse: 0.8328\n",
            "Epoch 15/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 2.8454 - mse: 3.9412 - val_mse: 0.8366\n",
            "Epoch 16/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 2.6799 - mse: 4.1645 - val_mse: 2.2521\n",
            "Epoch 17/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 2.9032 - mse: 3.9110 - val_mse: 0.8107\n",
            "Epoch 18/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 2.6784 - mse: 4.1053 - val_mse: 1.3045\n",
            "Epoch 19/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 2.5946 - mse: 4.1722 - val_mse: 1.7573\n",
            "Epoch 20/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 1.9869 - mse: 4.7401 - val_mse: 1.3909\n",
            "Epoch 21/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 1.3238 - mse: 5.4467 - val_mse: 0.8675\n",
            "Epoch 22/100\n",
            "164/164 [==============================] - 4s 26ms/step - loss: 1.1465 - mse: 5.5818 - val_mse: 0.6652\n",
            "Epoch 23/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 1.3047 - mse: 5.4671 - val_mse: 0.7363\n",
            "Epoch 24/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 1.2092 - mse: 5.5181 - val_mse: 1.0364\n",
            "Epoch 25/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 1.6030 - mse: 5.2144 - val_mse: 1.0206\n",
            "Epoch 26/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 1.4336 - mse: 5.2583 - val_mse: 0.8536\n",
            "Epoch 27/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 1.2489 - mse: 5.5215 - val_mse: 0.8643\n",
            "Epoch 28/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 0.7311 - mse: 5.9305 - val_mse: 0.6650\n",
            "Epoch 29/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 0.6434 - mse: 6.0926 - val_mse: 0.6058\n",
            "Epoch 30/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 2.0600 - mse: 4.7777 - val_mse: 0.5816\n",
            "Epoch 31/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 0.8604 - mse: 5.8811 - val_mse: 0.6551\n",
            "Epoch 32/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 1.6916 - mse: 5.0757 - val_mse: 0.6677\n",
            "Epoch 33/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 0.9745 - mse: 5.7583 - val_mse: 0.5708\n",
            "Epoch 34/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 0.8812 - mse: 5.8071 - val_mse: 0.9056\n",
            "Epoch 35/100\n",
            "164/164 [==============================] - 4s 26ms/step - loss: 0.6700 - mse: 6.0481 - val_mse: 0.7195\n",
            "Epoch 36/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 0.7354 - mse: 5.9454 - val_mse: 0.4702\n",
            "Epoch 37/100\n",
            "164/164 [==============================] - 4s 26ms/step - loss: 0.5453 - mse: 6.2035 - val_mse: 0.4725\n",
            "Epoch 38/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 0.4774 - mse: 6.2354 - val_mse: 0.4572\n",
            "Epoch 39/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 2.5586 - mse: 4.7597 - val_mse: 0.7205\n",
            "Epoch 40/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 2.7617 - mse: 3.9356 - val_mse: 0.6429\n",
            "Epoch 41/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 2.9605 - mse: 4.2283 - val_mse: 1.9455\n",
            "Epoch 42/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 5.0116 - mse: 2.1502 - val_mse: 1.8969\n",
            "Epoch 43/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 5.0305 - mse: 1.8823 - val_mse: 1.9015\n",
            "Epoch 44/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 5.0274 - mse: 1.8836 - val_mse: 1.9114\n",
            "Epoch 45/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 4.9818 - mse: 1.8855 - val_mse: 1.8678\n",
            "Epoch 46/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 4.9591 - mse: 1.9287 - val_mse: 2.0583\n",
            "Epoch 47/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 5.0071 - mse: 2.1864 - val_mse: 1.9370\n",
            "Epoch 48/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 5.0300 - mse: 1.8878 - val_mse: 1.9097\n",
            "Epoch 49/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 5.0292 - mse: 1.8889 - val_mse: 1.9009\n",
            "Epoch 50/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 5.0285 - mse: 1.8874 - val_mse: 1.8947\n",
            "Epoch 51/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 5.0287 - mse: 1.8875 - val_mse: 1.9211\n",
            "Epoch 52/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 5.0285 - mse: 1.8881 - val_mse: 1.9155\n",
            "Epoch 53/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 5.0261 - mse: 1.8854 - val_mse: 1.9423\n",
            "Epoch 54/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 5.0288 - mse: 1.8877 - val_mse: 1.8883\n",
            "Epoch 55/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 5.0278 - mse: 1.8875 - val_mse: 1.9399\n",
            "Epoch 56/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 4.9868 - mse: 1.9019 - val_mse: 2.3499\n",
            "Epoch 57/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 5.0083 - mse: 1.9152 - val_mse: 1.8683\n",
            "Epoch 58/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 5.0315 - mse: 1.8890 - val_mse: 1.8906\n",
            "Epoch 59/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 5.0295 - mse: 1.8881 - val_mse: 1.9080\n",
            "Epoch 60/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 5.0290 - mse: 1.8882 - val_mse: 1.9075\n",
            "Epoch 61/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 4.9525 - mse: 1.9524 - val_mse: 1.5882\n",
            "Epoch 62/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 3.8281 - mse: 2.7502 - val_mse: 1.1034\n",
            "Epoch 63/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 3.0330 - mse: 3.7856 - val_mse: 1.2498\n",
            "Epoch 64/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 2.9179 - mse: 3.8919 - val_mse: 0.9079\n",
            "Epoch 65/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 2.9293 - mse: 3.8698 - val_mse: 0.9288\n",
            "Epoch 66/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 2.9020 - mse: 3.9037 - val_mse: 0.9880\n",
            "Epoch 67/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 2.8996 - mse: 3.9241 - val_mse: 0.9389\n",
            "Epoch 68/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 2.9164 - mse: 3.9027 - val_mse: 1.0182\n",
            "Epoch 69/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 2.9853 - mse: 3.8400 - val_mse: 1.0806\n",
            "Epoch 70/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 2.9133 - mse: 3.9283 - val_mse: 0.8859\n",
            "Epoch 71/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 2.9089 - mse: 3.9212 - val_mse: 0.9730\n",
            "Epoch 72/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 2.8701 - mse: 3.9437 - val_mse: 1.0727\n",
            "Epoch 73/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 2.8879 - mse: 3.9383 - val_mse: 0.9091\n",
            "Epoch 74/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 2.8673 - mse: 3.9547 - val_mse: 0.7138\n",
            "Epoch 75/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 3.0317 - mse: 3.8173 - val_mse: 1.3463\n",
            "Epoch 76/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 2.9695 - mse: 3.7862 - val_mse: 0.8791\n",
            "Epoch 77/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 2.6289 - mse: 4.0631 - val_mse: 1.0024\n",
            "Epoch 78/100\n",
            "164/164 [==============================] - 4s 26ms/step - loss: 2.8725 - mse: 4.0119 - val_mse: 1.1084\n",
            "Epoch 79/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 2.9076 - mse: 3.9164 - val_mse: 0.9674\n",
            "Epoch 80/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 2.8914 - mse: 3.9291 - val_mse: 1.1304\n",
            "Epoch 81/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 2.7651 - mse: 3.9850 - val_mse: 1.0541\n",
            "Epoch 82/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 2.3984 - mse: 4.3487 - val_mse: 1.1305\n",
            "Epoch 83/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 3.1771 - mse: 3.6921 - val_mse: 0.9496\n",
            "Epoch 84/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 2.7881 - mse: 3.9907 - val_mse: 1.0491\n",
            "Epoch 85/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 2.8957 - mse: 3.9782 - val_mse: 1.2258\n",
            "Epoch 86/100\n",
            "164/164 [==============================] - 4s 26ms/step - loss: 3.3932 - mse: 3.6542 - val_mse: 1.8654\n",
            "Epoch 87/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 4.5501 - mse: 2.1786 - val_mse: 1.1656\n",
            "Epoch 88/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 3.1757 - mse: 3.7343 - val_mse: 0.9602\n",
            "Epoch 89/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 3.0720 - mse: 3.6253 - val_mse: 0.9439\n",
            "Epoch 90/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 2.9007 - mse: 3.9335 - val_mse: 0.8875\n",
            "Epoch 91/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 2.9676 - mse: 3.8635 - val_mse: 0.9484\n",
            "Epoch 92/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 2.8764 - mse: 3.9540 - val_mse: 0.9113\n",
            "Epoch 93/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 2.8806 - mse: 3.9483 - val_mse: 0.8895\n",
            "Epoch 94/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 2.8722 - mse: 3.9405 - val_mse: 1.0604\n",
            "Epoch 95/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 2.8658 - mse: 3.9549 - val_mse: 0.7589\n",
            "Epoch 96/100\n",
            "164/164 [==============================] - 4s 26ms/step - loss: 2.8686 - mse: 3.9571 - val_mse: 0.8171\n",
            "Epoch 97/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 2.7708 - mse: 4.0304 - val_mse: 1.6067\n",
            "Epoch 98/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 3.0531 - mse: 3.7497 - val_mse: 1.1303\n",
            "Epoch 99/100\n",
            "164/164 [==============================] - 4s 24ms/step - loss: 1.8503 - mse: 4.8307 - val_mse: 1.1237\n",
            "Epoch 100/100\n",
            "164/164 [==============================] - 4s 25ms/step - loss: 1.2601 - mse: 5.4447 - val_mse: 1.0708\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZ8P46b6tq11"
      },
      "source": [
        "model.save('/content/drive/MyDrive/models/IDC-LSTM-AE/fan/1/model_fan.h5') \n",
        "with open('/content/drive/MyDrive/models/IDC-LSTM-AE/fan/1/trainHistoryDict', 'wb') as file_pi:\n",
        "    pickle.dump(history.history, file_pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEtzqLn2liwP"
      },
      "source": [
        "# TESTING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y42D3ejetbrN"
      },
      "source": [
        "import csv\n",
        "\n",
        "def save_csv(save_file_path,\n",
        "             save_data):\n",
        "    with open(save_file_path, \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f, lineterminator='\\n')\n",
        "        writer.writerows(save_data)\n",
        "\n",
        "\n",
        "# load dataset\n",
        "def select_dirs(path):\n",
        "    dir_path = os.path.abspath(path)\n",
        "    dirs = sorted(glob.glob(dir_path))\n",
        "    return dirs\n",
        "\n",
        "def file_load(wav_name, mono=False):\n",
        "    try:\n",
        "        return librosa.load(wav_name, sr=None, mono=mono)\n",
        "    except:\n",
        "        logger.error(\"file_broken or not exists!! : {}\".format(wav_name))\n",
        "\n",
        "def file_list_generator(target_dir, dir_name=\"train\", ext=\"wav\"):\n",
        "    print(\"target_dir : {}\".format(target_dir))\n",
        "\n",
        "    # generate training list\n",
        "    training_list_path = os.path.abspath(\"{dir}/{dir_name}/*.{ext}\".format(dir=target_dir, dir_name=dir_name, ext=ext))\n",
        "    files = sorted(glob.glob(training_list_path))\n",
        "    if len(files) == 0:\n",
        "      print(\"errore\")\n",
        "    return files\n",
        "\n",
        "\n",
        "def file_to_vector_array(file_name, n_mels=64, n_fft=1024, hop_length=512, power=2.0):\n",
        "    # 02 generate melspectrogram using librosa\n",
        "    y, sr = file_load(file_name)\n",
        "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, power=power)\n",
        "\n",
        "    # 03 convert melspectrogram to log mel energy\n",
        "    log_mel_spectrogram = 20.0 / power * numpy.log10(mel_spectrogram + sys.float_info.epsilon)\n",
        "\n",
        "    return log_mel_spectrogram\n",
        "\n",
        "  \n",
        "def list_to_vector_array(file_list, msg=\"calc...\", n_mels=64, n_fft=1024, hop_length=512, power=2.0, frames=10):\n",
        "    # iterate file_to_vector_array()\n",
        "    for idx in tqdm(range(len(file_list)), desc=msg):\n",
        "        vector_array = file_to_vector_array(file_list[idx], n_mels=n_mels, n_fft=n_fft, hop_length=hop_length, power=power)\n",
        "\n",
        "        if idx == 0:\n",
        "            dataset = numpy.zeros((len(file_list), n_mels, frames), float)\n",
        "        dataset[vector_array.shape[0] * idx: vector_array.shape[0] * (idx + 1), :] = vector_array\n",
        "    return dataset\n",
        "\n",
        "def key_by_id(item):\n",
        "  path_splitted = item.split(\"/\")\n",
        "  file_name = path_splitted[ len(path_splitted) - 1 ]\n",
        "  file_name_splitted = file_name.split(\"_\")\n",
        "  machine_id = file_name_splitted = file_name_splitted[2]\n",
        "  return machine_id\n",
        "\n",
        "def get_machine_id_list_for_test(target_dir,\n",
        "                                 dir_name=\"test\",\n",
        "                                 ext=\"wav\"):\n",
        "\n",
        "    # create test files\n",
        "    dir_path = os.path.abspath(\"{dir}/{dir_name}/*.{ext}\".format(dir=target_dir, dir_name=dir_name, ext=ext))\n",
        "    file_paths = sorted(glob.glob(dir_path))\n",
        "    # extract id\n",
        "    machine_id_list = sorted(list(set(itertools.chain.from_iterable(\n",
        "        [re.findall('id_[0-9][0-9]', ext_id) for ext_id in file_paths]))))\n",
        "    return machine_id_list\n",
        "\n",
        "def test_file_list_generator(target_dir,\n",
        "                             id_name,\n",
        "                             dir_name=\"test\",\n",
        "                             prefix_normal=\"normal\",\n",
        "                             prefix_anomaly=\"anomaly\",\n",
        "                             ext=\"wav\"):\n",
        "  \n",
        "    print(\"target_dir : {}\".format(target_dir+\"_\"+id_name))\n",
        "\n",
        "    normal_files = sorted(\n",
        "    glob.glob(\"{dir}/{dir_name}/{prefix_normal}_{id_name}*.{ext}\".format(dir=target_dir,\n",
        "                                                                                 dir_name=dir_name,\n",
        "                                                                                 prefix_normal=prefix_normal,\n",
        "                                                                                 id_name=id_name,\n",
        "                                                                                 ext=ext)))\n",
        "    normal_labels = numpy.zeros(len(normal_files))\n",
        "    anomaly_files = sorted(\n",
        "    glob.glob(\"{dir}/{dir_name}/{prefix_anomaly}_{id_name}*.{ext}\".format(dir=target_dir,\n",
        "                                                                                  dir_name=dir_name,\n",
        "                                                                                  prefix_anomaly=prefix_anomaly,\n",
        "                                                                                  id_name=id_name,\n",
        "                                                                                  ext=ext)))\n",
        "    anomaly_labels = numpy.ones(len(anomaly_files))\n",
        "    files = numpy.concatenate((normal_files, anomaly_files), axis=0)\n",
        "    labels = numpy.concatenate((normal_labels, anomaly_labels), axis=0)\n",
        "    print(\"test_file  num : {num}\".format(num=len(files)))\n",
        "    if len(files) == 0:\n",
        "        print(\"no_wav_file!!\")\n",
        "    print(\"\\n========================================\")\n",
        "\n",
        "    return files, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQLl5W5n7_T5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a523b59-789c-4b83-e44c-4a1a6736b1a4"
      },
      "source": [
        "target_dir = \"/content/drive/MyDrive/test/fan\"\n",
        "\n",
        "machine_type = os.path.split(target_dir)[1]\n",
        "print(\"============== MODEL LOAD ==============\")\n",
        "# set model path\n",
        "model_file = \"/content/drive/MyDrive/models/IDC-LSTM-AE/fan/1/model_fan.h5\"\n",
        "\n",
        "# load model file\n",
        "if not os.path.exists(model_file):\n",
        "  print(\"{} model not found \".format(machine_type))\n",
        "  sys.exit(-1)\n",
        "model = keras.models.load_model(model_file, custom_objects={'CustomModel': CustomModel, 'mse':mse_metric})\n",
        "# model.summary()\n",
        "\n",
        "machine_id_list = get_machine_id_list_for_test(target_dir)\n",
        "\n",
        "# initialize lines in csv for AUC and pAUC\n",
        "csv_lines = []\n",
        "\n",
        "csv_lines.append([machine_type])\n",
        "csv_lines.append([\"id\", \"AUC\", \"pAUC\"])\n",
        "performance = []\n",
        "\n",
        "for id_str in machine_id_list:\n",
        "  # load test file\n",
        "\n",
        "  id_num = id_str.split(\"_\")[1]\n",
        "\n",
        "  # Definizione della label \"match\" da utilizzare in fase di testing e del min e max da utilizzare per la normalizzazione\n",
        "  # i min e max sono stati calcolati a partire dai dati di training.\n",
        "  if id_num == \"00\":\n",
        "    match_labels = numpy.asarray([1,0,0,0])\n",
        "    mean = mean_00\n",
        "    std = std_00\n",
        "  if id_num == \"02\":\n",
        "    match_labels = numpy.asarray([0,1,0,0])\n",
        "    mean = mean_02\n",
        "    std = std_02\n",
        "  if id_num == \"04\":\n",
        "    match_labels = numpy.asarray([0,0,1,0])\n",
        "    mean = mean_04\n",
        "    std = std_04\n",
        "  if id_num == \"06\":\n",
        "    match_labels = numpy.asarray([0,0,0,1])\n",
        "    mean = mean_06\n",
        "    std = std_06\n",
        "\n",
        "  test_files, y_true = test_file_list_generator(target_dir, id_str)\n",
        "\n",
        "  # setup anomaly score file path\n",
        "  anomaly_score_csv = \"/content/drive/MyDrive/models/IDC-LSTM-AE/fan/1/anomaly_score_{machine_type}_{id_str}.csv\".format(machine_type=machine_type, id_str=id_str)\n",
        "  anomaly_score_list = []\n",
        "\n",
        "  print(\"\\n============== BEGIN TEST FOR A MACHINE ID {id} ==============\".format(id=id_num))\n",
        "\n",
        "  y_pred = [0. for k in test_files]\n",
        "\n",
        "  for file_idx, file_path in tqdm(enumerate(test_files), total=len(test_files)):\n",
        "\n",
        "    # Estrazione spettrogramma audio test\n",
        "    data = file_to_vector_array(file_path, n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH, power=POWER)\n",
        "    # Normalizzazione spettrogramma di test\n",
        "    data = ( data - mean ) / (std)\n",
        "\n",
        "    #print(data_aug_transpose.shape)\n",
        "\n",
        "    data_aug = numpy.zeros((12, 128, 32))\n",
        "    index = 0\n",
        "    i = 0\n",
        "    while (i+32) <= 313:\n",
        "      vector_i = numpy.zeros((128,32))\n",
        "      for j in range(0,128):\n",
        "        vector_i[j] = data[j][i:i+32]\n",
        "      data_aug[index] = vector_i\n",
        "      index += 1\n",
        "      i = i+25\n",
        "\n",
        "    data_aug_transpose = numpy.zeros((len(data_aug), 32, 128))\n",
        "    index = 0\n",
        "    for elem in data_aug:\n",
        "      data_aug_transpose[index] = elem.T\n",
        "      index += 1\n",
        "    #print(data_aug_transpose.shape)\n",
        "    \n",
        "    # Calcolo dell'errore medio sulle frame estratte dallo spettrogramma\n",
        "    elem_error = []\n",
        "    for elem in data_aug_transpose:\n",
        "      predicted = model.predict([elem.reshape(1,32,128), match_labels.reshape(1,4)])\n",
        "      errors = numpy.mean(numpy.square(elem - predicted), axis=1)\n",
        "      elem_error.append(numpy.mean(errors))\n",
        "\n",
        "    # Log dell'errore associato all'istanza di test\n",
        "    y_pred[file_idx] = numpy.mean(errors)\n",
        "    anomaly_score_list.append([os.path.basename(file_path), y_pred[file_idx]])\n",
        "  \n",
        "  save_csv(save_file_path=anomaly_score_csv, save_data=anomaly_score_list)\n",
        "    \n",
        "  # Calcolo AUC e pAUC per i dati con un certo ID_0x\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  p_auc = metrics.roc_auc_score(y_true, y_pred, max_fpr=0.1)\n",
        "  csv_lines.append([id_str.split(\"_\", 1)[1], auc, p_auc])\n",
        "  performance.append([auc, p_auc])\n",
        "  print(\"AUC : {}\".format(auc))\n",
        "  print(\"pAUC : {}\".format(p_auc))\n",
        "\n",
        "  print(\"\\n============ END OF TEST FOR A MACHINE ID ============\")\n",
        "\n",
        "# Stampa di AUC e pAUC medi su tutti i dati di test (media di AUC e pAUC sui vari ID).\n",
        "print(\"\\n============ AVERAGE PERFORMANCES ============\")\n",
        "averaged_performance = numpy.mean(numpy.array(performance, dtype=float), axis=0)\n",
        "csv_lines.append([\"Average\"] + list(averaged_performance))\n",
        "csv_lines.append([])\n",
        "print(averaged_performance)\n",
        "\n",
        "result_path = \"/content/drive/MyDrive/models/IDC-LSTM-AE/fan/1/anomaly_score_avg.csv\"\n",
        "save_csv(save_file_path=result_path, save_data=csv_lines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============== MODEL LOAD ==============\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/507 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "target_dir : /content/drive/MyDrive/test/fan_id_00\n",
            "test_file  num : 507\n",
            "\n",
            "========================================\n",
            "\n",
            "============== BEGIN TEST FOR A MACHINE ID 00 ==============\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 507/507 [08:30<00:00,  1.01s/it]\n",
            "  0%|          | 0/459 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AUC : 0.5084520884520884\n",
            "pAUC : 0.5004526057157637\n",
            "\n",
            "============ END OF TEST FOR A MACHINE ID ============\n",
            "target_dir : /content/drive/MyDrive/test/fan_id_02\n",
            "test_file  num : 459\n",
            "\n",
            "========================================\n",
            "\n",
            "============== BEGIN TEST FOR A MACHINE ID 02 ==============\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 459/459 [07:30<00:00,  1.02it/s]\n",
            "  0%|          | 0/448 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AUC : 0.789108635097493\n",
            "pAUC : 0.7447588330156869\n",
            "\n",
            "============ END OF TEST FOR A MACHINE ID ============\n",
            "target_dir : /content/drive/MyDrive/test/fan_id_04\n",
            "test_file  num : 448\n",
            "\n",
            "========================================\n",
            "\n",
            "============== BEGIN TEST FOR A MACHINE ID 04 ==============\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 448/448 [07:38<00:00,  1.02s/it]\n",
            "  0%|          | 0/461 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AUC : 0.42402298850574716\n",
            "pAUC : 0.49516031457955234\n",
            "\n",
            "============ END OF TEST FOR A MACHINE ID ============\n",
            "target_dir : /content/drive/MyDrive/test/fan_id_06\n",
            "test_file  num : 461\n",
            "\n",
            "========================================\n",
            "\n",
            "============== BEGIN TEST FOR A MACHINE ID 06 ==============\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 461/461 [07:55<00:00,  1.03s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AUC : 0.8401385041551247\n",
            "pAUC : 0.8282548476454293\n",
            "\n",
            "============ END OF TEST FOR A MACHINE ID ============\n",
            "\n",
            "============ AVERAGE PERFORMANCES ============\n",
            "[0.64043055 0.64215665]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}