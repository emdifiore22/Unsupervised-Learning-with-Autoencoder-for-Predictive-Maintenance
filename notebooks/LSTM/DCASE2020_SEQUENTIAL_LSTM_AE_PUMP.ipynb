{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCASE2020_SEQUENTIAL_LSTM_AE_PUMP.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jAaR5zVEmI8q",
        "m5WjHqPlmFJ7",
        "_cm3XsLil_GJ"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAaR5zVEmI8q"
      },
      "source": [
        "# IMPORT AND DATA LOADING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDG8iAV_W2fa",
        "outputId": "11833b19-7a10-48db-b12b-eb0ca1922f9c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9uFgr4mYC5v"
      },
      "source": [
        "# import necessari\n",
        "import librosa\n",
        "import numpy\n",
        "import sys\n",
        "import os\n",
        "import glob\n",
        "import itertools\n",
        "import re\n",
        "import pickle\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.models\n",
        "import tensorflow.keras.backend as K\n",
        "import keras.optimizers\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "from keras.layers import LSTM, RepeatVector, TimeDistributed, Dense, Dropout, Input, Activation, Multiply, Add\n",
        "from tqdm import tqdm\n",
        "from itertools import groupby\n",
        "from keras.utils import to_categorical\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# costanti \n",
        "ALPHA = 0.75\n",
        "N_MELS = 128\n",
        "HOP_LENGTH = 512\n",
        "N_FFT = 1024\n",
        "POWER = 2.0\n",
        "FRAME_NUMS = 313\n",
        "FRAMES = 10\n",
        "VAL = 0.05\n",
        "\n",
        "# FEATURES EXTRACTION\n",
        "\n",
        "# Loading da Google Drive\n",
        "train_data = numpy.load(\"/content/drive/MyDrive/DCASE_DATA_EXTRACTED/train/training_pump.npy\")\n",
        "grouped_list_by_machine_id = pickle.load( open( \"/content/drive/MyDrive/DCASE_DATA_EXTRACTED/train/training_pump_grouped_list.npy\", \"rb\" ) )"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDq3dIgZxVEC",
        "outputId": "212ef256-9eec-4b14-c103-eac5fcc8687a"
      },
      "source": [
        "# GENERAZIONE DELLE LABELS\n",
        "# One-hot encoding\n",
        "label = []\n",
        "choices = []\n",
        "for i in range(0, len(grouped_list_by_machine_id)):\n",
        "  for j in range(0, len(grouped_list_by_machine_id[i])):\n",
        "    machine_id = grouped_list_by_machine_id[i][j].split('/')[7].split('_')[2]\n",
        "    #print(grouped_list_by_machine_id[i][j].split('/')[7])\n",
        "    random_choice = numpy.random.choice([\"match\", \"non_match\"], p = [ALPHA, 1-ALPHA]) \n",
        "\n",
        "    if machine_id == '00':\n",
        "      if random_choice == \"match\":\n",
        "        to_append = [1,0,0,0]\n",
        "      else: \n",
        "        not_match_label = numpy.random.choice([1, 2, 3]) \n",
        "        if not_match_label == 1:\n",
        "          to_append = [0,1,0,0]\n",
        "        elif not_match_label == 2:\n",
        "          to_append = [0,0,1,0]\n",
        "        else: \n",
        "          to_append = [0,0,0,1]\n",
        "\n",
        "    elif machine_id == '02': \n",
        "      if random_choice == \"match\":\n",
        "        to_append = [0,1,0,0]\n",
        "      else: \n",
        "        not_match_label = numpy.random.choice( [ 1, 2, 3] ) \n",
        "        if not_match_label == 1:\n",
        "          to_append = [1,0,0,0]\n",
        "        elif not_match_label == 2:\n",
        "          to_append = [0,0,1,0]\n",
        "        else: \n",
        "          to_append = [0,0,0,1]\n",
        "\n",
        "    elif machine_id == \"04\":\n",
        "      if random_choice == \"match\":\n",
        "        to_append = [0,0,1,0]\n",
        "      else: \n",
        "        not_match_label = numpy.random.choice( [ 1, 2, 3] ) \n",
        "        if not_match_label == 1:\n",
        "          to_append = [1,0,0,0]\n",
        "        elif not_match_label == 2:\n",
        "          to_append = [0,1,0,0]\n",
        "        else: \n",
        "          to_append = [0,0,0,1]\n",
        "\n",
        "    elif machine_id == \"06\":\n",
        "      if random_choice == \"match\":\n",
        "        to_append = [0,0,0,1]\n",
        "      else: \n",
        "        not_match_label = numpy.random.choice( [ 1, 2, 3] ) \n",
        "        if not_match_label == 1:\n",
        "          to_append = [1,0,0,0]\n",
        "        elif not_match_label == 2:\n",
        "          to_append = [0,1,0,0]\n",
        "        else: \n",
        "          to_append = [0,0,1,0]\n",
        "    \n",
        "    label.append(to_append) # Append della label associata a ciascuno spettrogramma\n",
        "    choices.append(random_choice) # Append della choice utilizzata per associare la label\n",
        "                                  # La choice sarà utile in fase di addestramento per capire che tipo di loss calcolare\n",
        "\n",
        "# Trasformazione in numpy.array     \n",
        "label = numpy.asarray(label)\n",
        "choices = numpy.asarray(choices)\n",
        "print(label.shape)\n",
        "print(choices.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3349, 4)\n",
            "(3349,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQagajQlV65x",
        "outputId": "6c43ec02-aa15-4d07-f341-70682f4d627f"
      },
      "source": [
        "# Estrazione spettrogrammi divisi per ID\n",
        "id_00 = train_data[0:906]\n",
        "label_00 = label[0:906]\n",
        "choices_00 = choices[0:906]\n",
        "\n",
        "id_02 = train_data[906:1811]\n",
        "label_02 = label[906:1811]\n",
        "choices_02 = choices[906:1811]\n",
        "\n",
        "id_04 = train_data[1811:2413]\n",
        "label_04 = label[1811:2413]\n",
        "choices_04 = choices[1811:2413]\n",
        "\n",
        "id_06 = train_data[2413:3349]\n",
        "label_06 = label[2413:3349]\n",
        "choices_06 = choices[2413:3349]\n",
        "\n",
        "id_00_training, \\\n",
        "id_00_validation, \\\n",
        "label_00_train, \\\n",
        "label_00_validation, \\\n",
        "choices_00_train, \\\n",
        "choices_00_validation = train_test_split(id_00, label_00, choices_00, test_size=VAL, random_state=42)\n",
        "\n",
        "id_02_training, \\\n",
        "id_02_validation, \\\n",
        "label_02_train, \\\n",
        "label_02_validation, \\\n",
        "choices_02_train, \\\n",
        "choices_02_validation = train_test_split(id_02, label_02, choices_02, test_size=VAL, random_state=42)\n",
        "\n",
        "id_04_training, \\\n",
        "id_04_validation, \\\n",
        "label_04_train, \\\n",
        "label_04_validation, \\\n",
        "choices_04_train, \\\n",
        "choices_04_validation = train_test_split(id_04, label_04, choices_04, test_size=VAL, random_state=42)\n",
        "\n",
        "id_06_training, \\\n",
        "id_06_validation, \\\n",
        "label_06_train, \\\n",
        "label_06_validation, \\\n",
        "choices_06_train, \\\n",
        "choices_06_validation = train_test_split(id_06, label_06, choices_06, test_size=VAL, random_state=42)\n",
        "\n",
        "# Normalization ID_00\n",
        "id_00_norm = numpy.empty_like(id_00_training)\n",
        "mean_00 = numpy.mean(id_00_training)\n",
        "std_00 = numpy.std(id_00_training)\n",
        "id_00_norm = (id_00_training - mean_00) / (std_00)\n",
        "id_00_norm_validation = (id_00_validation - mean_00) / (std_00)\n",
        "\n",
        "# Normalization ID_02\n",
        "id_02_norm = numpy.empty_like(id_02_training)\n",
        "mean_02 = numpy.mean(id_02_training)\n",
        "std_02 = numpy.std(id_02_training)\n",
        "id_02_norm = (id_02_training - mean_02) / (std_02)\n",
        "id_02_norm_validation = (id_02_validation - mean_02) / (std_02)\n",
        "\n",
        "# Normalization ID_04\n",
        "id_04_norm = numpy.empty_like(id_04_training)\n",
        "mean_04 = numpy.mean(id_04_training)\n",
        "std_04 = numpy.std(id_04_training)\n",
        "id_04_norm = (id_04_training - mean_04) / (std_04)\n",
        "id_04_norm_validation = (id_04_validation - mean_04) / (std_04)\n",
        "\n",
        "# Normalization ID_06\n",
        "id_06_norm = numpy.empty_like(id_06_training)\n",
        "mean_06 = numpy.mean(id_06_training)\n",
        "std_06 = numpy.std(id_06_training)\n",
        "id_06_norm = (id_06_training - mean_06) / (std_06)\n",
        "id_06_norm_validation = (id_06_validation - mean_06) / (std_06)\n",
        "\n",
        "print(\"==== DATA ====\")\n",
        "total_training = numpy.concatenate([id_00_norm, id_02_norm, id_04_norm, id_06_norm])\n",
        "print(total_training.shape)\n",
        "total_validation = numpy.concatenate([id_00_norm_validation, id_02_norm_validation, id_04_norm_validation, id_06_norm_validation])\n",
        "print(total_validation.shape)\n",
        "\n",
        "print(\"==== LABELS ====\")\n",
        "total_training_label = numpy.concatenate([label_00_train, label_02_train, label_04_train, label_06_train])\n",
        "print(total_training_label.shape)\n",
        "total_validation_label = numpy.concatenate([label_00_validation, label_02_validation, label_04_validation, label_06_validation])\n",
        "print(total_validation_label.shape)\n",
        "\n",
        "print(\"==== CHOICES ====\")\n",
        "total_training_choices = numpy.concatenate([choices_00_train, choices_02_train, choices_04_train, choices_06_train])\n",
        "print(total_training_choices.shape)\n",
        "total_validation_choices = numpy.concatenate([choices_00_validation, choices_02_validation, choices_04_validation, choices_06_validation])\n",
        "print(total_validation_choices.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==== DATA ====\n",
            "(3179, 128, 313)\n",
            "(170, 128, 313)\n",
            "==== LABELS ====\n",
            "(3179, 4)\n",
            "(170, 4)\n",
            "==== CHOICES ====\n",
            "(3179,)\n",
            "(170,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_KQ1vZ2Yh1x",
        "outputId": "d65648ab-48e1-4185-e79a-bac0bf7b0e38"
      },
      "source": [
        "training_aug = numpy.zeros((len(total_training)*12, 128, 32)) # Dataset utilizzato per il training\n",
        "index = 0\n",
        "for vector_array in total_training:\n",
        "  i = 0\n",
        "  while (i+32) <= 313:\n",
        "    vector_i = numpy.zeros((128,32))\n",
        "    for j in range(0,128):\n",
        "      vector_i[j] = vector_array[j][i:i+32]\n",
        "    training_aug[index] = vector_i\n",
        "    index += 1\n",
        "    i = i+25\n",
        "\n",
        "validation_aug = numpy.zeros((len(total_validation)*12, 128, 32)) # Dataset utilizzato per il training\n",
        "index = 0\n",
        "for vector_array in total_validation:\n",
        "  i = 0\n",
        "  while (i+32) <= 313:\n",
        "    vector_i = numpy.zeros((128,32))\n",
        "    for j in range(0,128):\n",
        "      vector_i[j] = vector_array[j][i:i+32]\n",
        "    validation_aug[index] = vector_i\n",
        "    index += 1\n",
        "    i = i+25\n",
        "\n",
        "training_aug_transpose = numpy.zeros((len(training_aug), 32, 128))\n",
        "index = 0\n",
        "for elem in training_aug:\n",
        "  training_aug_transpose[index] = elem.T\n",
        "  index += 1\n",
        "print(training_aug_transpose.shape)\n",
        "\n",
        "validation_aug_transpose = numpy.zeros((len(validation_aug), 32, 128))\n",
        "index = 0\n",
        "for elem in validation_aug:\n",
        "  validation_aug_transpose[index] = elem.T\n",
        "  index += 1\n",
        "print(validation_aug_transpose.shape)\n",
        "\n",
        "\n",
        "####### LABELS ######\n",
        "# Associazione della label associata a ciascun spettrogramma a ciascuno dei frame estratto da esso.\n",
        "training_labels = []\n",
        "for elem in total_training_label:\n",
        "  if numpy.array_equal(elem, numpy.asarray([1,0,0,0])) :\n",
        "    for i in range(12):\n",
        "      training_labels.append([1,0,0,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,1,0,0])):\n",
        "    for i in range(12):\n",
        "      training_labels.append([0,1,0,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,0,1,0])):\n",
        "    for i in range(12):\n",
        "      training_labels.append([0,0,1,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,0,0,1])):\n",
        "    for i in range(12):\n",
        "      training_labels.append([0,0,0,1])\n",
        "\n",
        "validation_labels = []\n",
        "for elem in total_validation_label:\n",
        "  if numpy.array_equal(elem, numpy.asarray([1,0,0,0])) :\n",
        "    for i in range(12):\n",
        "      validation_labels.append([1,0,0,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,1,0,0])):\n",
        "    for i in range(12):\n",
        "      validation_labels.append([0,1,0,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,0,1,0])):\n",
        "    for i in range(12):\n",
        "      validation_labels.append([0,0,1,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,0,0,1])):\n",
        "    for i in range(12):\n",
        "      validation_labels.append([0,0,0,1])\n",
        "\n",
        "training_labels = numpy.asarray(training_labels) # Dataset utilizzato per il training\n",
        "validation_labels = numpy.asarray(validation_labels) # Dataset utilizzato per il training\n",
        "#####################\n",
        "\n",
        "\n",
        "####### CHOICES ######\n",
        "# Associazione della choice associata a ciascun spettrogramma a ciascuno dei frame estratto da esso. \n",
        "training_choices = []\n",
        "for elem in total_training_choices:\n",
        "  if numpy.array_equal(elem, numpy.asarray(\"match\")) :\n",
        "    for i in range(12):\n",
        "      training_choices.append(\"match\")\n",
        "  elif numpy.array_equal(elem, numpy.asarray(\"non_match\")):\n",
        "    for i in range(12):\n",
        "      training_choices.append(\"non_match\")\n",
        "\n",
        "validation_choices = []\n",
        "for elem in total_validation_choices:\n",
        "  if numpy.array_equal(elem, numpy.asarray(\"match\")) :\n",
        "    for i in range(12):\n",
        "      validation_choices.append(\"match\")\n",
        "  elif numpy.array_equal(elem, numpy.asarray(\"non_match\")):\n",
        "    for i in range(12):\n",
        "      validation_choices.append(\"non_match\")\n",
        "\n",
        "training_choices = numpy.asarray(training_choices) # Dataset utilizzato per il training\n",
        "validation_choices = numpy.asarray(validation_choices) # Dataset utilizzato per il training\n",
        "######################\n",
        "\n",
        "print(\"==== DATA ====\")\n",
        "print(training_aug_transpose.shape)\n",
        "print(validation_aug_transpose.shape)\n",
        "\n",
        "print(\"==== LABELS ====\")\n",
        "print(training_labels.shape)\n",
        "print(validation_labels.shape)\n",
        "\n",
        "print(\"==== CHOICES ====\")\n",
        "print(training_choices.shape)\n",
        "print(validation_choices.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(38148, 32, 128)\n",
            "(2040, 32, 128)\n",
            "==== DATA ====\n",
            "(38148, 32, 128)\n",
            "(2040, 32, 128)\n",
            "==== LABELS ====\n",
            "(38148, 4)\n",
            "(2040, 4)\n",
            "==== CHOICES ====\n",
            "(38148,)\n",
            "(2040,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5WjHqPlmFJ7"
      },
      "source": [
        "# KERAS MODEL DEFINITION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISWY6OjXbZNd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeebd9d1-e075-4bd1-b345-25b72c500f09"
      },
      "source": [
        "timesteps = 32\n",
        "num_features = 128\n",
        "\n",
        "input_Spect = Input(shape = [timesteps, num_features])\n",
        "input_Label = Input(shape = [4,])\n",
        "\n",
        "x = LSTM(64, \n",
        "        batch_input_shape=(None, timesteps, num_features), \n",
        "        return_sequences=True, name='encoder_1', \n",
        "        kernel_constraint = max_norm(1), \n",
        "        recurrent_constraint = max_norm(1), \n",
        "        bias_constraint = max_norm(1))(input_Spect)\n",
        "\n",
        "x = LSTM(32, \n",
        "        return_sequences=True, name='encoder_2', \n",
        "        kernel_constraint = max_norm(1), \n",
        "        recurrent_constraint = max_norm(1), \n",
        "        bias_constraint = max_norm(1))(x)\n",
        "\n",
        "x = LSTM(16, \n",
        "        return_sequences=False, \n",
        "        name='encoder_3', \n",
        "        kernel_constraint = max_norm(1), \n",
        "        recurrent_constraint = max_norm(1), \n",
        "        bias_constraint = max_norm(1))(x)\n",
        "\n",
        "# Second Branch - Conditioning Feed Forward Neural Network\n",
        "m = Dense(16)(input_Label)\n",
        "m = Activation('sigmoid')(m)\n",
        "q = Dense(16)(input_Label)\n",
        "\n",
        "# Encoded Input Conditioning\n",
        "m = Multiply()([x, m])\n",
        "encoded_input_conditioned = Add()([q, m]) # Input da passare al decoder\n",
        "\n",
        "x = RepeatVector(timesteps, name='encoder_decoder_bridge')(encoded_input_conditioned)\n",
        "\n",
        "x = LSTM(16, \n",
        "        return_sequences=True, name='decoder_1', \n",
        "        kernel_constraint = max_norm(1), \n",
        "        recurrent_constraint = max_norm(1), \n",
        "        bias_constraint = max_norm(1))(x)\n",
        "\n",
        "x = LSTM(32, \n",
        "        return_sequences=True, name='decoder_2', \n",
        "        kernel_constraint = max_norm(1), \n",
        "        recurrent_constraint = max_norm(1), \n",
        "        bias_constraint = max_norm(1))(x)\n",
        "\n",
        "x = LSTM(64, \n",
        "        return_sequences=True, name='decoder_3', \n",
        "        kernel_constraint = max_norm(1), \n",
        "        recurrent_constraint = max_norm(1), \n",
        "        bias_constraint = max_norm(1))(x)\n",
        "\n",
        "x = TimeDistributed(Dense(num_features))(x)\n",
        "\n",
        "loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "mse_metric = keras.metrics.MeanSquaredError(name=\"mse\")\n",
        "\n",
        "class CustomModel(tensorflow.keras.Model):\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [loss_tracker, mse_metric]\n",
        "\n",
        "    def test_step(self, data):\n",
        "        # Unpack the data\n",
        "        x, y = data\n",
        "        # Compute predictions\n",
        "        y_pred = self([x[0],x[1]], training=False)\n",
        "        # Indici match\n",
        "        match = tf.where ( tf.equal(x[2][:], \"match\") )\n",
        "        # Dati match\n",
        "        data_match = K.gather(y, match)\n",
        "        # Separazione dei dati PREDETTI sulla base degli indici relativi a match/non_match\n",
        "        # Dati match\n",
        "        pred_match = K.gather(y_pred, match)\n",
        "\n",
        "        # Update metrica\n",
        "        mse_metric.update_state(data_match, pred_match)\n",
        "\n",
        "        return {\"mse\": mse_metric.result()}\n",
        "    \n",
        "    def train_step(self, data):\n",
        "          # Unpack the data. Its structure depends on your model and on what you pass to `fit()`.\n",
        "          x, y = data\n",
        "\n",
        "          # Vettore C utilizzato per il calcolo della loss in caso di non_match\n",
        "          C = 5 \n",
        "          # Valore di probabilità utilizzato come peso\n",
        "          ALPHA = 0.75 \n",
        "\n",
        "          # Indici match\n",
        "          match = tf.where ( tf.equal(x[2][:], \"match\") )\n",
        "\n",
        "          # Indici non_match\n",
        "          not_match = tf.where ( tf.equal(x[2][:], \"non_match\") )\n",
        "\n",
        "          # Dati match\n",
        "          data_match = K.gather(y, match)\n",
        "\n",
        "          with tf.GradientTape() as tape:\n",
        "              y_pred = self([x[0],x[1]], training=True)  # Forward pass\n",
        "\n",
        "              # Separazione dei dati PREDETTI sulla base degli indici relativi a match/non_match\n",
        "              # Dati match\n",
        "              pred_match = K.gather(y_pred, match)\n",
        "              # Dati non match\n",
        "              pred_not_match = K.gather(y_pred, not_match) \n",
        "\n",
        "              loss_m = K.mean(keras.losses.mean_squared_error(data_match, pred_match)) + 1e-6  # Calcolo Loss Match\n",
        "              loss_nm = K.mean(keras.losses.mean_squared_error(C,pred_not_match)) + 1e-6     # Calcolo Loss Non_Match\n",
        "\n",
        "              loss = ALPHA * loss_m + (1 - ALPHA) * loss_nm     # loss utilizzata per l'update dei pesi\n",
        "\n",
        "          # Compute gradients\n",
        "          trainable_vars = self.trainable_variables\n",
        "          gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "          # Update weights\n",
        "          self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "          # Compute our own metrics\n",
        "          loss_tracker.update_state(loss)\n",
        "          mse_metric.update_state(y, y_pred)\n",
        "          return {\"loss\": loss_tracker.result(), \"mse\": mse_metric.result()}\n",
        "\n",
        "model = CustomModel(inputs=(input_Spect, input_Label), outputs = x)\n",
        "model.compile(metrics=[\"mse\"], optimizer = \"adam\")\n",
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"custom_model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 128)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 4)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder_1 (LSTM)                (None, 32, 64)       49408       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "encoder_2 (LSTM)                (None, 32, 32)       12416       encoder_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 16)           80          input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "encoder_3 (LSTM)                (None, 16)           3136        encoder_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 16)           0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 16)           80          input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "multiply (Multiply)             (None, 16)           0           encoder_3[0][0]                  \n",
            "                                                                 activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 16)           0           dense_1[0][0]                    \n",
            "                                                                 multiply[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "encoder_decoder_bridge (RepeatV (None, 32, 16)       0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "decoder_1 (LSTM)                (None, 32, 16)       2112        encoder_decoder_bridge[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "decoder_2 (LSTM)                (None, 32, 32)       6272        decoder_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "decoder_3 (LSTM)                (None, 32, 64)       24832       decoder_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, 32, 128)      8320        decoder_3[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 106,656\n",
            "Trainable params: 106,656\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbvCY_H6hDZA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "906ad901-a317-4649-e340-8f92b48cb8d2"
      },
      "source": [
        "history = model.fit([training_aug_transpose, training_labels, training_choices],\n",
        "                    training_aug_transpose, \n",
        "                    epochs=100, \n",
        "                    validation_data=([validation_aug_transpose, validation_labels, validation_choices], validation_aug_transpose), \n",
        "                    batch_size=512)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "75/75 [==============================] - 49s 129ms/step - loss: 5.8071 - mse: 1.5985 - val_mse: 1.8079\n",
            "Epoch 2/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 5.1065 - mse: 1.9006 - val_mse: 1.8787\n",
            "Epoch 3/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 5.0588 - mse: 1.9377 - val_mse: 1.9019\n",
            "Epoch 4/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 5.0346 - mse: 1.9658 - val_mse: 1.9043\n",
            "Epoch 5/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 4.9848 - mse: 1.9833 - val_mse: 1.8800\n",
            "Epoch 6/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 4.0006 - mse: 2.8195 - val_mse: 1.2312\n",
            "Epoch 7/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 3.5969 - mse: 3.3634 - val_mse: 1.3011\n",
            "Epoch 8/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 3.5134 - mse: 3.4130 - val_mse: 1.0112\n",
            "Epoch 9/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 3.3961 - mse: 3.5442 - val_mse: 1.0815\n",
            "Epoch 10/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 3.3299 - mse: 3.6432 - val_mse: 1.0174\n",
            "Epoch 11/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 3.2860 - mse: 3.6716 - val_mse: 0.9635\n",
            "Epoch 12/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 3.2766 - mse: 3.6787 - val_mse: 1.0920\n",
            "Epoch 13/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 3.2653 - mse: 3.6944 - val_mse: 1.0426\n",
            "Epoch 14/100\n",
            "75/75 [==============================] - 7s 90ms/step - loss: 3.2574 - mse: 3.6916 - val_mse: 1.0574\n",
            "Epoch 15/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 3.2380 - mse: 3.7233 - val_mse: 0.9591\n",
            "Epoch 16/100\n",
            "75/75 [==============================] - 7s 90ms/step - loss: 3.2409 - mse: 3.7098 - val_mse: 0.9585\n",
            "Epoch 17/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 3.2202 - mse: 3.7359 - val_mse: 1.1018\n",
            "Epoch 18/100\n",
            "75/75 [==============================] - 7s 90ms/step - loss: 3.2685 - mse: 3.6521 - val_mse: 0.9992\n",
            "Epoch 19/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 3.2119 - mse: 3.7284 - val_mse: 0.9604\n",
            "Epoch 20/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 3.3166 - mse: 3.6148 - val_mse: 0.9935\n",
            "Epoch 21/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 3.1760 - mse: 3.7649 - val_mse: 0.8321\n",
            "Epoch 22/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 3.2154 - mse: 3.7529 - val_mse: 0.9327\n",
            "Epoch 23/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 3.2380 - mse: 3.7061 - val_mse: 1.0284\n",
            "Epoch 24/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 3.2085 - mse: 3.7443 - val_mse: 1.0018\n",
            "Epoch 25/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 3.1944 - mse: 3.7393 - val_mse: 0.9448\n",
            "Epoch 26/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 3.0304 - mse: 3.8905 - val_mse: 0.9349\n",
            "Epoch 27/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 2.7960 - mse: 4.1789 - val_mse: 0.6511\n",
            "Epoch 28/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 4.5170 - mse: 2.8593 - val_mse: 1.8747\n",
            "Epoch 29/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 3.5473 - mse: 3.2505 - val_mse: 1.4127\n",
            "Epoch 30/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 2.6964 - mse: 4.2403 - val_mse: 0.9293\n",
            "Epoch 31/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 2.4859 - mse: 4.4188 - val_mse: 0.8302\n",
            "Epoch 32/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 2.3701 - mse: 4.5486 - val_mse: 0.7537\n",
            "Epoch 33/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 1.9835 - mse: 4.8418 - val_mse: 1.0785\n",
            "Epoch 34/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 1.9096 - mse: 4.9822 - val_mse: 0.8260\n",
            "Epoch 35/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 1.8358 - mse: 5.0816 - val_mse: 0.6057\n",
            "Epoch 36/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 1.6701 - mse: 5.2214 - val_mse: 0.6224\n",
            "Epoch 37/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 1.6545 - mse: 5.2323 - val_mse: 0.6754\n",
            "Epoch 38/100\n",
            "75/75 [==============================] - 7s 90ms/step - loss: 1.5962 - mse: 5.2977 - val_mse: 0.6315\n",
            "Epoch 39/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 1.5996 - mse: 5.2874 - val_mse: 0.7044\n",
            "Epoch 40/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 1.5722 - mse: 5.3141 - val_mse: 0.6136\n",
            "Epoch 41/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 2.5555 - mse: 4.3980 - val_mse: 1.5354\n",
            "Epoch 42/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 1.8427 - mse: 5.0824 - val_mse: 0.4674\n",
            "Epoch 43/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 1.6243 - mse: 5.2428 - val_mse: 0.5957\n",
            "Epoch 44/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 1.5953 - mse: 5.2856 - val_mse: 0.5790\n",
            "Epoch 45/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 1.5516 - mse: 5.3101 - val_mse: 0.5778\n",
            "Epoch 46/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 1.5352 - mse: 5.3095 - val_mse: 0.5941\n",
            "Epoch 47/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 1.4575 - mse: 5.4121 - val_mse: 0.5044\n",
            "Epoch 48/100\n",
            "75/75 [==============================] - 7s 90ms/step - loss: 1.3063 - mse: 5.5379 - val_mse: 0.5484\n",
            "Epoch 49/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 1.1342 - mse: 5.7249 - val_mse: 0.6494\n",
            "Epoch 50/100\n",
            "75/75 [==============================] - 7s 90ms/step - loss: 1.0657 - mse: 5.8074 - val_mse: 0.6417\n",
            "Epoch 51/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 0.9032 - mse: 5.9529 - val_mse: 0.5383\n",
            "Epoch 52/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 0.8170 - mse: 6.0544 - val_mse: 0.4188\n",
            "Epoch 53/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 0.8133 - mse: 6.0709 - val_mse: 0.5792\n",
            "Epoch 54/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 0.6725 - mse: 6.2006 - val_mse: 0.4581\n",
            "Epoch 55/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 0.8896 - mse: 6.0167 - val_mse: 0.5163\n",
            "Epoch 56/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 0.7800 - mse: 6.0848 - val_mse: 0.5446\n",
            "Epoch 57/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 0.5840 - mse: 6.2894 - val_mse: 0.5676\n",
            "Epoch 58/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 0.5905 - mse: 6.2714 - val_mse: 0.8208\n",
            "Epoch 59/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 0.6412 - mse: 6.2285 - val_mse: 0.5438\n",
            "Epoch 60/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 0.6072 - mse: 6.2314 - val_mse: 0.4601\n",
            "Epoch 61/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 0.5956 - mse: 6.2427 - val_mse: 0.5234\n",
            "Epoch 62/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 0.5531 - mse: 6.3425 - val_mse: 0.5252\n",
            "Epoch 63/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 0.5118 - mse: 6.3040 - val_mse: 0.4467\n",
            "Epoch 64/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 0.4437 - mse: 6.4050 - val_mse: 0.3941\n",
            "Epoch 65/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 0.4711 - mse: 6.3571 - val_mse: 0.3360\n",
            "Epoch 66/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 0.3956 - mse: 6.4508 - val_mse: 0.4017\n",
            "Epoch 67/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 0.5053 - mse: 6.3331 - val_mse: 0.3819\n",
            "Epoch 68/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 0.3811 - mse: 6.4561 - val_mse: 0.4630\n",
            "Epoch 69/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 0.3850 - mse: 6.4476 - val_mse: 0.3856\n",
            "Epoch 70/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 0.3448 - mse: 6.4787 - val_mse: 0.4630\n",
            "Epoch 71/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 0.3732 - mse: 6.4644 - val_mse: 0.3838\n",
            "Epoch 72/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 0.3410 - mse: 6.4895 - val_mse: 0.3488\n",
            "Epoch 73/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 0.3867 - mse: 6.4068 - val_mse: 0.4286\n",
            "Epoch 74/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 0.3459 - mse: 6.4971 - val_mse: 0.4924\n",
            "Epoch 75/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 0.3589 - mse: 6.4659 - val_mse: 0.3365\n",
            "Epoch 76/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 0.3124 - mse: 6.5113 - val_mse: 0.3984\n",
            "Epoch 77/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 0.3737 - mse: 6.4512 - val_mse: 0.4028\n",
            "Epoch 78/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 0.4112 - mse: 6.3908 - val_mse: 0.4794\n",
            "Epoch 79/100\n",
            "75/75 [==============================] - 7s 93ms/step - loss: 0.3487 - mse: 6.4997 - val_mse: 0.5302\n",
            "Epoch 80/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 0.3069 - mse: 6.5084 - val_mse: 0.3406\n",
            "Epoch 81/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 0.3003 - mse: 6.5141 - val_mse: 0.4103\n",
            "Epoch 82/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 0.2893 - mse: 6.5198 - val_mse: 0.4525\n",
            "Epoch 83/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 0.3096 - mse: 6.5107 - val_mse: 0.3743\n",
            "Epoch 84/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 0.2697 - mse: 6.5315 - val_mse: 0.3531\n",
            "Epoch 85/100\n",
            "75/75 [==============================] - 7s 93ms/step - loss: 0.2817 - mse: 6.5417 - val_mse: 0.3699\n",
            "Epoch 86/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 0.2991 - mse: 6.5056 - val_mse: 0.3431\n",
            "Epoch 87/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 0.2901 - mse: 6.5235 - val_mse: 0.3475\n",
            "Epoch 88/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 0.3456 - mse: 6.4660 - val_mse: 0.3795\n",
            "Epoch 89/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 0.2849 - mse: 6.5309 - val_mse: 0.3807\n",
            "Epoch 90/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 0.2904 - mse: 6.5099 - val_mse: 0.3319\n",
            "Epoch 91/100\n",
            "75/75 [==============================] - 7s 93ms/step - loss: 0.4331 - mse: 6.4070 - val_mse: 0.4269\n",
            "Epoch 92/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 0.5195 - mse: 6.3155 - val_mse: 0.5062\n",
            "Epoch 93/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 0.3438 - mse: 6.4609 - val_mse: 0.3366\n",
            "Epoch 94/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 0.3485 - mse: 6.4798 - val_mse: 0.4053\n",
            "Epoch 95/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 0.3318 - mse: 6.4808 - val_mse: 0.3613\n",
            "Epoch 96/100\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 0.3181 - mse: 6.5007 - val_mse: 0.3753\n",
            "Epoch 97/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 0.3446 - mse: 6.4578 - val_mse: 0.3117\n",
            "Epoch 98/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 0.3372 - mse: 6.4817 - val_mse: 0.3429\n",
            "Epoch 99/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 0.2780 - mse: 6.5099 - val_mse: 0.2946\n",
            "Epoch 100/100\n",
            "75/75 [==============================] - 7s 92ms/step - loss: 0.2515 - mse: 6.5587 - val_mse: 0.2980\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZ8P46b6tq11"
      },
      "source": [
        "model.save('/content/drive/MyDrive/models/IDC-LSTM-AE/pump/2/model_pump.h5') \n",
        "with open('/content/drive/MyDrive/models/IDC-LSTM-AE/pump/2/trainHistoryDict', 'wb') as file_pi:\n",
        "    pickle.dump(history.history, file_pi)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cm3XsLil_GJ"
      },
      "source": [
        "# TESTING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y42D3ejetbrN"
      },
      "source": [
        "import csv\n",
        "\n",
        "def save_csv(save_file_path,\n",
        "             save_data):\n",
        "    with open(save_file_path, \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f, lineterminator='\\n')\n",
        "        writer.writerows(save_data)\n",
        "\n",
        "\n",
        "# load dataset\n",
        "def select_dirs(path):\n",
        "    dir_path = os.path.abspath(path)\n",
        "    dirs = sorted(glob.glob(dir_path))\n",
        "    return dirs\n",
        "\n",
        "def file_load(wav_name, mono=False):\n",
        "    try:\n",
        "        return librosa.load(wav_name, sr=None, mono=mono)\n",
        "    except:\n",
        "        logger.error(\"file_broken or not exists!! : {}\".format(wav_name))\n",
        "\n",
        "def file_list_generator(target_dir, dir_name=\"train\", ext=\"wav\"):\n",
        "    print(\"target_dir : {}\".format(target_dir))\n",
        "\n",
        "    # generate training list\n",
        "    training_list_path = os.path.abspath(\"{dir}/{dir_name}/*.{ext}\".format(dir=target_dir, dir_name=dir_name, ext=ext))\n",
        "    files = sorted(glob.glob(training_list_path))\n",
        "    if len(files) == 0:\n",
        "      print(\"errore\")\n",
        "    return files\n",
        "\n",
        "\n",
        "def file_to_vector_array(file_name, n_mels=64, n_fft=1024, hop_length=512, power=2.0):\n",
        "    # 02 generate melspectrogram using librosa\n",
        "    y, sr = file_load(file_name)\n",
        "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, power=power)\n",
        "\n",
        "    # 03 convert melspectrogram to log mel energy\n",
        "    log_mel_spectrogram = 20.0 / power * numpy.log10(mel_spectrogram + sys.float_info.epsilon)\n",
        "\n",
        "    return log_mel_spectrogram\n",
        "\n",
        "  \n",
        "def list_to_vector_array(file_list, msg=\"calc...\", n_mels=64, n_fft=1024, hop_length=512, power=2.0, frames=10):\n",
        "    # iterate file_to_vector_array()\n",
        "    for idx in tqdm(range(len(file_list)), desc=msg):\n",
        "        vector_array = file_to_vector_array(file_list[idx], n_mels=n_mels, n_fft=n_fft, hop_length=hop_length, power=power)\n",
        "\n",
        "        if idx == 0:\n",
        "            dataset = numpy.zeros((len(file_list), n_mels, frames), float)\n",
        "        dataset[vector_array.shape[0] * idx: vector_array.shape[0] * (idx + 1), :] = vector_array\n",
        "    return dataset\n",
        "\n",
        "def key_by_id(item):\n",
        "  path_splitted = item.split(\"/\")\n",
        "  file_name = path_splitted[ len(path_splitted) - 1 ]\n",
        "  file_name_splitted = file_name.split(\"_\")\n",
        "  machine_id = file_name_splitted = file_name_splitted[2]\n",
        "  return machine_id\n",
        "\n",
        "def get_machine_id_list_for_test(target_dir,\n",
        "                                 dir_name=\"test\",\n",
        "                                 ext=\"wav\"):\n",
        "\n",
        "    # create test files\n",
        "    dir_path = os.path.abspath(\"{dir}/{dir_name}/*.{ext}\".format(dir=target_dir, dir_name=dir_name, ext=ext))\n",
        "    file_paths = sorted(glob.glob(dir_path))\n",
        "    # extract id\n",
        "    machine_id_list = sorted(list(set(itertools.chain.from_iterable(\n",
        "        [re.findall('id_[0-9][0-9]', ext_id) for ext_id in file_paths]))))\n",
        "    return machine_id_list\n",
        "\n",
        "def test_file_list_generator(target_dir,\n",
        "                             id_name,\n",
        "                             dir_name=\"test\",\n",
        "                             prefix_normal=\"normal\",\n",
        "                             prefix_anomaly=\"anomaly\",\n",
        "                             ext=\"wav\"):\n",
        "  \n",
        "    print(\"target_dir : {}\".format(target_dir+\"_\"+id_name))\n",
        "\n",
        "    normal_files = sorted(\n",
        "    glob.glob(\"{dir}/{dir_name}/{prefix_normal}_{id_name}*.{ext}\".format(dir=target_dir,\n",
        "                                                                                 dir_name=dir_name,\n",
        "                                                                                 prefix_normal=prefix_normal,\n",
        "                                                                                 id_name=id_name,\n",
        "                                                                                 ext=ext)))\n",
        "    normal_labels = numpy.zeros(len(normal_files))\n",
        "    anomaly_files = sorted(\n",
        "    glob.glob(\"{dir}/{dir_name}/{prefix_anomaly}_{id_name}*.{ext}\".format(dir=target_dir,\n",
        "                                                                                  dir_name=dir_name,\n",
        "                                                                                  prefix_anomaly=prefix_anomaly,\n",
        "                                                                                  id_name=id_name,\n",
        "                                                                                  ext=ext)))\n",
        "    anomaly_labels = numpy.ones(len(anomaly_files))\n",
        "    files = numpy.concatenate((normal_files, anomaly_files), axis=0)\n",
        "    labels = numpy.concatenate((normal_labels, anomaly_labels), axis=0)\n",
        "    print(\"test_file  num : {num}\".format(num=len(files)))\n",
        "    if len(files) == 0:\n",
        "        print(\"no_wav_file!!\")\n",
        "    print(\"\\n========================================\")\n",
        "\n",
        "    return files, labels"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQLl5W5n7_T5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed9d82f6-abbe-43cb-aba0-08baf3356e6a"
      },
      "source": [
        "target_dir = \"/content/drive/MyDrive/test/pump\"\n",
        "\n",
        "machine_type = os.path.split(target_dir)[1]\n",
        "print(\"============== MODEL LOAD ==============\")\n",
        "# set model path\n",
        "model_file = \"/content/drive/MyDrive/models/IDC-LSTM-AE/pump/2/model_pump.h5\"\n",
        "\n",
        "# load model file\n",
        "if not os.path.exists(model_file):\n",
        "  print(\"{} model not found \".format(machine_type))\n",
        "  sys.exit(-1)\n",
        "model = keras.models.load_model(model_file, custom_objects={'CustomModel': CustomModel, 'mse':mse_metric})\n",
        "# model.summary()\n",
        "\n",
        "machine_id_list = get_machine_id_list_for_test(target_dir)\n",
        "\n",
        "# initialize lines in csv for AUC and pAUC\n",
        "csv_lines = []\n",
        "\n",
        "csv_lines.append([machine_type])\n",
        "csv_lines.append([\"id\", \"AUC\", \"pAUC\"])\n",
        "performance = []\n",
        "\n",
        "for id_str in machine_id_list:\n",
        "  # load test file\n",
        "\n",
        "  id_num = id_str.split(\"_\")[1]\n",
        "\n",
        "  # Definizione della label \"match\" da utilizzare in fase di testing e del min e max da utilizzare per la normalizzazione\n",
        "  # i min e max sono stati calcolati a partire dai dati di training.\n",
        "  if id_num == \"00\":\n",
        "    match_labels = numpy.asarray([1,0,0,0])\n",
        "    mean = mean_00\n",
        "    std = std_00\n",
        "  if id_num == \"02\":\n",
        "    match_labels = numpy.asarray([0,1,0,0])\n",
        "    mean = mean_02\n",
        "    std = std_02\n",
        "  if id_num == \"04\":\n",
        "    match_labels = numpy.asarray([0,0,1,0])\n",
        "    mean = mean_04\n",
        "    std = std_04\n",
        "  if id_num == \"06\":\n",
        "    match_labels = numpy.asarray([0,0,0,1])\n",
        "    mean = mean_06\n",
        "    std = std_06\n",
        "\n",
        "  test_files, y_true = test_file_list_generator(target_dir, id_str)\n",
        "\n",
        "  # setup anomaly score file path\n",
        "  anomaly_score_csv = \"/content/drive/MyDrive/models/IDC-LSTM-AE/pump/2/anomaly_score_{machine_type}_{id_str}.csv\".format(machine_type=machine_type, id_str=id_str)\n",
        "  anomaly_score_list = []\n",
        "\n",
        "  print(\"\\n============== BEGIN TEST FOR A MACHINE ID {id} ==============\".format(id=id_num))\n",
        "\n",
        "  y_pred = [0. for k in test_files]\n",
        "\n",
        "  for file_idx, file_path in tqdm(enumerate(test_files), total=len(test_files)):\n",
        "\n",
        "    # Estrazione spettrogramma audio test\n",
        "    data = file_to_vector_array(file_path, n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH, power=POWER)\n",
        "    # Normalizzazione spettrogramma di test\n",
        "    data = ( data - mean ) / (std)\n",
        "\n",
        "    #print(data_aug_transpose.shape)\n",
        "\n",
        "    data_aug = numpy.zeros((12, 128, 32))\n",
        "    index = 0\n",
        "    i = 0\n",
        "    while (i+32) <= 313:\n",
        "      vector_i = numpy.zeros((128,32))\n",
        "      for j in range(0,128):\n",
        "        vector_i[j] = data[j][i:i+32]\n",
        "      data_aug[index] = vector_i\n",
        "      index += 1\n",
        "      i = i+25\n",
        "\n",
        "    data_aug_transpose = numpy.zeros((len(data_aug), 32, 128))\n",
        "    index = 0\n",
        "    for elem in data_aug:\n",
        "      data_aug_transpose[index] = elem.T\n",
        "      index += 1\n",
        "    #print(data_aug_transpose.shape)\n",
        "    \n",
        "    # Calcolo dell'errore medio sulle frame estratte dallo spettrogramma\n",
        "    elem_error = []\n",
        "    for elem in data_aug_transpose:\n",
        "      predicted = model.predict([elem.reshape(1,32,128), match_labels.reshape(1,4)])\n",
        "      errors = numpy.mean(numpy.square(elem - predicted), axis=1)\n",
        "      elem_error.append(numpy.mean(errors))\n",
        "\n",
        "    # Log dell'errore associato all'istanza di test\n",
        "    y_pred[file_idx] = numpy.mean(errors)\n",
        "    anomaly_score_list.append([os.path.basename(file_path), y_pred[file_idx]])\n",
        "  \n",
        "  save_csv(save_file_path=anomaly_score_csv, save_data=anomaly_score_list)\n",
        "    \n",
        "  # Calcolo AUC e pAUC per i dati con un certo ID_0x\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  p_auc = metrics.roc_auc_score(y_true, y_pred, max_fpr=0.1)\n",
        "  csv_lines.append([id_str.split(\"_\", 1)[1], auc, p_auc])\n",
        "  performance.append([auc, p_auc])\n",
        "  print(\"AUC : {}\".format(auc))\n",
        "  print(\"pAUC : {}\".format(p_auc))\n",
        "\n",
        "  print(\"\\n============ END OF TEST FOR A MACHINE ID ============\")\n",
        "\n",
        "# Stampa di AUC e pAUC medi su tutti i dati di test (media di AUC e pAUC sui vari ID).\n",
        "print(\"\\n============ AVERAGE PERFORMANCES ============\")\n",
        "averaged_performance = numpy.mean(numpy.array(performance, dtype=float), axis=0)\n",
        "csv_lines.append([\"Average\"] + list(averaged_performance))\n",
        "csv_lines.append([])\n",
        "print(averaged_performance)\n",
        "\n",
        "result_path = \"/content/drive/MyDrive/models/IDC-LSTM-AE/pump/2/anomaly_score_avg.csv\"\n",
        "save_csv(save_file_path=result_path, save_data=csv_lines)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============== MODEL LOAD ==============\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/243 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "target_dir : /content/drive/MyDrive/test/pump_id_00\n",
            "test_file  num : 243\n",
            "\n",
            "========================================\n",
            "\n",
            "============== BEGIN TEST FOR A MACHINE ID 00 ==============\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 243/243 [07:38<00:00,  1.89s/it]\n",
            "  0%|          | 0/211 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AUC : 0.7791608391608392\n",
            "pAUC : 0.6429885903570114\n",
            "\n",
            "============ END OF TEST FOR A MACHINE ID ============\n",
            "target_dir : /content/drive/MyDrive/test/pump_id_02\n",
            "test_file  num : 211\n",
            "\n",
            "========================================\n",
            "\n",
            "============== BEGIN TEST FOR A MACHINE ID 02 ==============\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 211/211 [06:08<00:00,  1.75s/it]\n",
            "  0%|          | 0/200 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AUC : 0.6415315315315315\n",
            "pAUC : 0.5642484589853011\n",
            "\n",
            "============ END OF TEST FOR A MACHINE ID ============\n",
            "target_dir : /content/drive/MyDrive/test/pump_id_04\n",
            "test_file  num : 200\n",
            "\n",
            "========================================\n",
            "\n",
            "============== BEGIN TEST FOR A MACHINE ID 04 ==============\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [05:53<00:00,  1.77s/it]\n",
            "  0%|          | 0/202 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AUC : 0.9692000000000001\n",
            "pAUC : 0.9615789473684211\n",
            "\n",
            "============ END OF TEST FOR A MACHINE ID ============\n",
            "target_dir : /content/drive/MyDrive/test/pump_id_06\n",
            "test_file  num : 202\n",
            "\n",
            "========================================\n",
            "\n",
            "============== BEGIN TEST FOR A MACHINE ID 06 ==============\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 202/202 [05:41<00:00,  1.69s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AUC : 0.7999999999999999\n",
            "pAUC : 0.6646026831785345\n",
            "\n",
            "============ END OF TEST FOR A MACHINE ID ============\n",
            "\n",
            "============ AVERAGE PERFORMANCES ============\n",
            "[0.79747309 0.70835467]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}