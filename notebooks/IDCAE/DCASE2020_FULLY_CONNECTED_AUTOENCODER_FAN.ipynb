{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DCASE2020_FULLY_CONNECTED_AUTOENCODER_FAN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "hItH65INljvG",
        "PM9bEsD3Eo2x",
        "QdtRhIAYXsLn",
        "sN8yMBbSLYb1"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hItH65INljvG"
      },
      "source": [
        "#IMPORT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5tvHOgSnS1i",
        "outputId": "4d666bcb-0b0c-4123-ccc9-179dc2d76127"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kq5KmwSlEv8v"
      },
      "source": [
        "# import necessari\n",
        "import librosa\n",
        "import numpy\n",
        "import sys\n",
        "import os\n",
        "import glob\n",
        "import itertools\n",
        "import re\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.models\n",
        "import tensorflow.keras.backend as K\n",
        "import keras.optimizers\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, BatchNormalization, Activation, Flatten, Multiply, Add, Reshape\n",
        "from tqdm import tqdm\n",
        "from itertools import groupby\n",
        "from keras.utils import to_categorical\n",
        "from sklearn import metrics\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0PLnQAqM3m4"
      },
      "source": [
        "# costanti \n",
        "ALPHA = 0.75\n",
        "N_MELS = 128\n",
        "HOP_LENGTH = 512\n",
        "N_FFT = 1024\n",
        "POWER = 2.0\n",
        "FRAME_NUMS = 313\n",
        "VAL = 0.05"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PM9bEsD3Eo2x"
      },
      "source": [
        "# DATA LOADING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGMOe52TFiAH"
      },
      "source": [
        "# Loading da Google Drive\n",
        "train_data = numpy.load(\"/content/drive/MyDrive/DCASE_DATA_EXTRACTED/train/training_fan.npy\")\n",
        "grouped_list_by_machine_id = pickle.load( open( \"/content/drive/MyDrive/DCASE_DATA_EXTRACTED/train/training_fan_grouped_list.npy\", \"rb\" ) )"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1LgXuBycWdr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a683b47e-ab02-4b77-8de3-23b8f2c1a0e8"
      },
      "source": [
        "# GENERAZIONE DELLE LABELS\n",
        "# One-hot encoding\n",
        "label = []\n",
        "choices = []\n",
        "for i in range(0, len(grouped_list_by_machine_id)):\n",
        "  for j in range(0, len(grouped_list_by_machine_id[i])):\n",
        "    machine_id = grouped_list_by_machine_id[i][j].split('/')[7].split('_')[2]\n",
        "    #print(grouped_list_by_machine_id[i][j].split('/')[7])\n",
        "    random_choice = numpy.random.choice([\"match\", \"non_match\"], p = [ALPHA, 1-ALPHA]) \n",
        "\n",
        "    if machine_id == '00':\n",
        "      if random_choice == \"match\":\n",
        "        to_append = [1,0,0,0]\n",
        "      else: \n",
        "        not_match_label = numpy.random.choice([1, 2, 3]) \n",
        "        if not_match_label == 1:\n",
        "          to_append = [0,1,0,0]\n",
        "        elif not_match_label == 2:\n",
        "          to_append = [0,0,1,0]\n",
        "        else: \n",
        "          to_append = [0,0,0,1]\n",
        "\n",
        "    elif machine_id == '02': \n",
        "\n",
        "      if random_choice == \"match\":\n",
        "        to_append = [0,1,0,0]\n",
        "      else: \n",
        "        not_match_label = numpy.random.choice( [ 1, 2, 3] ) \n",
        "        if not_match_label == 1:\n",
        "          to_append = [1,0,0,0]\n",
        "        elif not_match_label == 2:\n",
        "          to_append = [0,0,1,0]\n",
        "        else: \n",
        "          to_append = [0,0,0,1]\n",
        "\n",
        "    elif machine_id == \"04\":\n",
        "      \n",
        "      if random_choice == \"match\":\n",
        "        to_append = [0,0,1,0]\n",
        "      else: \n",
        "        not_match_label = numpy.random.choice( [ 1, 2, 3] ) \n",
        "        if not_match_label == 1:\n",
        "          to_append = [1,0,0,0]\n",
        "        elif not_match_label == 2:\n",
        "          to_append = [0,1,0,0]\n",
        "        else: \n",
        "          to_append = [0,0,0,1]\n",
        "\n",
        "    elif machine_id == \"06\":\n",
        "      if random_choice == \"match\":\n",
        "        to_append = [0,0,0,1]\n",
        "      else: \n",
        "        not_match_label = numpy.random.choice( [ 1, 2, 3] ) \n",
        "        if not_match_label == 1:\n",
        "          to_append = [1,0,0,0]\n",
        "        elif not_match_label == 2:\n",
        "          to_append = [0,1,0,0]\n",
        "        else: \n",
        "          to_append = [0,0,1,0]\n",
        "    \n",
        "    label.append(to_append) # Append della label associata a ciascuno spettrogramma\n",
        "    choices.append(random_choice) # Append della choice utilizzata per associare la label\n",
        "                                  # La choice sarà utile in fase di addestramento per capire che tipo di loss calcolare\n",
        "\n",
        "# Trasformazione in numpy.array     \n",
        "label = numpy.asarray(label)\n",
        "choices = numpy.asarray(choices)\n",
        "print(label.shape)\n",
        "print(choices.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3675, 4)\n",
            "(3675,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8ZoHhepkGk8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bbe7ad6-082c-4e9d-fb2c-4e6a294216c7"
      },
      "source": [
        "print(len(grouped_list_by_machine_id[0]))\n",
        "print(len(grouped_list_by_machine_id[1]))\n",
        "print(len(grouped_list_by_machine_id[2]))\n",
        "print(len(grouped_list_by_machine_id[3]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "911\n",
            "916\n",
            "933\n",
            "915\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjCvfyq2jA_b",
        "outputId": "578ec838-f7bc-40c5-e04a-db13488e1cb0"
      },
      "source": [
        "# Estrazione spettrogrammi divisi per ID\n",
        "id_00 = train_data[0:911]\n",
        "label_00 = label[0:911]\n",
        "choices_00 = choices[0:911]\n",
        "\n",
        "id_02 = train_data[911:1827]\n",
        "label_02 = label[911:1827]\n",
        "choices_02 = choices[911:1827]\n",
        "\n",
        "id_04 = train_data[1827:2760]\n",
        "label_04 = label[1827:2760]\n",
        "choices_04 = choices[1827:2760]\n",
        "\n",
        "id_06 = train_data[2760:3675]\n",
        "label_06 = label[2760:3675]\n",
        "choices_06 = choices[2760:3675]\n",
        "\n",
        "id_00_training, \\\n",
        "id_00_validation, \\\n",
        "label_00_train, \\\n",
        "label_00_validation, \\\n",
        "choices_00_train, \\\n",
        "choices_00_validation = train_test_split(id_00, label_00, choices_00, test_size=0.33, random_state=42)\n",
        "\n",
        "id_02_training, \\\n",
        "id_02_validation, \\\n",
        "label_02_train, \\\n",
        "label_02_validation, \\\n",
        "choices_02_train, \\\n",
        "choices_02_validation = train_test_split(id_02, label_02, choices_02, test_size=0.33, random_state=42)\n",
        "\n",
        "id_04_training, \\\n",
        "id_04_validation, \\\n",
        "label_04_train, \\\n",
        "label_04_validation, \\\n",
        "choices_04_train, \\\n",
        "choices_04_validation = train_test_split(id_04, label_04, choices_04, test_size=0.33, random_state=42)\n",
        "\n",
        "id_06_training, \\\n",
        "id_06_validation, \\\n",
        "label_06_train, \\\n",
        "label_06_validation, \\\n",
        "choices_06_train, \\\n",
        "choices_06_validation = train_test_split(id_06, label_06, choices_06, test_size=0.33, random_state=42)\n",
        "\n",
        "# Min-Max Normalization ID_00\n",
        "id_00_norm = numpy.empty_like(id_00_training)\n",
        "max_00 = numpy.max(id_00_training)\n",
        "min_00 = numpy.min(id_00_training)\n",
        "id_00_norm = (id_00_training - min_00) / (max_00-min_00)\n",
        "id_00_norm_validation = (id_00_validation - min_00) / (max_00-min_00)\n",
        "\n",
        "# Min-Max Normalization ID_02\n",
        "id_02_norm = numpy.empty_like(id_02_training)\n",
        "max_02 = numpy.max(id_02_training)\n",
        "min_02 = numpy.min(id_02_training)\n",
        "id_02_norm = (id_02_training - min_02) / (max_02-min_02)\n",
        "id_02_norm_validation = (id_02_validation - min_02) / (max_02-min_02)\n",
        "\n",
        "# Min-Max Normalization ID_04\n",
        "id_04_norm = numpy.empty_like(id_04_training)\n",
        "max_04 = numpy.max(id_04_training)\n",
        "min_04 = numpy.min(id_04_training)\n",
        "id_04_norm = (id_04_training - min_04) / (max_04-min_04)\n",
        "id_04_norm_validation = (id_04_validation - min_04) / (max_04-min_04)\n",
        "\n",
        "# Min-Max Normalization ID_06\n",
        "id_06_norm = numpy.empty_like(id_06_training)\n",
        "max_06 = numpy.max(id_06_training)\n",
        "min_06 = numpy.min(id_06_training)\n",
        "id_06_norm = (id_06_training - min_06) / (max_06-min_06)\n",
        "id_06_norm_validation = (id_06_validation - min_06) / (max_06-min_06)\n",
        "\n",
        "print(\"==== DATA ====\")\n",
        "total_training = numpy.concatenate([id_00_norm, id_02_norm, id_04_norm, id_06_norm])\n",
        "print(total_training.shape)\n",
        "total_validation = numpy.concatenate([id_00_norm_validation, id_02_norm_validation, id_04_norm_validation, id_06_norm_validation])\n",
        "print(total_validation.shape)\n",
        "\n",
        "print(\"==== LABELS ====\")\n",
        "total_training_label = numpy.concatenate([label_00_train, label_02_train, label_04_train, label_06_train])\n",
        "print(total_training_label.shape)\n",
        "total_validation_label = numpy.concatenate([label_00_validation, label_02_validation, label_04_validation, label_06_validation])\n",
        "print(total_validation_label.shape)\n",
        "\n",
        "print(\"==== CHOICES ====\")\n",
        "total_training_choices = numpy.concatenate([choices_00_train, choices_02_train, choices_04_train, choices_06_train])\n",
        "print(total_training_choices.shape)\n",
        "total_validation_choices = numpy.concatenate([choices_00_validation, choices_02_validation, choices_04_validation, choices_06_validation])\n",
        "print(total_validation_choices.shape)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==== DATA ====\n",
            "(2461, 128, 313)\n",
            "(1214, 128, 313)\n",
            "==== LABELS ====\n",
            "(2461, 4)\n",
            "(1214, 4)\n",
            "==== CHOICES ====\n",
            "(2461,)\n",
            "(1214,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkFwLBYH89tx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "289184c3-58c1-4f04-f8e0-b4b428cef1bf"
      },
      "source": [
        "# DATA AUGMENTATION\n",
        "\n",
        "####### DATA ######\n",
        "# Estrazione frame 128x10 da ciascun spettrogramma\n",
        "training_data = numpy.zeros((len(total_training)*61, 128, 10)) # Dataset utilizzato per il training\n",
        "index = 0\n",
        "for vector_array in total_training:\n",
        "  i = 0\n",
        "  while i < 303:\n",
        "    vector_i = numpy.zeros((128,10))\n",
        "    for j in range(0,128):\n",
        "      vector_i[j] = vector_array[j][i:i+10]\n",
        "    training_data[index] = vector_i\n",
        "    index += 1\n",
        "    i = i+5\n",
        "\n",
        "validation_data = numpy.zeros((len(total_validation)*61, 128, 10)) # Dataset utilizzato per il training\n",
        "index = 0\n",
        "for vector_array in total_validation:\n",
        "  i = 0\n",
        "  while i < 303:\n",
        "    vector_i = numpy.zeros((128,10))\n",
        "    for j in range(0,128):\n",
        "      vector_i[j] = vector_array[j][i:i+10]\n",
        "    validation_data[index] = vector_i\n",
        "    index += 1\n",
        "    i = i+5\n",
        "###################\n",
        "\n",
        "####### LABELS ######\n",
        "# Associazione della label associata a ciascun spettrogramma a ciascuno dei frame estratto da esso.\n",
        "training_labels = []\n",
        "for elem in total_training_label:\n",
        "  if numpy.array_equal(elem, numpy.asarray([1,0,0,0])) :\n",
        "    for i in range(61):\n",
        "      training_labels.append([1,0,0,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,1,0,0])):\n",
        "    for i in range(61):\n",
        "      training_labels.append([0,1,0,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,0,1,0])):\n",
        "    for i in range(61):\n",
        "      training_labels.append([0,0,1,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,0,0,1])):\n",
        "    for i in range(61):\n",
        "      training_labels.append([0,0,0,1])\n",
        "\n",
        "validation_labels = []\n",
        "for elem in total_validation_label:\n",
        "  if numpy.array_equal(elem, numpy.asarray([1,0,0,0])) :\n",
        "    for i in range(61):\n",
        "      validation_labels.append([1,0,0,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,1,0,0])):\n",
        "    for i in range(61):\n",
        "      validation_labels.append([0,1,0,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,0,1,0])):\n",
        "    for i in range(61):\n",
        "      validation_labels.append([0,0,1,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,0,0,1])):\n",
        "    for i in range(61):\n",
        "      validation_labels.append([0,0,0,1])\n",
        "\n",
        "training_labels = numpy.asarray(training_labels) # Dataset utilizzato per il training\n",
        "validation_labels = numpy.asarray(validation_labels) # Dataset utilizzato per il training\n",
        "#####################\n",
        "\n",
        "\n",
        "####### CHOICES ######\n",
        "# Associazione della choice associata a ciascun spettrogramma a ciascuno dei frame estratto da esso. \n",
        "training_choices = []\n",
        "for elem in total_training_choices:\n",
        "  if numpy.array_equal(elem, numpy.asarray(\"match\")) :\n",
        "    for i in range(61):\n",
        "      training_choices.append(\"match\")\n",
        "  elif numpy.array_equal(elem, numpy.asarray(\"non_match\")):\n",
        "    for i in range(61):\n",
        "      training_choices.append(\"non_match\")\n",
        "\n",
        "validation_choices = []\n",
        "for elem in total_validation_choices:\n",
        "  if numpy.array_equal(elem, numpy.asarray(\"match\")) :\n",
        "    for i in range(61):\n",
        "      validation_choices.append(\"match\")\n",
        "  elif numpy.array_equal(elem, numpy.asarray(\"non_match\")):\n",
        "    for i in range(61):\n",
        "      validation_choices.append(\"non_match\")\n",
        "\n",
        "training_choices = numpy.asarray(training_choices) # Dataset utilizzato per il training\n",
        "validation_choices = numpy.asarray(validation_choices) # Dataset utilizzato per il training\n",
        "######################\n",
        "\n",
        "print(\"==== DATA ====\")\n",
        "print(training_data.shape)\n",
        "print(validation_data.shape)\n",
        "\n",
        "print(\"==== LABELS ====\")\n",
        "print(training_labels.shape)\n",
        "print(validation_labels.shape)\n",
        "\n",
        "print(\"==== CHOICES ====\")\n",
        "print(training_choices.shape)\n",
        "print(validation_choices.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==== DATA ====\n",
            "(150121, 128, 10)\n",
            "(74054, 128, 10)\n",
            "==== LABELS ====\n",
            "(150121, 4)\n",
            "(74054, 4)\n",
            "==== CHOICES ====\n",
            "(150121,)\n",
            "(74054,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdtRhIAYXsLn"
      },
      "source": [
        "# KERAS MODEL DEFINITION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96e3XmeeX2GS"
      },
      "source": [
        "# LAYER DEFINITION\n",
        "\n",
        "def DenseBlock(input,n):\n",
        "  x = Dense(n)(input)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  return x\n",
        "\n",
        "input_Spect = Input(shape = [128, 10])\n",
        "input_Label = Input(shape = [4,])\n",
        "\n",
        "# First Branch - Encoder\n",
        "m = Flatten(input_shape = [128, 10])(input_Spect)\n",
        "m = DenseBlock(m, 128)\n",
        "m = DenseBlock(m, 64)\n",
        "m = DenseBlock(m, 32)\n",
        "m = DenseBlock(m, 16)\n",
        "\n",
        "# Second Branch - Conditioning Feed Forward Neural Network\n",
        "x = Dense(16)(input_Label)\n",
        "x = Activation('sigmoid')(x)\n",
        "q = Dense(16)(input_Label)\n",
        "\n",
        "# Encoded Input Conditioning\n",
        "m = Multiply()([x,m])\n",
        "encoded_input_conditioned = Add()([q, m]) # Input da passare al decoder\n",
        "\n",
        "# Decoder\n",
        "m = DenseBlock(encoded_input_conditioned, 128)\n",
        "m = DenseBlock(m, 128)\n",
        "m = DenseBlock(m, 128)\n",
        "m = DenseBlock(m, 128)\n",
        "m = Dense(128*10)(m)\n",
        "m = Reshape((128,10),  input_shape=(128*10,))(m) # Output del modello\n",
        "\n",
        "loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "mse_metric = keras.metrics.MeanSquaredError(name=\"mse\")\n",
        "\n",
        "class CustomModel(tensorflow.keras.Model):\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [loss_tracker, mse_metric]\n",
        "\n",
        "    def test_step(self, data):\n",
        "        # Unpack the data\n",
        "        x, y = data\n",
        "        # Compute predictions\n",
        "        y_pred = self([x[0],x[1]], training=False)\n",
        "        # Indici match\n",
        "        match = tf.where ( tf.equal(x[2][:], \"match\") )\n",
        "        # Dati match\n",
        "        data_match = K.gather(y, match)\n",
        "        # Separazione dei dati PREDETTI sulla base degli indici relativi a match/non_match\n",
        "        # Dati match\n",
        "        pred_match = K.gather(y_pred, match)\n",
        "\n",
        "        # Update metrica\n",
        "        mse_metric.update_state(data_match, pred_match)\n",
        "\n",
        "        return {\"mse\": mse_metric.result()}\n",
        "    \n",
        "    def train_step(self, data):\n",
        "          # Unpack the data. Its structure depends on your model and on what you pass to `fit()`.\n",
        "          x, y = data\n",
        "\n",
        "          # Vettore C utilizzato per il calcolo della loss in caso di non_match\n",
        "          C = 5 \n",
        "          # Valore di probabilità utilizzato come peso\n",
        "          ALPHA = 0.75 \n",
        "\n",
        "          # Indici match\n",
        "          match = tf.where ( tf.equal(x[2][:], \"match\") )\n",
        "\n",
        "          # Indici non_match\n",
        "          not_match = tf.where ( tf.equal(x[2][:], \"non_match\") )\n",
        "\n",
        "          # Dati match\n",
        "          data_match = K.gather(y, match)\n",
        "\n",
        "          with tf.GradientTape() as tape:\n",
        "              y_pred = self([x[0],x[1]], training=True)  # Forward pass\n",
        "\n",
        "              # Separazione dei dati PREDETTI sulla base degli indici relativi a match/non_match\n",
        "              # Dati match\n",
        "              pred_match = K.gather(y_pred, match)\n",
        "              # Dati non match\n",
        "              pred_not_match = K.gather(y_pred, not_match) \n",
        "\n",
        "              loss_m = K.mean(keras.losses.mean_squared_error(data_match, pred_match)) + 1e-6  # Calcolo Loss Match\n",
        "              loss_nm = K.mean(keras.losses.mean_squared_error(C,pred_not_match)) + 1e-6     # Calcolo Loss Non_Match\n",
        "\n",
        "              loss = ALPHA * loss_m + (1 - ALPHA) * loss_nm     # loss utilizzata per l'update dei pesi\n",
        "\n",
        "          # Compute gradients\n",
        "          trainable_vars = self.trainable_variables\n",
        "          gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "          # Update weights\n",
        "          self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "          # Compute our own metrics\n",
        "          loss_tracker.update_state(loss)\n",
        "          mse_metric.update_state(y, y_pred)\n",
        "          return {\"loss\": loss_tracker.result(), \"mse\": mse_metric.result()}\n",
        "\n",
        "def get_lr_metric(optimizer):\n",
        "    def lr(y_true, y_pred):\n",
        "        return optimizer._decayed_lr(tf.float32) # I use ._decayed_lr method instead of .lr\n",
        "    return lr\n",
        "\n",
        "opt = keras.optimizers.Adam(\n",
        "    learning_rate = 0.00001,\n",
        "    beta_1=0.95,\n",
        "    beta_2=0.999\n",
        ")\n",
        "\n",
        "lr_metric = get_lr_metric(opt)\n",
        "model = CustomModel(inputs=(input_Spect, input_Label), outputs = m)\n",
        "model.compile(optimizer = opt, metrics=[\"mse\", lr_metric])\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HjXJIU4bvUH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c50c626-153d-410c-b549-05a5836de2cb"
      },
      "source": [
        "history = model.fit([training_data, training_labels, training_choices], \n",
        "          training_data, \n",
        "          epochs=100, \n",
        "          batch_size=128, \n",
        "          validation_data=([validation_data, validation_labels, validation_choices],validation_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1173/1173 [==============================] - 19s 12ms/step - loss: 4.9262 - mse: 0.3266 - val_mse: 0.2516\n",
            "Epoch 2/100\n",
            "1173/1173 [==============================] - 13s 11ms/step - loss: 1.4780 - mse: 2.2186 - val_mse: 0.2560\n",
            "Epoch 3/100\n",
            "1173/1173 [==============================] - 13s 11ms/step - loss: 0.5251 - mse: 4.1918 - val_mse: 0.2474\n",
            "Epoch 4/100\n",
            "1173/1173 [==============================] - ETA: 0s - loss: 0.3519 - mse: 4.6485"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktyeNLjDcxJZ"
      },
      "source": [
        "# Salvataggio del modello\n",
        "model.save('/content/drive/MyDrive/models/IDCAE/fan/1/model_fan.h5')\n",
        "\n",
        "# Salvataggio history di apprendimento\n",
        "with open('/content/drive/MyDrive/models/IDCAE/fan/1/trainHistoryDict', 'wb') as file_pi:\n",
        "    pickle.dump(history.history, file_pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sN8yMBbSLYb1"
      },
      "source": [
        "# TESTING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "alzE-821wFIi",
        "outputId": "59e2e81c-0c0a-4de8-e676-127dc572ff4f"
      },
      "source": [
        "import csv\n",
        "\n",
        "def save_csv(save_file_path,\n",
        "             save_data):\n",
        "    with open(save_file_path, \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f, lineterminator='\\n')\n",
        "        writer.writerows(save_data)\n",
        "\n",
        "# load dataset\n",
        "def select_dirs(path):\n",
        "    dir_path = os.path.abspath(path)\n",
        "    dirs = sorted(glob.glob(dir_path))\n",
        "    return dirs\n",
        "\n",
        "def file_load(wav_name, mono=False):\n",
        "    try:\n",
        "        return librosa.load(wav_name, sr=None, mono=mono)\n",
        "    except:\n",
        "        logger.error(\"file_broken or not exists!! : {}\".format(wav_name))\n",
        "\n",
        "def file_list_generator(target_dir, dir_name=\"train\", ext=\"wav\"):\n",
        "    print(\"target_dir : {}\".format(target_dir))\n",
        "\n",
        "    # generate training list\n",
        "    training_list_path = os.path.abspath(\"{dir}/{dir_name}/*.{ext}\".format(dir=target_dir, dir_name=dir_name, ext=ext))\n",
        "    files = sorted(glob.glob(training_list_path))\n",
        "    if len(files) == 0:\n",
        "      print(\"errore\")\n",
        "    return files\n",
        "\n",
        "\n",
        "def file_to_vector_array(file_name, n_mels=64, n_fft=1024, hop_length=512, power=2.0):\n",
        "    # 02 generate melspectrogram using librosa\n",
        "    y, sr = file_load(file_name)\n",
        "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, power=power)\n",
        "\n",
        "    # 03 convert melspectrogram to log mel energy\n",
        "    log_mel_spectrogram = 20.0 / power * numpy.log10(mel_spectrogram + sys.float_info.epsilon)\n",
        "\n",
        "    return log_mel_spectrogram\n",
        "\n",
        "  \n",
        "def list_to_vector_array(file_list, msg=\"calc...\", n_mels=64, n_fft=1024, hop_length=512, power=2.0, frames=10):\n",
        "    # iterate file_to_vector_array()\n",
        "    for idx in tqdm(range(len(file_list)), desc=msg):\n",
        "        vector_array = file_to_vector_array(file_list[idx], n_mels=n_mels, n_fft=n_fft, hop_length=hop_length, power=power)\n",
        "       \n",
        "        # vector_array = numpy.delete(vector_array,[310,311,312], axis=1)\n",
        "        # vector_array = numpy.asarray(numpy.hsplit(vector_array, 31))\n",
        "\n",
        "        if idx == 0:\n",
        "            dataset = numpy.zeros((len(file_list), n_mels, frames), float)\n",
        "        dataset[vector_array.shape[0] * idx: vector_array.shape[0] * (idx + 1), :] = vector_array\n",
        "    return dataset\n",
        "\n",
        "def key_by_id(item):\n",
        "  path_splitted = item.split(\"/\")\n",
        "  file_name = path_splitted[ len(path_splitted) - 1 ]\n",
        "  file_name_splitted = file_name.split(\"_\")\n",
        "  machine_id = file_name_splitted = file_name_splitted[2]\n",
        "  return machine_id\n",
        "\n",
        "def get_machine_id_list_for_test(target_dir,\n",
        "                                 dir_name=\"test\",\n",
        "                                 ext=\"wav\"):\n",
        "\n",
        "    # create test files\n",
        "    dir_path = os.path.abspath(\"{dir}/{dir_name}/*.{ext}\".format(dir=target_dir, dir_name=dir_name, ext=ext))\n",
        "    file_paths = sorted(glob.glob(dir_path))\n",
        "    # extract id\n",
        "    machine_id_list = sorted(list(set(itertools.chain.from_iterable(\n",
        "        [re.findall('id_[0-9][0-9]', ext_id) for ext_id in file_paths]))))\n",
        "    return machine_id_list\n",
        "\n",
        "def test_file_list_generator(target_dir,\n",
        "                             id_name,\n",
        "                             dir_name=\"test\",\n",
        "                             prefix_normal=\"normal\",\n",
        "                             prefix_anomaly=\"anomaly\",\n",
        "                             ext=\"wav\"):\n",
        "  \n",
        "    print(\"target_dir : {}\".format(target_dir+\"_\"+id_name))\n",
        "\n",
        "    normal_files = sorted(\n",
        "    glob.glob(\"{dir}/{dir_name}/{prefix_normal}_{id_name}*.{ext}\".format(dir=target_dir,\n",
        "                                                                                 dir_name=dir_name,\n",
        "                                                                                 prefix_normal=prefix_normal,\n",
        "                                                                                 id_name=id_name,\n",
        "                                                                                 ext=ext)))\n",
        "    normal_labels = numpy.zeros(len(normal_files))\n",
        "    anomaly_files = sorted(\n",
        "    glob.glob(\"{dir}/{dir_name}/{prefix_anomaly}_{id_name}*.{ext}\".format(dir=target_dir,\n",
        "                                                                                  dir_name=dir_name,\n",
        "                                                                                  prefix_anomaly=prefix_anomaly,\n",
        "                                                                                  id_name=id_name,\n",
        "                                                                                  ext=ext)))\n",
        "    anomaly_labels = numpy.ones(len(anomaly_files))\n",
        "    files = numpy.concatenate((normal_files, anomaly_files), axis=0)\n",
        "    labels = numpy.concatenate((normal_labels, anomaly_labels), axis=0)\n",
        "    print(\"test_file  num : {num}\".format(num=len(files)))\n",
        "    if len(files) == 0:\n",
        "        print(\"no_wav_file!!\")\n",
        "    print(\"\\n========================================\")\n",
        "\n",
        "    return files, labels\n",
        "\n",
        "target_dir = \"/content/drive/MyDrive/test/fan\"\n",
        "\n",
        "performance = []\n",
        "machine_type = os.path.split(target_dir)[1]\n",
        "print(\"============== MODEL LOAD ==============\")\n",
        "# set model path\n",
        "model_file = \"/content/drive/MyDrive/models/IDCAE/fan/1/model_fan.h5\"\n",
        "\n",
        "# load model file\n",
        "if not os.path.exists(model_file):\n",
        "  print(\"{} model not found \".format(machine_type))\n",
        "  sys.exit(-1)\n",
        "model = tensorflow.keras.models.load_model(model_file, custom_objects={'CustomModel': CustomModel, 'mse':mse_metric, 'lr': lr_metric})\n",
        "# model.summary()\n",
        "\n",
        "machine_id_list = get_machine_id_list_for_test(target_dir)\n",
        "\n",
        "# initialize lines in csv for AUC and pAUC\n",
        "csv_lines = []\n",
        "\n",
        "csv_lines.append([machine_type])\n",
        "csv_lines.append([\"id\", \"AUC\", \"pAUC\"])\n",
        "performance = []\n",
        "\n",
        "for id_str in machine_id_list:\n",
        "  # load test file\n",
        "\n",
        "  id_num = id_str.split(\"_\")[1]\n",
        "\n",
        "  # Definizione della label \"match\" da utilizzare in fase di testing e del min e max da utilizzare per la normalizzazione\n",
        "  # i min e max sono stati calcolati a partire dai dati di training.\n",
        "  if id_num == \"00\":\n",
        "    match_labels = numpy.asarray([1,0,0,0])\n",
        "    max = max_00\n",
        "    min = min_00\n",
        "  elif id_num == \"02\":\n",
        "    match_labels = numpy.asarray([0,1,0,0])\n",
        "    max = max_02\n",
        "    min = min_02\n",
        "  elif id_num == \"04\":\n",
        "    match_labels = numpy.asarray([0,0,1,0])\n",
        "    max = max_04\n",
        "    min = min_04\n",
        "  elif id_num == \"06\": \n",
        "    match_labels = numpy.asarray([0,0,0,1])\n",
        "    max = max_06\n",
        "    min = min_06\n",
        "\n",
        "  test_files, y_true = test_file_list_generator(target_dir, id_str)\n",
        "  #print(\"\\n====== True Labels ======\")\n",
        "  #print(y_true)\n",
        "  #print(\"==> ====== Match ID Labels ======\")\n",
        "  #print(match_labels.shape)\n",
        "  #print(\"=================================\\n\")\n",
        "\n",
        "  # setup anomaly score file path\n",
        "  anomaly_score_csv = \"/content/drive/MyDrive/models/IDCAE/fan/1/anomaly_score_{machine_type}_{id_str}.csv\".format(machine_type=machine_type, id_str=id_str)\n",
        "  anomaly_score_list = []\n",
        "\n",
        "  print(\"\\n============== BEGIN TEST FOR A MACHINE ID {id} ==============\".format(id=id_num))\n",
        "\n",
        "  y_pred = [0. for k in test_files]\n",
        "\n",
        "  for file_idx, file_path in tqdm(enumerate(test_files), total=len(test_files)):\n",
        "\n",
        "    # Estrazione spettrogramma audio test\n",
        "    data = file_to_vector_array(file_path, n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH, power=POWER)\n",
        "\n",
        "    # Normalizzazione spettrogramma di test\n",
        "    data = ( data - min ) / ( max - min )\n",
        "\n",
        "    # Estrazione delle frame 128x10\n",
        "    data_splitted = numpy.zeros((61, 128, 10))\n",
        "    index = 0\n",
        "    i = 0\n",
        "    while i < 303:\n",
        "      vector_i = numpy.zeros((128,10))\n",
        "      for j in range(0,128):\n",
        "        vector_i[j] = data[j][i:i+10]\n",
        "      data_splitted[index] = vector_i\n",
        "      index += 1\n",
        "      i = i+5\n",
        "\n",
        "    # Calcolo dell'errore medio sulle frame estratte dallo spettrogramma\n",
        "    elem_error = []\n",
        "    for elem in data_splitted:\n",
        "      predicted = model.predict([elem.reshape(1,128,10), match_labels.reshape(1,4)])\n",
        "\n",
        "      errors = numpy.mean(numpy.square(elem - predicted), axis=1)\n",
        "      elem_error.append(numpy.mean(errors))\n",
        "    # Log dell'errore associato all'istanza di test\n",
        "    y_pred[file_idx] = numpy.mean(elem_error)\n",
        "    anomaly_score_list.append([os.path.basename(file_path), y_pred[file_idx]])\n",
        "  \n",
        "  save_csv(save_file_path=anomaly_score_csv, save_data=anomaly_score_list)\n",
        "\n",
        " # Calcolo AUC e pAUC per i dati con un certo ID_0x\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  p_auc = metrics.roc_auc_score(y_true, y_pred, max_fpr=0.1)\n",
        "  csv_lines.append([id_str.split(\"_\", 1)[1], auc, p_auc])\n",
        "  performance.append([auc, p_auc])\n",
        "  print(\"AUC : {}\".format(auc))\n",
        "  print(\"pAUC : {}\".format(p_auc))\n",
        "\n",
        "  print(\"\\n============ END OF TEST FOR A MACHINE ID ============\")\n",
        "\n",
        "# Stampa di AUC e pAUC medi su tutti i dati di test (media di AUC e pAUC sui vari ID).\n",
        "print(\"\\n============ AVERAGE PERFORMANCES ============\")\n",
        "averaged_performance = numpy.mean(numpy.array(performance, dtype=float), axis=0)\n",
        "csv_lines.append([\"Average\"] + list(averaged_performance))\n",
        "print(averaged_performance)\n",
        "\n",
        "result_path = \"/content/drive/MyDrive/models/IDCAE/fan/1/anomaly_score_avg.csv\"\n",
        "save_csv(save_file_path=result_path, save_data=csv_lines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============== MODEL LOAD ==============\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/507 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "target_dir : /content/drive/MyDrive/test/fan_id_00\n",
            "test_file  num : 507\n",
            "\n",
            "========================================\n",
            "\n",
            "============== BEGIN TEST FOR A MACHINE ID 00 ==============\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 507/507 [22:34<00:00,  2.67s/it]\n",
            "  0%|          | 0/459 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AUC : 0.5276167076167076\n",
            "pAUC : 0.5039441355230829\n",
            "\n",
            "============ END OF TEST FOR A MACHINE ID ============\n",
            "target_dir : /content/drive/MyDrive/test/fan_id_02\n",
            "test_file  num : 459\n",
            "\n",
            "========================================\n",
            "\n",
            "============== BEGIN TEST FOR A MACHINE ID 02 ==============\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 459/459 [20:21<00:00,  2.66s/it]\n",
            "  0%|          | 0/448 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AUC : 0.9370752089136489\n",
            "pAUC : 0.8671749010409031\n",
            "\n",
            "============ END OF TEST FOR A MACHINE ID ============\n",
            "target_dir : /content/drive/MyDrive/test/fan_id_04\n",
            "test_file  num : 448\n",
            "\n",
            "========================================\n",
            "\n",
            "============== BEGIN TEST FOR A MACHINE ID 04 ==============\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 448/448 [19:39<00:00,  2.63s/it]\n",
            "  0%|          | 0/461 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AUC : 0.6292241379310345\n",
            "pAUC : 0.5916515426497277\n",
            "\n",
            "============ END OF TEST FOR A MACHINE ID ============\n",
            "target_dir : /content/drive/MyDrive/test/fan_id_06\n",
            "test_file  num : 461\n",
            "\n",
            "========================================\n",
            "\n",
            "============== BEGIN TEST FOR A MACHINE ID 06 ==============\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 60/461 [02:37<17:16,  2.58s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-c90df76a5cbd>\u001b[0m in \u001b[0;36mfile_load\u001b[0;34m(wav_name, mono)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmono\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmono\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoundFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msf_desc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[0msr_native\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf_desc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplerate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/soundfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[1;32m    628\u001b[0m                                          format, subtype, endian)\n\u001b[0;32m--> 629\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosefd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missuperset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'r+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseekable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/soundfile.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1174\u001b[0m                     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetfilesystemencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m             \u001b[0mfile_ptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-c90df76a5cbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# Estrazione spettrogramma audio test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_to_vector_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_mels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_MELS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_fft\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_FFT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhop_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHOP_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPOWER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;31m# Normalizzazione spettrogramma di test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-c90df76a5cbd>\u001b[0m in \u001b[0;36mfile_to_vector_array\u001b[0;34m(file_name, n_mels, n_fft, hop_length, power)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfile_to_vector_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_mels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_fft\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhop_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# 02 generate melspectrogram using librosa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mmel_spectrogram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmelspectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_fft\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_fft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhop_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhop_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_mels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_mels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-c90df76a5cbd>\u001b[0m in \u001b[0;36mfile_load\u001b[0;34m(wav_name, mono)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmono\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmono\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"file_broken or not exists!! : {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfile_list_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"wav\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'logger' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQWFuCYywfJf"
      },
      "source": [
        "!wget -O /content/drive/MyDrive/dev_data_fan.zip https://zenodo.org/record/3678171/files/dev_data_fan.zip?download=1\n",
        "!unzip /content/drive/MyDrive/dev_data_fan.zip -d /content/drive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}