{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DCASE2020_FULLY_CONNECTED_AUTOENCODER_SLIDER.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hItH65INljvG"
      },
      "source": [
        "#IMPORT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5tvHOgSnS1i",
        "outputId": "827e6bf6-7f48-4f78-a7f2-6492e68ee08d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kq5KmwSlEv8v"
      },
      "source": [
        "# import necessari\n",
        "import librosa\n",
        "import numpy\n",
        "import sys\n",
        "import os\n",
        "import glob\n",
        "import itertools\n",
        "import re\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.models\n",
        "import tensorflow.keras.backend as K\n",
        "import keras.optimizers\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, BatchNormalization, Activation, Flatten, Multiply, Add, Reshape\n",
        "from tqdm import tqdm\n",
        "from itertools import groupby\n",
        "from keras.utils import to_categorical\n",
        "from sklearn import metrics\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0PLnQAqM3m4"
      },
      "source": [
        "# costanti \n",
        "ALPHA = 0.75\n",
        "N_MELS = 128\n",
        "HOP_LENGTH = 512\n",
        "N_FFT = 1024\n",
        "POWER = 2.0\n",
        "FRAME_NUMS = 313\n",
        "VAL = 0.05"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PM9bEsD3Eo2x"
      },
      "source": [
        "# DATA LOADING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGMOe52TFiAH"
      },
      "source": [
        "# Loading da Google Drive\n",
        "train_data = numpy.load(\"/content/drive/MyDrive/DCASE_DATA_EXTRACTED/train/training_slider.npy\")\n",
        "grouped_list_by_machine_id = pickle.load( open( \"/content/drive/MyDrive/DCASE_DATA_EXTRACTED/train/training_slider_grouped_list.npy\", \"rb\" ) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1LgXuBycWdr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa04cb41-c46f-4ffe-dccd-e902a8b3876b"
      },
      "source": [
        "# GENERAZIONE DELLE LABELS\n",
        "# One-hot encoding\n",
        "label = []\n",
        "choices = []\n",
        "for i in range(0, len(grouped_list_by_machine_id)):\n",
        "  for j in range(0, len(grouped_list_by_machine_id[i])):\n",
        "    machine_id = grouped_list_by_machine_id[i][j].split('/')[7].split('_')[2]\n",
        "    #print(grouped_list_by_machine_id[i][j].split('/')[7])\n",
        "    random_choice = numpy.random.choice([\"match\", \"non_match\"], p = [ALPHA, 1-ALPHA]) \n",
        "\n",
        "    if machine_id == '00':\n",
        "      if random_choice == \"match\":\n",
        "        to_append = [1,0,0,0]\n",
        "      else: \n",
        "        not_match_label = numpy.random.choice([1, 2, 3]) \n",
        "        if not_match_label == 1:\n",
        "          to_append = [0,1,0,0]\n",
        "        elif not_match_label == 2:\n",
        "          to_append = [0,0,1,0]\n",
        "        else: \n",
        "          to_append = [0,0,0,1]\n",
        "\n",
        "    elif machine_id == '02': \n",
        "      if random_choice == \"match\":\n",
        "        to_append = [0,1,0,0]\n",
        "      else: \n",
        "        not_match_label = numpy.random.choice( [ 1, 2, 3] ) \n",
        "        if not_match_label == 1:\n",
        "          to_append = [1,0,0,0]\n",
        "        elif not_match_label == 2:\n",
        "          to_append = [0,0,1,0]\n",
        "        else: \n",
        "          to_append = [0,0,0,1]\n",
        "\n",
        "    elif machine_id == \"04\":\n",
        "      if random_choice == \"match\":\n",
        "        to_append = [0,0,1,0]\n",
        "      else: \n",
        "        not_match_label = numpy.random.choice( [ 1, 2, 3] ) \n",
        "        if not_match_label == 1:\n",
        "          to_append = [1,0,0,0]\n",
        "        elif not_match_label == 2:\n",
        "          to_append = [0,1,0,0]\n",
        "        else: \n",
        "          to_append = [0,0,0,1]\n",
        "\n",
        "    elif machine_id == \"06\":\n",
        "      if random_choice == \"match\":\n",
        "        to_append = [0,0,0,1]\n",
        "      else: \n",
        "        not_match_label = numpy.random.choice( [ 1, 2, 3] ) \n",
        "        if not_match_label == 1:\n",
        "          to_append = [1,0,0,0]\n",
        "        elif not_match_label == 2:\n",
        "          to_append = [0,1,0,0]\n",
        "        else: \n",
        "          to_append = [0,0,1,0]\n",
        "    \n",
        "    label.append(to_append) # Append della label associata a ciascuno spettrogramma\n",
        "    choices.append(random_choice) # Append della choice utilizzata per associare la label\n",
        "                                  # La choice sarà utile in fase di addestramento per capire che tipo di loss calcolare\n",
        "\n",
        "# Trasformazione in numpy.array     \n",
        "label = numpy.asarray(label)\n",
        "choices = numpy.asarray(choices)\n",
        "print(label.shape)\n",
        "print(choices.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2804, 4)\n",
            "(2804,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8ZoHhepkGk8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "417c7c84-4cc9-46a6-ac3b-1e1b9cccebe4"
      },
      "source": [
        "print(len(grouped_list_by_machine_id[0]))\n",
        "print(len(grouped_list_by_machine_id[1]))\n",
        "print(len(grouped_list_by_machine_id[2]))\n",
        "print(len(grouped_list_by_machine_id[3]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "968\n",
            "968\n",
            "434\n",
            "434\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjCvfyq2jA_b",
        "outputId": "bb392762-3535-48e0-dc2b-19cb11f5bfaa"
      },
      "source": [
        "# Estrazione spettrogrammi divisi per ID\n",
        "id_00 = train_data[0:968]\n",
        "label_00 = label[0:968]\n",
        "choices_00 = choices[0:968]\n",
        "\n",
        "id_02 = train_data[968:1936]\n",
        "label_02 = label[968:1936]\n",
        "choices_02 = choices[968:1936]\n",
        "\n",
        "id_04 = train_data[1936:2370]\n",
        "label_04 = label[1936:2370]\n",
        "choices_04 = choices[1936:2370]\n",
        "\n",
        "id_06 = train_data[2370:2804]\n",
        "label_06 = label[2370:2804]\n",
        "choices_06 = choices[2370:2804]\n",
        "\n",
        "id_00_training, \\\n",
        "id_00_validation, \\\n",
        "label_00_train, \\\n",
        "label_00_validation, \\\n",
        "choices_00_train, \\\n",
        "choices_00_validation = train_test_split(id_00, label_00, choices_00, test_size=VAL, random_state=42)\n",
        "\n",
        "id_02_training, \\\n",
        "id_02_validation, \\\n",
        "label_02_train, \\\n",
        "label_02_validation, \\\n",
        "choices_02_train, \\\n",
        "choices_02_validation = train_test_split(id_02, label_02, choices_02, test_size=VAL, random_state=42)\n",
        "\n",
        "id_04_training, \\\n",
        "id_04_validation, \\\n",
        "label_04_train, \\\n",
        "label_04_validation, \\\n",
        "choices_04_train, \\\n",
        "choices_04_validation = train_test_split(id_04, label_04, choices_04, test_size=VAL, random_state=42)\n",
        "\n",
        "id_06_training, \\\n",
        "id_06_validation, \\\n",
        "label_06_train, \\\n",
        "label_06_validation, \\\n",
        "choices_06_train, \\\n",
        "choices_06_validation = train_test_split(id_06, label_06, choices_06, test_size=VAL, random_state=42)\n",
        "\n",
        "# Min-Max Normalization ID_00\n",
        "id_00_norm = numpy.empty_like(id_00_training)\n",
        "mean_00 = numpy.mean(id_00_training)\n",
        "std_00 = numpy.std(id_00_training)\n",
        "id_00_norm = (id_00_training - mean_00) / (std_00)\n",
        "id_00_norm_validation = (id_00_validation - mean_00) / (std_00)\n",
        "\n",
        "# Min-Max Normalization ID_02\n",
        "id_02_norm = numpy.empty_like(id_02_training)\n",
        "mean_02 = numpy.mean(id_02_training)\n",
        "std_02 = numpy.std(id_02_training)\n",
        "id_02_norm = (id_02_training - mean_02) / (std_02)\n",
        "id_02_norm_validation = (id_02_validation - mean_02) / (std_02)\n",
        "\n",
        "# Min-Max Normalization ID_04\n",
        "id_04_norm = numpy.empty_like(id_04_training)\n",
        "mean_04 = numpy.mean(id_04_training)\n",
        "std_04 = numpy.std(id_04_training)\n",
        "id_04_norm = (id_04_training - mean_04) / (std_04)\n",
        "id_04_norm_validation = (id_04_validation - mean_04) / (std_04)\n",
        "\n",
        "# Min-Max Normalization ID_06\n",
        "id_06_norm = numpy.empty_like(id_06_training)\n",
        "mean_06 = numpy.mean(id_06_training)\n",
        "std_06 = numpy.std(id_06_training)\n",
        "id_06_norm = (id_06_training - mean_06) / (std_06)\n",
        "id_06_norm_validation = (id_06_validation - mean_06) / (std_06)\n",
        "\n",
        "print(\"==== DATA ====\")\n",
        "total_training = numpy.concatenate([id_00_norm, id_02_norm, id_04_norm, id_06_norm])\n",
        "print(total_training.shape)\n",
        "total_validation = numpy.concatenate([id_00_norm_validation, id_02_norm_validation, id_04_norm_validation, id_06_norm_validation])\n",
        "print(total_validation.shape)\n",
        "\n",
        "print(\"==== LABELS ====\")\n",
        "total_training_label = numpy.concatenate([label_00_train, label_02_train, label_04_train, label_06_train])\n",
        "print(total_training_label.shape)\n",
        "total_validation_label = numpy.concatenate([label_00_validation, label_02_validation, label_04_validation, label_06_validation])\n",
        "print(total_validation_label.shape)\n",
        "\n",
        "print(\"==== CHOICES ====\")\n",
        "total_training_choices = numpy.concatenate([choices_00_train, choices_02_train, choices_04_train, choices_06_train])\n",
        "print(total_training_choices.shape)\n",
        "total_validation_choices = numpy.concatenate([choices_00_validation, choices_02_validation, choices_04_validation, choices_06_validation])\n",
        "print(total_validation_choices.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==== DATA ====\n",
            "(2522, 128, 313)\n",
            "(282, 128, 313)\n",
            "==== LABELS ====\n",
            "(2522, 4)\n",
            "(282, 4)\n",
            "==== CHOICES ====\n",
            "(2522,)\n",
            "(282,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkFwLBYH89tx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44fcfdc6-c615-442f-b6e8-671f37f616e3"
      },
      "source": [
        "# DATA AUGMENTATION\n",
        "\n",
        "####### DATA ######\n",
        "# Estrazione frame 128x10 da ciascun spettrogramma\n",
        "training_data = numpy.zeros((len(total_training)*101, 128, 10)) # Dataset utilizzato per il training\n",
        "index = 0\n",
        "for vector_array in total_training:\n",
        "  i = 0\n",
        "  while i < 303:\n",
        "    vector_i = numpy.zeros((128,10))\n",
        "    for j in range(0,128):\n",
        "      vector_i[j] = vector_array[j][i:i+10]\n",
        "    training_data[index] = vector_i\n",
        "    index += 1\n",
        "    i = i+3\n",
        "\n",
        "validation_data = numpy.zeros((len(total_validation)*101, 128, 10)) # Dataset utilizzato per il training\n",
        "index = 0\n",
        "for vector_array in total_validation:\n",
        "  i = 0\n",
        "  while i < 303:\n",
        "    vector_i = numpy.zeros((128,10))\n",
        "    for j in range(0,128):\n",
        "      vector_i[j] = vector_array[j][i:i+10]\n",
        "    validation_data[index] = vector_i\n",
        "    index += 1\n",
        "    i = i+3\n",
        "###################\n",
        "\n",
        "####### LABELS ######\n",
        "# Associazione della label associata a ciascun spettrogramma a ciascuno dei frame estratto da esso.\n",
        "training_labels = []\n",
        "for elem in total_training_label:\n",
        "  if numpy.array_equal(elem, numpy.asarray([1,0,0,0])) :\n",
        "    for i in range(101):\n",
        "      training_labels.append([1,0,0,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,1,0,0])):\n",
        "    for i in range(101):\n",
        "      training_labels.append([0,1,0,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,0,1,0])):\n",
        "    for i in range(101):\n",
        "      training_labels.append([0,0,1,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,0,0,1])):\n",
        "    for i in range(101):\n",
        "      training_labels.append([0,0,0,1])\n",
        "\n",
        "validation_labels = []\n",
        "for elem in total_validation_label:\n",
        "  if numpy.array_equal(elem, numpy.asarray([1,0,0,0])) :\n",
        "    for i in range(101):\n",
        "      validation_labels.append([1,0,0,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,1,0,0])):\n",
        "    for i in range(101):\n",
        "      validation_labels.append([0,1,0,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,0,1,0])):\n",
        "    for i in range(101):\n",
        "      validation_labels.append([0,0,1,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,0,0,1])):\n",
        "    for i in range(101):\n",
        "      validation_labels.append([0,0,0,1])\n",
        "\n",
        "training_labels = numpy.asarray(training_labels) # Dataset utilizzato per il training\n",
        "validation_labels = numpy.asarray(validation_labels) # Dataset utilizzato per il training\n",
        "#####################\n",
        "\n",
        "\n",
        "####### CHOICES ######\n",
        "# Associazione della choice associata a ciascun spettrogramma a ciascuno dei frame estratto da esso. \n",
        "training_choices = []\n",
        "for elem in total_training_choices:\n",
        "  if numpy.array_equal(elem, numpy.asarray(\"match\")) :\n",
        "    for i in range(101):\n",
        "      training_choices.append(\"match\")\n",
        "  elif numpy.array_equal(elem, numpy.asarray(\"non_match\")):\n",
        "    for i in range(101):\n",
        "      training_choices.append(\"non_match\")\n",
        "\n",
        "validation_choices = []\n",
        "for elem in total_validation_choices:\n",
        "  if numpy.array_equal(elem, numpy.asarray(\"match\")) :\n",
        "    for i in range(101):\n",
        "      validation_choices.append(\"match\")\n",
        "  elif numpy.array_equal(elem, numpy.asarray(\"non_match\")):\n",
        "    for i in range(101):\n",
        "      validation_choices.append(\"non_match\")\n",
        "\n",
        "training_choices = numpy.asarray(training_choices) # Dataset utilizzato per il training\n",
        "validation_choices = numpy.asarray(validation_choices) # Dataset utilizzato per il training\n",
        "######################\n",
        "\n",
        "print(\"==== DATA ====\")\n",
        "print(training_data.shape)\n",
        "print(validation_data.shape)\n",
        "\n",
        "print(\"==== LABELS ====\")\n",
        "print(training_labels.shape)\n",
        "print(validation_labels.shape)\n",
        "\n",
        "print(\"==== CHOICES ====\")\n",
        "print(training_choices.shape)\n",
        "print(validation_choices.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==== DATA ====\n",
            "(254722, 128, 10)\n",
            "(28482, 128, 10)\n",
            "==== LABELS ====\n",
            "(254722, 4)\n",
            "(28482, 4)\n",
            "==== CHOICES ====\n",
            "(254722,)\n",
            "(28482,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdtRhIAYXsLn"
      },
      "source": [
        "# KERAS MODEL DEFINITION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96e3XmeeX2GS"
      },
      "source": [
        "# LAYER DEFINITION\n",
        "def DenseBlock(input,n):\n",
        "  x = Dense(n)(input)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  return x\n",
        "\n",
        "input_Spect = Input(shape = [128, 10])\n",
        "input_Label = Input(shape = [4,])\n",
        "\n",
        "# First Branch - Encoder\n",
        "m = Flatten(input_shape = [128, 10])(input_Spect)\n",
        "m = DenseBlock(m, 128)\n",
        "m = DenseBlock(m, 128)\n",
        "m = DenseBlock(m, 128)\n",
        "m = DenseBlock(m, 64)\n",
        "m = DenseBlock(m, 32)\n",
        "m = DenseBlock(m, 16)\n",
        "\n",
        "# Second Branch - Conditioning Feed Forward Neural Network\n",
        "x = Dense(16)(input_Label)\n",
        "x = Activation('sigmoid')(x)\n",
        "q = Dense(16)(input_Label)\n",
        "\n",
        "# Encoded Input Conditioning\n",
        "m = Multiply()([x,m])\n",
        "encoded_input_conditioned = Add()([q, m]) # Input da passare al decoder\n",
        "\n",
        "# Decoder\n",
        "m = DenseBlock(encoded_input_conditioned, 128)\n",
        "m = DenseBlock(m, 128)\n",
        "m = DenseBlock(m, 128)\n",
        "m = DenseBlock(m, 128)\n",
        "m = DenseBlock(m, 128)\n",
        "m = Dense(128*10)(m)\n",
        "m = Reshape((128,10),  input_shape=(128*10,))(m) # Output del modello\n",
        "\n",
        "loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "mse_metric = keras.metrics.MeanSquaredError(name=\"mse\")\n",
        "\n",
        "class CustomModel(tensorflow.keras.Model):\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [loss_tracker, mse_metric]\n",
        "\n",
        "    def test_step(self, data):\n",
        "        # Unpack the data\n",
        "        x, y = data\n",
        "        # Compute predictions\n",
        "        y_pred = self([x[0],x[1]], training=False)\n",
        "        # Indici match\n",
        "        match = tf.where ( tf.equal(x[2][:], \"match\") )\n",
        "        # Dati match\n",
        "        data_match = K.gather(y, match)\n",
        "        # Separazione dei dati PREDETTI sulla base degli indici relativi a match/non_match\n",
        "        # Dati match\n",
        "        pred_match = K.gather(y_pred, match)\n",
        "\n",
        "        # Update metrica\n",
        "        mse_metric.update_state(data_match, pred_match)\n",
        "\n",
        "        return {\"mse\": mse_metric.result()}\n",
        "    \n",
        "    def train_step(self, data):\n",
        "          # Unpack the data. Its structure depends on your model and on what you pass to `fit()`.\n",
        "          x, y = data\n",
        "\n",
        "          # Vettore C utilizzato per il calcolo della loss in caso di non_match\n",
        "          C = 5 \n",
        "          # Valore di probabilità utilizzato come peso\n",
        "          ALPHA = 0.75 \n",
        "\n",
        "          # Indici match\n",
        "          match = tf.where ( tf.equal(x[2][:], \"match\") )\n",
        "\n",
        "          # Indici non_match\n",
        "          not_match = tf.where ( tf.equal(x[2][:], \"non_match\") )\n",
        "\n",
        "          # Dati match\n",
        "          data_match = K.gather(y, match)\n",
        "\n",
        "          with tf.GradientTape() as tape:\n",
        "              y_pred = self([x[0],x[1]], training=True)  # Forward pass\n",
        "\n",
        "              # Separazione dei dati PREDETTI sulla base degli indici relativi a match/non_match\n",
        "              # Dati match\n",
        "              pred_match = K.gather(y_pred, match)\n",
        "              # Dati non match\n",
        "              pred_not_match = K.gather(y_pred, not_match) \n",
        "\n",
        "              loss_m = K.mean(keras.losses.mean_squared_error(data_match, pred_match)) + 1e-6  # Calcolo Loss Match\n",
        "              loss_nm = K.mean(keras.losses.mean_squared_error(C,pred_not_match)) + 1e-6     # Calcolo Loss Non_Match\n",
        "\n",
        "              loss = ALPHA * loss_m + (1 - ALPHA) * loss_nm     # loss utilizzata per l'update dei pesi\n",
        "\n",
        "          # Compute gradients\n",
        "          trainable_vars = self.trainable_variables\n",
        "          gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "          # Update weights\n",
        "          self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "          # Compute our own metrics\n",
        "          loss_tracker.update_state(loss)\n",
        "          mse_metric.update_state(y, y_pred)\n",
        "          return {\"loss\": loss_tracker.result(), \"mse\": mse_metric.result()}\n",
        "\n",
        "def get_lr_metric(optimizer):\n",
        "    def lr(y_true, y_pred):\n",
        "        return optimizer._decayed_lr(tf.float32) # I use ._decayed_lr method instead of .lr\n",
        "    return lr\n",
        "\n",
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.0001,\n",
        "                                                  decay_steps = 2490,\n",
        "                                                  decay_rate = 0.95)\n",
        "\n",
        "opt = keras.optimizers.Adam(learning_rate = lr_schedule)\n",
        "\n",
        "lr_metric = get_lr_metric(opt)\n",
        "\n",
        "model = CustomModel(inputs=(input_Spect, input_Label), outputs = m)\n",
        "\n",
        "model.compile(optimizer = opt, metrics=[\"mse\", lr_metric])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HjXJIU4bvUH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ede506e-5351-4471-f2e4-bef3041b558c"
      },
      "source": [
        "history = model.fit([training_data, training_labels, training_choices], \n",
        "          training_data, \n",
        "          epochs=100, \n",
        "          batch_size=512, \n",
        "          validation_data=([validation_data, validation_labels, validation_choices], validation_data), shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "498/498 [==============================] - 9s 13ms/step - loss: 4.1095 - mse: 3.0592 - val_mse: 1.3509\n",
            "Epoch 2/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 2.2479 - mse: 4.8799 - val_mse: 1.2370\n",
            "Epoch 3/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 1.6379 - mse: 5.4929 - val_mse: 1.1402\n",
            "Epoch 4/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 1.3304 - mse: 5.8065 - val_mse: 1.0730\n",
            "Epoch 5/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 1.1336 - mse: 5.9939 - val_mse: 1.0442\n",
            "Epoch 6/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.9963 - mse: 6.1307 - val_mse: 1.2943\n",
            "Epoch 7/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.8870 - mse: 6.2314 - val_mse: 1.1071\n",
            "Epoch 8/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.7999 - mse: 6.3193 - val_mse: 1.1042\n",
            "Epoch 9/100\n",
            "498/498 [==============================] - 6s 11ms/step - loss: 0.7342 - mse: 6.3830 - val_mse: 1.0910\n",
            "Epoch 10/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.6735 - mse: 6.4442 - val_mse: 1.1768\n",
            "Epoch 11/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.6202 - mse: 6.4925 - val_mse: 1.0671\n",
            "Epoch 12/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.5761 - mse: 6.5481 - val_mse: 1.0764\n",
            "Epoch 13/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.5406 - mse: 6.5781 - val_mse: 1.1321\n",
            "Epoch 14/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.5101 - mse: 6.5977 - val_mse: 1.2516\n",
            "Epoch 15/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.4833 - mse: 6.6322 - val_mse: 1.0663\n",
            "Epoch 16/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.4557 - mse: 6.6561 - val_mse: 1.1212\n",
            "Epoch 17/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.4338 - mse: 6.6787 - val_mse: 1.0869\n",
            "Epoch 18/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.4213 - mse: 6.7045 - val_mse: 1.2166\n",
            "Epoch 19/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.4007 - mse: 6.7179 - val_mse: 1.1598\n",
            "Epoch 20/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.3945 - mse: 6.7241 - val_mse: 1.1428\n",
            "Epoch 21/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.3748 - mse: 6.7435 - val_mse: 1.1284\n",
            "Epoch 22/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.3618 - mse: 6.7564 - val_mse: 1.1569\n",
            "Epoch 23/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.3512 - mse: 6.7669 - val_mse: 1.3356\n",
            "Epoch 24/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.3478 - mse: 6.7716 - val_mse: 1.1898\n",
            "Epoch 25/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.3416 - mse: 6.7751 - val_mse: 1.1636\n",
            "Epoch 26/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.3313 - mse: 6.7819 - val_mse: 1.1963\n",
            "Epoch 27/100\n",
            "498/498 [==============================] - 6s 11ms/step - loss: 0.3207 - mse: 6.8024 - val_mse: 1.1496\n",
            "Epoch 28/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.3183 - mse: 6.7961 - val_mse: 1.2376\n",
            "Epoch 29/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.3135 - mse: 6.8084 - val_mse: 1.1500\n",
            "Epoch 30/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.3064 - mse: 6.8076 - val_mse: 1.1953\n",
            "Epoch 31/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.3021 - mse: 6.8167 - val_mse: 1.1409\n",
            "Epoch 32/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2994 - mse: 6.8215 - val_mse: 1.0698\n",
            "Epoch 33/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2963 - mse: 6.8207 - val_mse: 1.1212\n",
            "Epoch 34/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2878 - mse: 6.8299 - val_mse: 1.0809\n",
            "Epoch 35/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2860 - mse: 6.8338 - val_mse: 1.2317\n",
            "Epoch 36/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2865 - mse: 6.8295 - val_mse: 1.1835\n",
            "Epoch 37/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2810 - mse: 6.8314 - val_mse: 1.1181\n",
            "Epoch 38/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2776 - mse: 6.8408 - val_mse: 1.1151\n",
            "Epoch 39/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2757 - mse: 6.8407 - val_mse: 1.1822\n",
            "Epoch 40/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2734 - mse: 6.8390 - val_mse: 1.1526\n",
            "Epoch 41/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2712 - mse: 6.8450 - val_mse: 1.1700\n",
            "Epoch 42/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2682 - mse: 6.8452 - val_mse: 1.2340\n",
            "Epoch 43/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2644 - mse: 6.8515 - val_mse: 1.1632\n",
            "Epoch 44/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2641 - mse: 6.8508 - val_mse: 1.1793\n",
            "Epoch 45/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2631 - mse: 6.8468 - val_mse: 1.2098\n",
            "Epoch 46/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2609 - mse: 6.8542 - val_mse: 1.1492\n",
            "Epoch 47/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2572 - mse: 6.8576 - val_mse: 1.1040\n",
            "Epoch 48/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2582 - mse: 6.8552 - val_mse: 1.1523\n",
            "Epoch 49/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2565 - mse: 6.8572 - val_mse: 1.0958\n",
            "Epoch 50/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2520 - mse: 6.8627 - val_mse: 1.1429\n",
            "Epoch 51/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2503 - mse: 6.8588 - val_mse: 1.2125\n",
            "Epoch 52/100\n",
            "498/498 [==============================] - 6s 11ms/step - loss: 0.2499 - mse: 6.8659 - val_mse: 1.1621\n",
            "Epoch 53/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2490 - mse: 6.8667 - val_mse: 1.1444\n",
            "Epoch 54/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2437 - mse: 6.8700 - val_mse: 1.1557\n",
            "Epoch 55/100\n",
            "498/498 [==============================] - 6s 11ms/step - loss: 0.2482 - mse: 6.8628 - val_mse: 1.1358\n",
            "Epoch 56/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2456 - mse: 6.8661 - val_mse: 1.1837\n",
            "Epoch 57/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2415 - mse: 6.8679 - val_mse: 1.0848\n",
            "Epoch 58/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2427 - mse: 6.8684 - val_mse: 1.1526\n",
            "Epoch 59/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2386 - mse: 6.8730 - val_mse: 1.0612\n",
            "Epoch 60/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2369 - mse: 6.8754 - val_mse: 1.1873\n",
            "Epoch 61/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2374 - mse: 6.8710 - val_mse: 1.0767\n",
            "Epoch 62/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2374 - mse: 6.8733 - val_mse: 1.1496\n",
            "Epoch 63/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2351 - mse: 6.8749 - val_mse: 1.0454\n",
            "Epoch 64/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2339 - mse: 6.8732 - val_mse: 1.1414\n",
            "Epoch 65/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2327 - mse: 6.8779 - val_mse: 1.0833\n",
            "Epoch 66/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2322 - mse: 6.8789 - val_mse: 1.0899\n",
            "Epoch 67/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2321 - mse: 6.8775 - val_mse: 1.0790\n",
            "Epoch 68/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2341 - mse: 6.8711 - val_mse: 1.1239\n",
            "Epoch 69/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2323 - mse: 6.8776 - val_mse: 1.1895\n",
            "Epoch 70/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2272 - mse: 6.8827 - val_mse: 1.0957\n",
            "Epoch 71/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2268 - mse: 6.8827 - val_mse: 1.1099\n",
            "Epoch 72/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2264 - mse: 6.8811 - val_mse: 1.1028\n",
            "Epoch 73/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2245 - mse: 6.8827 - val_mse: 1.1010\n",
            "Epoch 74/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2266 - mse: 6.8828 - val_mse: 1.0686\n",
            "Epoch 75/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2236 - mse: 6.8809 - val_mse: 1.0722\n",
            "Epoch 76/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2211 - mse: 6.8854 - val_mse: 1.1024\n",
            "Epoch 77/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2225 - mse: 6.8841 - val_mse: 1.0975\n",
            "Epoch 78/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2227 - mse: 6.8837 - val_mse: 1.0723\n",
            "Epoch 79/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2213 - mse: 6.8840 - val_mse: 1.1145\n",
            "Epoch 80/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2190 - mse: 6.8873 - val_mse: 1.0864\n",
            "Epoch 81/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2203 - mse: 6.8856 - val_mse: 1.0743\n",
            "Epoch 82/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2178 - mse: 6.8870 - val_mse: 1.1416\n",
            "Epoch 83/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2194 - mse: 6.8862 - val_mse: 1.1074\n",
            "Epoch 84/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2173 - mse: 6.8890 - val_mse: 1.0843\n",
            "Epoch 85/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2168 - mse: 6.8888 - val_mse: 1.1089\n",
            "Epoch 86/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2154 - mse: 6.8908 - val_mse: 1.0391\n",
            "Epoch 87/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2156 - mse: 6.8889 - val_mse: 1.1090\n",
            "Epoch 88/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2137 - mse: 6.8897 - val_mse: 1.0529\n",
            "Epoch 89/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2168 - mse: 6.8887 - val_mse: 1.1195\n",
            "Epoch 90/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2135 - mse: 6.8891 - val_mse: 1.0151\n",
            "Epoch 91/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2137 - mse: 6.8903 - val_mse: 1.0890\n",
            "Epoch 92/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2145 - mse: 6.8880 - val_mse: 1.0383\n",
            "Epoch 93/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2133 - mse: 6.8898 - val_mse: 1.0692\n",
            "Epoch 94/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2115 - mse: 6.8912 - val_mse: 1.1255\n",
            "Epoch 95/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2108 - mse: 6.8942 - val_mse: 1.1006\n",
            "Epoch 96/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2106 - mse: 6.8899 - val_mse: 1.1387\n",
            "Epoch 97/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2106 - mse: 6.8941 - val_mse: 1.0260\n",
            "Epoch 98/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2125 - mse: 6.8904 - val_mse: 1.1095\n",
            "Epoch 99/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2101 - mse: 6.8923 - val_mse: 1.0640\n",
            "Epoch 100/100\n",
            "498/498 [==============================] - 6s 12ms/step - loss: 0.2100 - mse: 6.8908 - val_mse: 1.1407\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktyeNLjDcxJZ"
      },
      "source": [
        "# Salvataggio del modello\n",
        "model.save('/content/drive/MyDrive/models/IDCAE/slider/1/model_slider.h5')\n",
        "\n",
        "# Salvataggio history di apprendimento\n",
        "with open('/content/drive/MyDrive/models/IDCAE/slider/1/trainHistoryDict', 'wb') as file_pi:\n",
        "    pickle.dump(history.history, file_pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sN8yMBbSLYb1"
      },
      "source": [
        "# TESTING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alzE-821wFIi",
        "outputId": "45c9dd9d-a00e-4f1f-cdcc-b9cf1b85f884"
      },
      "source": [
        "import csv\n",
        "\n",
        "def save_csv(save_file_path,\n",
        "             save_data):\n",
        "    with open(save_file_path, \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f, lineterminator='\\n')\n",
        "        writer.writerows(save_data)\n",
        "\n",
        "# load dataset\n",
        "def select_dirs(path):\n",
        "    dir_path = os.path.abspath(path)\n",
        "    dirs = sorted(glob.glob(dir_path))\n",
        "    return dirs\n",
        "\n",
        "def file_load(wav_name, mono=False):\n",
        "    try:\n",
        "        return librosa.load(wav_name, sr=None, mono=mono)\n",
        "    except:\n",
        "        logger.error(\"file_broken or not exists!! : {}\".format(wav_name))\n",
        "\n",
        "def file_list_generator(target_dir, dir_name=\"train\", ext=\"wav\"):\n",
        "    print(\"target_dir : {}\".format(target_dir))\n",
        "\n",
        "    # generate training list\n",
        "    training_list_path = os.path.abspath(\"{dir}/{dir_name}/*.{ext}\".format(dir=target_dir, dir_name=dir_name, ext=ext))\n",
        "    files = sorted(glob.glob(training_list_path))\n",
        "    if len(files) == 0:\n",
        "      print(\"errore\")\n",
        "    return files\n",
        "\n",
        "\n",
        "def file_to_vector_array(file_name, n_mels=64, n_fft=1024, hop_length=512, power=2.0):\n",
        "    # 02 generate melspectrogram using librosa\n",
        "    y, sr = file_load(file_name)\n",
        "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, power=power)\n",
        "\n",
        "    # 03 convert melspectrogram to log mel energy\n",
        "    log_mel_spectrogram = 20.0 / power * numpy.log10(mel_spectrogram + sys.float_info.epsilon)\n",
        "\n",
        "    return log_mel_spectrogram\n",
        "\n",
        "  \n",
        "def list_to_vector_array(file_list, msg=\"calc...\", n_mels=64, n_fft=1024, hop_length=512, power=2.0, frames=10):\n",
        "    # iterate file_to_vector_array()\n",
        "    for idx in tqdm(range(len(file_list)), desc=msg):\n",
        "        vector_array = file_to_vector_array(file_list[idx], n_mels=n_mels, n_fft=n_fft, hop_length=hop_length, power=power)\n",
        "       \n",
        "        # vector_array = numpy.delete(vector_array,[310,311,312], axis=1)\n",
        "        # vector_array = numpy.asarray(numpy.hsplit(vector_array, 31))\n",
        "\n",
        "        if idx == 0:\n",
        "            dataset = numpy.zeros((len(file_list), n_mels, frames), float)\n",
        "        dataset[vector_array.shape[0] * idx: vector_array.shape[0] * (idx + 1), :] = vector_array\n",
        "    return dataset\n",
        "\n",
        "def key_by_id(item):\n",
        "  path_splitted = item.split(\"/\")\n",
        "  file_name = path_splitted[ len(path_splitted) - 1 ]\n",
        "  file_name_splitted = file_name.split(\"_\")\n",
        "  machine_id = file_name_splitted = file_name_splitted[2]\n",
        "  return machine_id\n",
        "\n",
        "def get_machine_id_list_for_test(target_dir,\n",
        "                                 dir_name=\"test\",\n",
        "                                 ext=\"wav\"):\n",
        "\n",
        "    # create test files\n",
        "    dir_path = os.path.abspath(\"{dir}/{dir_name}/*.{ext}\".format(dir=target_dir, dir_name=dir_name, ext=ext))\n",
        "    file_paths = sorted(glob.glob(dir_path))\n",
        "    # extract id\n",
        "    machine_id_list = sorted(list(set(itertools.chain.from_iterable(\n",
        "        [re.findall('id_[0-9][0-9]', ext_id) for ext_id in file_paths]))))\n",
        "    return machine_id_list\n",
        "\n",
        "def test_file_list_generator(target_dir,\n",
        "                             id_name,\n",
        "                             dir_name=\"test\",\n",
        "                             prefix_normal=\"normal\",\n",
        "                             prefix_anomaly=\"anomaly\",\n",
        "                             ext=\"wav\"):\n",
        "  \n",
        "    print(\"target_dir : {}\".format(target_dir+\"_\"+id_name))\n",
        "\n",
        "    normal_files = sorted(\n",
        "    glob.glob(\"{dir}/{dir_name}/{prefix_normal}_{id_name}*.{ext}\".format(dir=target_dir,\n",
        "                                                                                 dir_name=dir_name,\n",
        "                                                                                 prefix_normal=prefix_normal,\n",
        "                                                                                 id_name=id_name,\n",
        "                                                                                 ext=ext)))\n",
        "    normal_labels = numpy.zeros(len(normal_files))\n",
        "    anomaly_files = sorted(\n",
        "    glob.glob(\"{dir}/{dir_name}/{prefix_anomaly}_{id_name}*.{ext}\".format(dir=target_dir,\n",
        "                                                                                  dir_name=dir_name,\n",
        "                                                                                  prefix_anomaly=prefix_anomaly,\n",
        "                                                                                  id_name=id_name,\n",
        "                                                                                  ext=ext)))\n",
        "    anomaly_labels = numpy.ones(len(anomaly_files))\n",
        "    files = numpy.concatenate((normal_files, anomaly_files), axis=0)\n",
        "    labels = numpy.concatenate((normal_labels, anomaly_labels), axis=0)\n",
        "    print(\"test_file  num : {num}\".format(num=len(files)))\n",
        "    if len(files) == 0:\n",
        "        print(\"no_wav_file!!\")\n",
        "    print(\"\\n========================================\")\n",
        "\n",
        "    return files, labels\n",
        "\n",
        "target_dir = \"/content/drive/MyDrive/test/slider\"\n",
        "\n",
        "performance = []\n",
        "machine_type = os.path.split(target_dir)[1]\n",
        "print(\"============== MODEL LOAD ==============\")\n",
        "# set model path\n",
        "model_file = \"/content/drive/MyDrive/models/IDCAE/slider/1/model_slider.h5\"\n",
        "\n",
        "# load model file\n",
        "if not os.path.exists(model_file):\n",
        "  print(\"{} model not found \".format(machine_type))\n",
        "  sys.exit(-1)\n",
        "model = tensorflow.keras.models.load_model(model_file, custom_objects={'CustomModel': CustomModel, 'mse':mse_metric, 'lr': lr_metric})\n",
        "# model.summary()\n",
        "\n",
        "machine_id_list = get_machine_id_list_for_test(target_dir)\n",
        "\n",
        "# initialize lines in csv for AUC and pAUC\n",
        "csv_lines = []\n",
        "\n",
        "csv_lines.append([machine_type])\n",
        "csv_lines.append([\"id\", \"AUC\", \"pAUC\"])\n",
        "performance = []\n",
        "\n",
        "for id_str in machine_id_list:\n",
        "  # load test file\n",
        "\n",
        "  id_num = id_str.split(\"_\")[1]\n",
        "\n",
        "  # Definizione della label \"match\" da utilizzare in fase di testing e del min e max da utilizzare per la normalizzazione\n",
        "  # i min e max sono stati calcolati a partire dai dati di training.\n",
        "  if id_num == \"00\":\n",
        "    match_labels = numpy.asarray([1,0,0,0])\n",
        "    mean = mean_00\n",
        "    std = std_00\n",
        "  elif id_num == \"02\":\n",
        "    match_labels = numpy.asarray([0,1,0,0])\n",
        "    mean = mean_02\n",
        "    std = std_02\n",
        "  elif id_num == \"04\":\n",
        "    match_labels = numpy.asarray([0,0,1,0])\n",
        "    mean = mean_04\n",
        "    std = std_04\n",
        "  elif id_num == \"06\": \n",
        "    match_labels = numpy.asarray([0,0,0,1])\n",
        "    mean = mean_06\n",
        "    std = std_06\n",
        "\n",
        "  test_files, y_true = test_file_list_generator(target_dir, id_str)\n",
        "  #print(\"\\n====== True Labels ======\")\n",
        "  #print(y_true)\n",
        "  #print(\"==> ====== Match ID Labels ======\")\n",
        "  #print(match_labels.shape)\n",
        "  #print(\"=================================\\n\")\n",
        "\n",
        "  # setup anomaly score file path\n",
        "  anomaly_score_csv = \"/content/drive/MyDrive/models/IDCAE/slider/1/anomaly_score_{machine_type}_{id_str}.csv\".format(machine_type=machine_type, id_str=id_str)\n",
        "  anomaly_score_list = []\n",
        "\n",
        "  print(\"\\n============== BEGIN TEST FOR A MACHINE ID {id} ==============\".format(id=id_num))\n",
        "\n",
        "  y_pred = [0. for k in test_files]\n",
        "\n",
        "  for file_idx, file_path in tqdm(enumerate(test_files), total=len(test_files)):\n",
        "\n",
        "    # Estrazione spettrogramma audio test\n",
        "    data = file_to_vector_array(file_path, n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH, power=POWER)\n",
        "\n",
        "    # Normalizzazione spettrogramma di test\n",
        "    data = ( data - mean ) / (std)\n",
        "\n",
        "    # Estrazione delle frame 128x10\n",
        "    data_splitted = numpy.zeros((101, 128, 10))\n",
        "    index = 0\n",
        "    i = 0\n",
        "    while i < 303:\n",
        "      vector_i = numpy.zeros((128,10))\n",
        "      for j in range(0,128):\n",
        "        vector_i[j] = data[j][i:i+10]\n",
        "      data_splitted[index] = vector_i\n",
        "      index += 1\n",
        "      i = i+3\n",
        "\n",
        "    # Calcolo dell'errore medio sulle frame estratte dallo spettrogramma\n",
        "    elem_error = []\n",
        "    for elem in data_splitted:\n",
        "      predicted = model.predict([elem.reshape(1,128,10), match_labels.reshape(1,4)])\n",
        "\n",
        "      errors = numpy.mean(numpy.square(elem - predicted), axis=1)\n",
        "      elem_error.append(numpy.mean(errors))\n",
        "    # Log dell'errore associato all'istanza di test\n",
        "    y_pred[file_idx] = numpy.mean(elem_error)\n",
        "    anomaly_score_list.append([os.path.basename(file_path), y_pred[file_idx]])\n",
        "  \n",
        "  save_csv(save_file_path=anomaly_score_csv, save_data=anomaly_score_list)\n",
        "\n",
        " # Calcolo AUC e pAUC per i dati con un certo ID_0x\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  p_auc = metrics.roc_auc_score(y_true, y_pred, max_fpr=0.1)\n",
        "  csv_lines.append([id_str.split(\"_\", 1)[1], auc, p_auc])\n",
        "  performance.append([auc, p_auc])\n",
        "  print(\"AUC : {}\".format(auc))\n",
        "  print(\"pAUC : {}\".format(p_auc))\n",
        "\n",
        "  print(\"\\n============ END OF TEST FOR A MACHINE ID ============\")\n",
        "\n",
        "# Stampa di AUC e pAUC medi su tutti i dati di test (media di AUC e pAUC sui vari ID).\n",
        "print(\"\\n============ AVERAGE PERFORMANCES ============\")\n",
        "averaged_performance = numpy.mean(numpy.array(performance, dtype=float), axis=0)\n",
        "csv_lines.append([\"Average\"] + list(averaged_performance))\n",
        "print(averaged_performance)\n",
        "\n",
        "result_path = \"/content/drive/MyDrive/models/IDCAE/slider/1/anomaly_score_avg.csv\"\n",
        "save_csv(save_file_path=result_path, save_data=csv_lines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============== MODEL LOAD ==============\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/456 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "target_dir : /content/drive/MyDrive/test/slider_id_00\n",
            "test_file  num : 456\n",
            "\n",
            "========================================\n",
            "\n",
            "============== BEGIN TEST FOR A MACHINE ID 00 ==============\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 456/456 [15:40<00:00,  2.06s/it]\n",
            "  0%|          | 0/367 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AUC : 0.8543539325842696\n",
            "pAUC : 0.7170313424009462\n",
            "\n",
            "============ END OF TEST FOR A MACHINE ID ============\n",
            "target_dir : /content/drive/MyDrive/test/slider_id_02\n",
            "test_file  num : 367\n",
            "\n",
            "========================================\n",
            "\n",
            "============== BEGIN TEST FOR A MACHINE ID 02 ==============\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 367/367 [12:35<00:00,  2.06s/it]\n",
            "  0%|          | 0/278 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AUC : 0.7238202247191011\n",
            "pAUC : 0.5385373546225113\n",
            "\n",
            "============ END OF TEST FOR A MACHINE ID ============\n",
            "target_dir : /content/drive/MyDrive/test/slider_id_04\n",
            "test_file  num : 278\n",
            "\n",
            "========================================\n",
            "\n",
            "============== BEGIN TEST FOR A MACHINE ID 04 ==============\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 278/278 [09:30<00:00,  2.05s/it]\n",
            "  0%|          | 0/189 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AUC : 0.8709550561797752\n",
            "pAUC : 0.6818450620934358\n",
            "\n",
            "============ END OF TEST FOR A MACHINE ID ============\n",
            "target_dir : /content/drive/MyDrive/test/slider_id_06\n",
            "test_file  num : 189\n",
            "\n",
            "========================================\n",
            "\n",
            "============== BEGIN TEST FOR A MACHINE ID 06 ==============\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 189/189 [06:28<00:00,  2.06s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AUC : 0.4807865168539326\n",
            "pAUC : 0.48432879952690716\n",
            "\n",
            "============ END OF TEST FOR A MACHINE ID ============\n",
            "\n",
            "============ AVERAGE PERFORMANCES ============\n",
            "[0.73247893 0.60543564]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}