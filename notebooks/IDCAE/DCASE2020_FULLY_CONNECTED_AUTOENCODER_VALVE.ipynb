{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"DCASE2020_FULLY_CONNECTED_AUTOENCODER_VALVE.ipynb","provenance":[],"collapsed_sections":["hItH65INljvG","PM9bEsD3Eo2x","QdtRhIAYXsLn","sN8yMBbSLYb1"],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"hItH65INljvG"},"source":["#IMPORT"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z5tvHOgSnS1i","outputId":"8b3ef583-442e-4b2a-a6ef-2095bf8e5532"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Kq5KmwSlEv8v"},"source":["# import necessari\n","import librosa\n","import numpy\n","import sys\n","import os\n","import glob\n","import itertools\n","import re\n","import pickle\n","import tensorflow as tf\n","import tensorflow.keras.models\n","import tensorflow.keras.backend as K\n","import keras.optimizers\n","from keras.models import Model\n","from keras.layers import Input, Dense, BatchNormalization, Activation, Flatten, Multiply, Add, Reshape\n","from tqdm import tqdm\n","from itertools import groupby\n","from keras.utils import to_categorical\n","from sklearn import metrics\n","from google.colab import drive\n","from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A0PLnQAqM3m4"},"source":["# costanti \n","ALPHA = 0.75\n","N_MELS = 128\n","HOP_LENGTH = 512\n","N_FFT = 1024\n","POWER = 2.0\n","FRAME_NUMS = 313\n","VAL = 0.10"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PM9bEsD3Eo2x"},"source":["# DATA LOADING"]},{"cell_type":"code","metadata":{"id":"WGMOe52TFiAH"},"source":["# Loading da Google Drive\n","train_data = numpy.load(\"/content/drive/MyDrive/DCASE_DATA_EXTRACTED/train/training_valve.npy\")\n","grouped_list_by_machine_id = pickle.load( open( \"/content/drive/MyDrive/DCASE_DATA_EXTRACTED/train/training_valve_grouped_list.npy\", \"rb\" ) )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z1LgXuBycWdr","colab":{"base_uri":"https://localhost:8080/"},"outputId":"27d76cfe-b139-4b08-f27a-d939c76e607b"},"source":["# GENERAZIONE DELLE LABELS\n","# One-hot encoding\n","label = []\n","choices = []\n","for i in range(0, len(grouped_list_by_machine_id)):\n","  for j in range(0, len(grouped_list_by_machine_id[i])):\n","    machine_id = grouped_list_by_machine_id[i][j].split('/')[7].split('_')[2]\n","    #print(grouped_list_by_machine_id[i][j].split('/')[7])\n","    random_choice = numpy.random.choice([\"match\", \"non_match\"], p = [ALPHA, 1-ALPHA]) \n","\n","    if machine_id == '00':\n","      if random_choice == \"match\":\n","        to_append = [1,0,0,0]\n","      else: \n","        not_match_label = numpy.random.choice([1, 2, 3]) \n","        if not_match_label == 1:\n","          to_append = [0,1,0,0]\n","        elif not_match_label == 2:\n","          to_append = [0,0,1,0]\n","        else: \n","          to_append = [0,0,0,1]\n","\n","    elif machine_id == '02': \n","      if random_choice == \"match\":\n","        to_append = [0,1,0,0]\n","      else: \n","        not_match_label = numpy.random.choice( [ 1, 2, 3] ) \n","        if not_match_label == 1:\n","          to_append = [1,0,0,0]\n","        elif not_match_label == 2:\n","          to_append = [0,0,1,0]\n","        else: \n","          to_append = [0,0,0,1]\n","\n","    elif machine_id == \"04\":\n","      if random_choice == \"match\":\n","        to_append = [0,0,1,0]\n","      else: \n","        not_match_label = numpy.random.choice( [ 1, 2, 3] ) \n","        if not_match_label == 1:\n","          to_append = [1,0,0,0]\n","        elif not_match_label == 2:\n","          to_append = [0,1,0,0]\n","        else: \n","          to_append = [0,0,0,1]\n","\n","    elif machine_id == \"06\":\n","      if random_choice == \"match\":\n","        to_append = [0,0,0,1]\n","      else: \n","        not_match_label = numpy.random.choice( [ 1, 2, 3] ) \n","        if not_match_label == 1:\n","          to_append = [1,0,0,0]\n","        elif not_match_label == 2:\n","          to_append = [0,1,0,0]\n","        else: \n","          to_append = [0,0,1,0]\n","    \n","    label.append(to_append) # Append della label associata a ciascuno spettrogramma\n","    choices.append(random_choice) # Append della choice utilizzata per associare la label\n","                                  # La choice sarà utile in fase di addestramento per capire che tipo di loss calcolare\n","\n","# Trasformazione in numpy.array     \n","label = numpy.asarray(label)\n","choices = numpy.asarray(choices)\n","print(label.shape)\n","print(choices.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(3291, 4)\n","(3291,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"F8ZoHhepkGk8","colab":{"base_uri":"https://localhost:8080/"},"outputId":"84002a57-b842-487e-af66-461213e1b22a"},"source":["print(len(grouped_list_by_machine_id[0]))\n","print(len(grouped_list_by_machine_id[1]))\n","print(len(grouped_list_by_machine_id[2]))\n","print(len(grouped_list_by_machine_id[3]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["891\n","608\n","900\n","892\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UjCvfyq2jA_b","outputId":"93a3c952-2d21-464e-9b7d-941d387adbdd"},"source":["# Estrazione spettrogrammi divisi per ID\n","id_00 = train_data[0:891]\n","label_00 = label[0:891]\n","choices_00 = choices[0:891]\n","\n","id_02 = train_data[891:1499]\n","label_02 = label[891:1499]\n","choices_02 = choices[891:1499]\n","\n","id_04 = train_data[1499:2399]\n","label_04 = label[1499:2399]\n","choices_04 = choices[1499:2399]\n","\n","id_06 = train_data[2399:3291]\n","label_06 = label[2399:3291]\n","choices_06 = choices[2399:3291]\n","\n","id_00_training, \\\n","id_00_validation, \\\n","label_00_train, \\\n","label_00_validation, \\\n","choices_00_train, \\\n","choices_00_validation = train_test_split(id_00, label_00, choices_00, test_size=VAL, random_state=42)\n","\n","id_02_training, \\\n","id_02_validation, \\\n","label_02_train, \\\n","label_02_validation, \\\n","choices_02_train, \\\n","choices_02_validation = train_test_split(id_02, label_02, choices_02, test_size=VAL, random_state=42)\n","\n","id_04_training, \\\n","id_04_validation, \\\n","label_04_train, \\\n","label_04_validation, \\\n","choices_04_train, \\\n","choices_04_validation = train_test_split(id_04, label_04, choices_04, test_size=VAL, random_state=42)\n","\n","id_06_training, \\\n","id_06_validation, \\\n","label_06_train, \\\n","label_06_validation, \\\n","choices_06_train, \\\n","choices_06_validation = train_test_split(id_06, label_06, choices_06, test_size=VAL, random_state=42)\n","\n","# Min-Max Normalization ID_00\n","id_00_norm = numpy.empty_like(id_00_training)\n","mean_00 = numpy.mean(id_00_training)\n","std_00 = numpy.std(id_00_training)\n","id_00_norm = (id_00_training - mean_00) / (std_00)\n","id_00_norm_validation = (id_00_validation - mean_00) / (std_00)\n","\n","# Min-Max Normalization ID_02\n","id_02_norm = numpy.empty_like(id_02_training)\n","mean_02 = numpy.mean(id_02_training)\n","std_02 = numpy.std(id_02_training)\n","id_02_norm = (id_02_training - mean_02) / (std_02)\n","id_02_norm_validation = (id_02_validation - mean_02) / (std_02)\n","\n","# Min-Max Normalization ID_04\n","id_04_norm = numpy.empty_like(id_04_training)\n","mean_04 = numpy.mean(id_04_training)\n","std_04 = numpy.std(id_04_training)\n","id_04_norm = (id_04_training - mean_04) / (std_04)\n","id_04_norm_validation = (id_04_validation - mean_04) / (std_04)\n","\n","# Min-Max Normalization ID_06\n","id_06_norm = numpy.empty_like(id_06_training)\n","mean_06 = numpy.mean(id_06_training)\n","std_06 = numpy.std(id_06_training)\n","id_06_norm = (id_06_training - mean_06) / (std_06)\n","id_06_norm_validation = (id_06_validation - mean_06) / (std_06)\n","\n","print(\"==== DATA ====\")\n","total_training = numpy.concatenate([id_00_norm, id_02_norm, id_04_norm, id_06_norm])\n","print(total_training.shape)\n","total_validation = numpy.concatenate([id_00_norm_validation, id_02_norm_validation, id_04_norm_validation, id_06_norm_validation])\n","print(total_validation.shape)\n","\n","print(\"==== LABELS ====\")\n","total_training_label = numpy.concatenate([label_00_train, label_02_train, label_04_train, label_06_train])\n","print(total_training_label.shape)\n","total_validation_label = numpy.concatenate([label_00_validation, label_02_validation, label_04_validation, label_06_validation])\n","print(total_validation_label.shape)\n","\n","print(\"==== CHOICES ====\")\n","total_training_choices = numpy.concatenate([choices_00_train, choices_02_train, choices_04_train, choices_06_train])\n","print(total_training_choices.shape)\n","total_validation_choices = numpy.concatenate([choices_00_validation, choices_02_validation, choices_04_validation, choices_06_validation])\n","print(total_validation_choices.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["==== DATA ====\n","(2960, 128, 313)\n","(331, 128, 313)\n","==== LABELS ====\n","(2960, 4)\n","(331, 4)\n","==== CHOICES ====\n","(2960,)\n","(331,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zkFwLBYH89tx","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9fc77d04-0fd2-45b9-efc1-d672221828c3"},"source":["# DATA AUGMENTATION\n","\n","####### DATA ######\n","# Estrazione frame 128x10 da ciascun spettrogramma\n","training_data = numpy.zeros((len(total_training)*101, 128, 10)) # Dataset utilizzato per il training\n","index = 0\n","for vector_array in total_training:\n","  i = 0\n","  while i < 303:\n","    vector_i = numpy.zeros((128,10))\n","    for j in range(0,128):\n","      vector_i[j] = vector_array[j][i:i+10]\n","    training_data[index] = vector_i\n","    index += 1\n","    i = i+3\n","\n","validation_data = numpy.zeros((len(total_validation)*101, 128, 10)) # Dataset utilizzato per il training\n","index = 0\n","for vector_array in total_validation:\n","  i = 0\n","  while i < 303:\n","    vector_i = numpy.zeros((128,10))\n","    for j in range(0,128):\n","      vector_i[j] = vector_array[j][i:i+10]\n","    validation_data[index] = vector_i\n","    index += 1\n","    i = i+3\n","###################\n","\n","####### LABELS ######\n","# Associazione della label associata a ciascun spettrogramma a ciascuno dei frame estratto da esso.\n","training_labels = []\n","for elem in total_training_label:\n","  if numpy.array_equal(elem, numpy.asarray([1,0,0,0])) :\n","    for i in range(101):\n","      training_labels.append([1,0,0,0])\n","  elif numpy.array_equal(elem, numpy.asarray([0,1,0,0])):\n","    for i in range(101):\n","      training_labels.append([0,1,0,0])\n","  elif numpy.array_equal(elem, numpy.asarray([0,0,1,0])):\n","    for i in range(101):\n","      training_labels.append([0,0,1,0])\n","  elif numpy.array_equal(elem, numpy.asarray([0,0,0,1])):\n","    for i in range(101):\n","      training_labels.append([0,0,0,1])\n","\n","validation_labels = []\n","for elem in total_validation_label:\n","  if numpy.array_equal(elem, numpy.asarray([1,0,0,0])) :\n","    for i in range(101):\n","      validation_labels.append([1,0,0,0])\n","  elif numpy.array_equal(elem, numpy.asarray([0,1,0,0])):\n","    for i in range(101):\n","      validation_labels.append([0,1,0,0])\n","  elif numpy.array_equal(elem, numpy.asarray([0,0,1,0])):\n","    for i in range(101):\n","      validation_labels.append([0,0,1,0])\n","  elif numpy.array_equal(elem, numpy.asarray([0,0,0,1])):\n","    for i in range(101):\n","      validation_labels.append([0,0,0,1])\n","\n","training_labels = numpy.asarray(training_labels) # Dataset utilizzato per il training\n","validation_labels = numpy.asarray(validation_labels) # Dataset utilizzato per il training\n","#####################\n","\n","\n","####### CHOICES ######\n","# Associazione della choice associata a ciascun spettrogramma a ciascuno dei frame estratto da esso. \n","training_choices = []\n","for elem in total_training_choices:\n","  if numpy.array_equal(elem, numpy.asarray(\"match\")) :\n","    for i in range(101):\n","      training_choices.append(\"match\")\n","  elif numpy.array_equal(elem, numpy.asarray(\"non_match\")):\n","    for i in range(101):\n","      training_choices.append(\"non_match\")\n","\n","validation_choices = []\n","for elem in total_validation_choices:\n","  if numpy.array_equal(elem, numpy.asarray(\"match\")) :\n","    for i in range(101):\n","      validation_choices.append(\"match\")\n","  elif numpy.array_equal(elem, numpy.asarray(\"non_match\")):\n","    for i in range(101):\n","      validation_choices.append(\"non_match\")\n","\n","training_choices = numpy.asarray(training_choices) # Dataset utilizzato per il training\n","validation_choices = numpy.asarray(validation_choices) # Dataset utilizzato per il training\n","######################\n","\n","print(\"==== DATA ====\")\n","print(training_data.shape)\n","print(validation_data.shape)\n","\n","print(\"==== LABELS ====\")\n","print(training_labels.shape)\n","print(validation_labels.shape)\n","\n","print(\"==== CHOICES ====\")\n","print(training_choices.shape)\n","print(validation_choices.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["==== DATA ====\n","(298960, 128, 10)\n","(33431, 128, 10)\n","==== LABELS ====\n","(298960, 4)\n","(33431, 4)\n","==== CHOICES ====\n","(298960,)\n","(33431,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QdtRhIAYXsLn"},"source":["# KERAS MODEL DEFINITION"]},{"cell_type":"code","metadata":{"id":"96e3XmeeX2GS"},"source":["# LAYER DEFINITION\n","def DenseBlock(input,n):\n","  x = Dense(n)(input)\n","  x = BatchNormalization()(x)\n","  x = Activation('relu')(x)\n","  return x\n","\n","input_Spect = Input(shape = [128, 10])\n","input_Label = Input(shape = [4,])\n","\n","# First Branch - Encoder\n","m = Flatten(input_shape = [128, 10])(input_Spect)\n","m = DenseBlock(m, 128)\n","m = DenseBlock(m, 128)\n","m = DenseBlock(m, 128)\n","m = DenseBlock(m, 64)\n","m = DenseBlock(m, 32)\n","m = DenseBlock(m, 16)\n","\n","# Second Branch - Conditioning Feed Forward Neural Network\n","x = Dense(16)(input_Label)\n","x = Activation('sigmoid')(x)\n","q = Dense(16)(input_Label)\n","\n","# Encoded Input Conditioning\n","m = Multiply()([x,m])\n","encoded_input_conditioned = Add()([q, m]) # Input da passare al decoder\n","\n","# Decoder\n","m = DenseBlock(encoded_input_conditioned, 128)\n","m = DenseBlock(m, 128)\n","m = DenseBlock(m, 128)\n","m = DenseBlock(m, 128)\n","m = DenseBlock(m, 128)\n","m = Dense(128*10)(m)\n","m = Reshape((128,10),  input_shape=(128*10,))(m) # Output del modello\n","\n","loss_tracker = keras.metrics.Mean(name=\"loss\")\n","mse_metric = keras.metrics.MeanSquaredError(name=\"mse\")\n","\n","class CustomModel(tensorflow.keras.Model):\n","    @property\n","    def metrics(self):\n","        return [loss_tracker, mse_metric]\n","\n","    def test_step(self, data):\n","        # Unpack the data\n","        x, y = data\n","        # Compute predictions\n","        y_pred = self([x[0],x[1]], training=False)\n","        # Indici match\n","        match = tf.where ( tf.equal(x[2][:], \"match\") )\n","        # Dati match\n","        data_match = K.gather(y, match)\n","        # Separazione dei dati PREDETTI sulla base degli indici relativi a match/non_match\n","        # Dati match\n","        pred_match = K.gather(y_pred, match)\n","\n","        # Update metrica\n","        mse_metric.update_state(data_match, pred_match)\n","\n","        return {\"mse\": mse_metric.result()}\n","    \n","    def train_step(self, data):\n","          # Unpack the data. Its structure depends on your model and on what you pass to `fit()`.\n","          x, y = data\n","\n","          # Vettore C utilizzato per il calcolo della loss in caso di non_match\n","          C = 5 \n","          # Valore di probabilità utilizzato come peso\n","          ALPHA = 0.75 \n","\n","          # Indici match\n","          match = tf.where ( tf.equal(x[2][:], \"match\") )\n","\n","          # Indici non_match\n","          not_match = tf.where ( tf.equal(x[2][:], \"non_match\") )\n","\n","          # Dati match\n","          data_match = K.gather(y, match)\n","\n","          with tf.GradientTape() as tape:\n","              y_pred = self([x[0],x[1]], training=True)  # Forward pass\n","\n","              # Separazione dei dati PREDETTI sulla base degli indici relativi a match/non_match\n","              # Dati match\n","              pred_match = K.gather(y_pred, match)\n","              # Dati non match\n","              pred_not_match = K.gather(y_pred, not_match) \n","\n","              loss_m = K.mean(keras.losses.mean_squared_error(data_match, pred_match)) + 1e-6  # Calcolo Loss Match\n","              loss_nm = K.mean(keras.losses.mean_squared_error(C,pred_not_match)) + 1e-6     # Calcolo Loss Non_Match\n","\n","              loss = ALPHA * loss_m + (1 - ALPHA) * loss_nm     # loss utilizzata per l'update dei pesi\n","\n","          # Compute gradients\n","          trainable_vars = self.trainable_variables\n","          gradients = tape.gradient(loss, trainable_vars)\n","\n","          # Update weights\n","          self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n","\n","          # Compute our own metrics\n","          loss_tracker.update_state(loss)\n","          mse_metric.update_state(y, y_pred)\n","          return {\"loss\": loss_tracker.result(), \"mse\": mse_metric.result()}\n","\n","def get_lr_metric(optimizer):\n","    def lr(y_true, y_pred):\n","        return optimizer._decayed_lr(tf.float32) # I use ._decayed_lr method instead of .lr\n","    return lr\n","\n","opt = keras.optimizers.Adam(\n","    learning_rate = 0.0001,\n","    beta_1=0.95,\n","    beta_2=0.999\n",")\n","\n","lr_metric = get_lr_metric(opt)\n","model = CustomModel(inputs=(input_Spect, input_Label), outputs = m)\n","model.compile(optimizer = opt, metrics=[\"mse\", lr_metric])\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7HjXJIU4bvUH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bdf475c5-d160-4106-92d6-bef3512b7c75"},"source":["history = model.fit([training_data, training_labels, training_choices], \n","          training_data, \n","          epochs=100, \n","          batch_size=128, \n","          validation_data=([validation_data, validation_labels, validation_choices], validation_data), shuffle=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","2336/2336 [==============================] - 46s 17ms/step - loss: 4.4862 - mse: 2.5465 - val_mse: 1.9044\n","Epoch 2/100\n","2336/2336 [==============================] - 40s 17ms/step - loss: 3.1397 - mse: 3.8106 - val_mse: 1.7468\n","Epoch 3/100\n","2336/2336 [==============================] - 40s 17ms/step - loss: 2.5885 - mse: 4.3490 - val_mse: 1.9587\n","Epoch 4/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 2.2950 - mse: 4.6334 - val_mse: 1.8671\n","Epoch 5/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 2.0925 - mse: 4.8225 - val_mse: 1.8583\n","Epoch 6/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 1.9390 - mse: 4.9696 - val_mse: 2.1792\n","Epoch 7/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 1.8152 - mse: 5.0979 - val_mse: 2.0829\n","Epoch 8/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 1.7043 - mse: 5.2040 - val_mse: 1.9117\n","Epoch 9/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 1.6230 - mse: 5.2851 - val_mse: 2.0806\n","Epoch 10/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 1.5336 - mse: 5.3749 - val_mse: 2.1738\n","Epoch 11/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 1.4516 - mse: 5.4606 - val_mse: 1.9329\n","Epoch 12/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 1.4017 - mse: 5.5049 - val_mse: 2.2666\n","Epoch 13/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 1.3419 - mse: 5.5717 - val_mse: 2.1828\n","Epoch 14/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 1.2884 - mse: 5.6258 - val_mse: 2.3657\n","Epoch 15/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 1.2257 - mse: 5.6837 - val_mse: 2.2875\n","Epoch 16/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 1.1793 - mse: 5.7381 - val_mse: 2.2522\n","Epoch 17/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 1.1475 - mse: 5.7740 - val_mse: 2.3211\n","Epoch 18/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 1.1030 - mse: 5.8153 - val_mse: 2.5237\n","Epoch 19/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 1.0592 - mse: 5.8654 - val_mse: 2.2851\n","Epoch 20/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 1.0300 - mse: 5.8967 - val_mse: 2.4657\n","Epoch 21/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.9926 - mse: 5.9333 - val_mse: 2.5535\n","Epoch 22/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.9767 - mse: 5.9508 - val_mse: 2.5343\n","Epoch 23/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.9435 - mse: 5.9806 - val_mse: 2.6036\n","Epoch 24/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.9117 - mse: 6.0192 - val_mse: 2.7112\n","Epoch 25/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.8797 - mse: 6.0547 - val_mse: 2.4821\n","Epoch 26/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.8669 - mse: 6.0635 - val_mse: 2.5293\n","Epoch 27/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.8390 - mse: 6.0954 - val_mse: 2.6295\n","Epoch 28/100\n","2336/2336 [==============================] - 37s 16ms/step - loss: 0.8185 - mse: 6.1178 - val_mse: 2.6087\n","Epoch 29/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.7994 - mse: 6.1395 - val_mse: 2.5083\n","Epoch 30/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.7882 - mse: 6.1480 - val_mse: 2.4248\n","Epoch 31/100\n","2336/2336 [==============================] - 37s 16ms/step - loss: 0.7664 - mse: 6.1753 - val_mse: 2.6635\n","Epoch 32/100\n","2336/2336 [==============================] - 37s 16ms/step - loss: 0.7551 - mse: 6.1886 - val_mse: 2.6036\n","Epoch 33/100\n","2336/2336 [==============================] - 37s 16ms/step - loss: 0.7285 - mse: 6.2122 - val_mse: 2.6479\n","Epoch 34/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.7159 - mse: 6.2264 - val_mse: 2.5385\n","Epoch 35/100\n","2336/2336 [==============================] - 37s 16ms/step - loss: 0.7099 - mse: 6.2339 - val_mse: 2.7215\n","Epoch 36/100\n","2336/2336 [==============================] - 37s 16ms/step - loss: 0.6889 - mse: 6.2553 - val_mse: 2.6865\n","Epoch 37/100\n","2336/2336 [==============================] - 37s 16ms/step - loss: 0.6727 - mse: 6.2728 - val_mse: 2.5120\n","Epoch 38/100\n","2336/2336 [==============================] - 37s 16ms/step - loss: 0.6613 - mse: 6.2840 - val_mse: 2.8793\n","Epoch 39/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.6595 - mse: 6.2877 - val_mse: 2.5812\n","Epoch 40/100\n","2336/2336 [==============================] - 39s 17ms/step - loss: 0.6425 - mse: 6.3059 - val_mse: 2.6498\n","Epoch 41/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.6359 - mse: 6.3099 - val_mse: 2.6785\n","Epoch 42/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.6238 - mse: 6.3230 - val_mse: 2.5909\n","Epoch 43/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.6104 - mse: 6.3366 - val_mse: 2.4257\n","Epoch 44/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.5965 - mse: 6.3509 - val_mse: 2.6933\n","Epoch 45/100\n","2336/2336 [==============================] - 37s 16ms/step - loss: 0.5964 - mse: 6.3516 - val_mse: 2.7653\n","Epoch 46/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.5879 - mse: 6.3606 - val_mse: 2.8343\n","Epoch 47/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.5779 - mse: 6.3714 - val_mse: 2.6698\n","Epoch 48/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.5683 - mse: 6.3800 - val_mse: 2.5063\n","Epoch 49/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.5636 - mse: 6.3869 - val_mse: 2.6327\n","Epoch 50/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.5552 - mse: 6.3928 - val_mse: 2.7270\n","Epoch 51/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.5455 - mse: 6.4017 - val_mse: 2.9922\n","Epoch 52/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.5384 - mse: 6.4105 - val_mse: 2.7364\n","Epoch 53/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.5293 - mse: 6.4218 - val_mse: 2.8785\n","Epoch 54/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.5304 - mse: 6.4196 - val_mse: 2.5841\n","Epoch 55/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.5182 - mse: 6.4317 - val_mse: 2.5902\n","Epoch 56/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.5171 - mse: 6.4332 - val_mse: 2.7225\n","Epoch 57/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.5120 - mse: 6.4370 - val_mse: 2.6463\n","Epoch 58/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.5074 - mse: 6.4444 - val_mse: 2.4092\n","Epoch 59/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.4958 - mse: 6.4549 - val_mse: 2.7539\n","Epoch 60/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.4968 - mse: 6.4528 - val_mse: 2.7619\n","Epoch 61/100\n","2336/2336 [==============================] - 39s 17ms/step - loss: 0.4933 - mse: 6.4555 - val_mse: 2.4470\n","Epoch 62/100\n","2336/2336 [==============================] - 39s 17ms/step - loss: 0.4846 - mse: 6.4647 - val_mse: 2.8196\n","Epoch 63/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.4813 - mse: 6.4678 - val_mse: 2.5077\n","Epoch 64/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.4726 - mse: 6.4787 - val_mse: 2.6383\n","Epoch 65/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.4712 - mse: 6.4797 - val_mse: 2.6943\n","Epoch 66/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.4695 - mse: 6.4795 - val_mse: 2.6603\n","Epoch 67/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.4611 - mse: 6.4879 - val_mse: 2.8160\n","Epoch 68/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.4572 - mse: 6.4938 - val_mse: 2.7766\n","Epoch 69/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.4550 - mse: 6.4953 - val_mse: 2.8940\n","Epoch 70/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.4569 - mse: 6.4929 - val_mse: 2.8590\n","Epoch 71/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.4529 - mse: 6.4964 - val_mse: 2.8573\n","Epoch 72/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.4475 - mse: 6.5059 - val_mse: 2.7368\n","Epoch 73/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.4414 - mse: 6.5073 - val_mse: 2.8358\n","Epoch 74/100\n","2336/2336 [==============================] - 37s 16ms/step - loss: 0.4390 - mse: 6.5118 - val_mse: 2.7589\n","Epoch 75/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.4283 - mse: 6.5203 - val_mse: 2.7968\n","Epoch 76/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.4269 - mse: 6.5208 - val_mse: 2.8597\n","Epoch 77/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.4336 - mse: 6.5167 - val_mse: 2.8738\n","Epoch 78/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.4239 - mse: 6.5274 - val_mse: 2.9013\n","Epoch 79/100\n","2336/2336 [==============================] - 37s 16ms/step - loss: 0.4215 - mse: 6.5258 - val_mse: 2.6689\n","Epoch 80/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.4200 - mse: 6.5303 - val_mse: 2.7010\n","Epoch 81/100\n","2336/2336 [==============================] - 37s 16ms/step - loss: 0.4125 - mse: 6.5346 - val_mse: 2.4653\n","Epoch 82/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.4163 - mse: 6.5314 - val_mse: 2.7855\n","Epoch 83/100\n","2336/2336 [==============================] - 37s 16ms/step - loss: 0.4071 - mse: 6.5418 - val_mse: 2.9294\n","Epoch 84/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.4080 - mse: 6.5424 - val_mse: 2.9103\n","Epoch 85/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.3986 - mse: 6.5500 - val_mse: 2.8207\n","Epoch 86/100\n","2336/2336 [==============================] - 37s 16ms/step - loss: 0.4020 - mse: 6.5472 - val_mse: 2.9509\n","Epoch 87/100\n","2336/2336 [==============================] - 37s 16ms/step - loss: 0.3964 - mse: 6.5518 - val_mse: 2.8714\n","Epoch 88/100\n","2336/2336 [==============================] - 37s 16ms/step - loss: 0.3965 - mse: 6.5530 - val_mse: 2.7073\n","Epoch 89/100\n","2336/2336 [==============================] - 37s 16ms/step - loss: 0.3913 - mse: 6.5546 - val_mse: 2.6202\n","Epoch 90/100\n","2336/2336 [==============================] - 37s 16ms/step - loss: 0.3939 - mse: 6.5528 - val_mse: 2.7137\n","Epoch 91/100\n","2336/2336 [==============================] - 37s 16ms/step - loss: 0.3901 - mse: 6.5566 - val_mse: 2.6429\n","Epoch 92/100\n","2336/2336 [==============================] - 38s 16ms/step - loss: 0.3779 - mse: 6.5690 - val_mse: 3.1082\n","Epoch 93/100\n","2336/2336 [==============================] - 39s 17ms/step - loss: 0.3845 - mse: 6.5639 - val_mse: 2.7196\n","Epoch 94/100\n","2336/2336 [==============================] - 40s 17ms/step - loss: 0.3748 - mse: 6.5728 - val_mse: 2.8754\n","Epoch 95/100\n","2336/2336 [==============================] - 39s 17ms/step - loss: 0.3806 - mse: 6.5660 - val_mse: 2.7883\n","Epoch 96/100\n","2336/2336 [==============================] - 40s 17ms/step - loss: 0.3715 - mse: 6.5762 - val_mse: 2.7296\n","Epoch 97/100\n","2336/2336 [==============================] - 39s 17ms/step - loss: 0.3748 - mse: 6.5720 - val_mse: 2.8898\n","Epoch 98/100\n","2336/2336 [==============================] - 40s 17ms/step - loss: 0.3759 - mse: 6.5708 - val_mse: 2.7221\n","Epoch 99/100\n","2336/2336 [==============================] - 39s 17ms/step - loss: 0.3724 - mse: 6.5745 - val_mse: 2.8355\n","Epoch 100/100\n","2336/2336 [==============================] - 40s 17ms/step - loss: 0.3716 - mse: 6.5754 - val_mse: 2.7200\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ktyeNLjDcxJZ"},"source":["# Salvataggio del modello\n","model.save('/content/drive/MyDrive/models/valve/model_valve.h5')\n","\n","# Salvataggio history di apprendimento\n","with open('/content/drive/MyDrive/models/valve/trainHistoryDict', 'wb') as file_pi:\n","    pickle.dump(history.history, file_pi)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sN8yMBbSLYb1"},"source":["# TESTING"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"alzE-821wFIi","outputId":"b327c617-bf5d-4b5b-b01a-f6947de0a592"},"source":["import csv\n","\n","def save_csv(save_file_path,\n","             save_data):\n","    with open(save_file_path, \"w\", newline=\"\") as f:\n","        writer = csv.writer(f, lineterminator='\\n')\n","        writer.writerows(save_data)\n","\n","# load dataset\n","def select_dirs(path):\n","    dir_path = os.path.abspath(path)\n","    dirs = sorted(glob.glob(dir_path))\n","    return dirs\n","\n","def file_load(wav_name, mono=False):\n","    try:\n","        return librosa.load(wav_name, sr=None, mono=mono)\n","    except:\n","        logger.error(\"file_broken or not exists!! : {}\".format(wav_name))\n","\n","def file_list_generator(target_dir, dir_name=\"train\", ext=\"wav\"):\n","    print(\"target_dir : {}\".format(target_dir))\n","\n","    # generate training list\n","    training_list_path = os.path.abspath(\"{dir}/{dir_name}/*.{ext}\".format(dir=target_dir, dir_name=dir_name, ext=ext))\n","    files = sorted(glob.glob(training_list_path))\n","    if len(files) == 0:\n","      print(\"errore\")\n","    return files\n","\n","\n","def file_to_vector_array(file_name, n_mels=64, n_fft=1024, hop_length=512, power=2.0):\n","    # 02 generate melspectrogram using librosa\n","    y, sr = file_load(file_name)\n","    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, power=power)\n","\n","    # 03 convert melspectrogram to log mel energy\n","    log_mel_spectrogram = 20.0 / power * numpy.log10(mel_spectrogram + sys.float_info.epsilon)\n","\n","    return log_mel_spectrogram\n","\n","  \n","def list_to_vector_array(file_list, msg=\"calc...\", n_mels=64, n_fft=1024, hop_length=512, power=2.0, frames=10):\n","    # iterate file_to_vector_array()\n","    for idx in tqdm(range(len(file_list)), desc=msg):\n","        vector_array = file_to_vector_array(file_list[idx], n_mels=n_mels, n_fft=n_fft, hop_length=hop_length, power=power)\n","       \n","        # vector_array = numpy.delete(vector_array,[310,311,312], axis=1)\n","        # vector_array = numpy.asarray(numpy.hsplit(vector_array, 31))\n","\n","        if idx == 0:\n","            dataset = numpy.zeros((len(file_list), n_mels, frames), float)\n","        dataset[vector_array.shape[0] * idx: vector_array.shape[0] * (idx + 1), :] = vector_array\n","    return dataset\n","\n","def key_by_id(item):\n","  path_splitted = item.split(\"/\")\n","  file_name = path_splitted[ len(path_splitted) - 1 ]\n","  file_name_splitted = file_name.split(\"_\")\n","  machine_id = file_name_splitted = file_name_splitted[2]\n","  return machine_id\n","\n","def get_machine_id_list_for_test(target_dir,\n","                                 dir_name=\"test\",\n","                                 ext=\"wav\"):\n","\n","    # create test files\n","    dir_path = os.path.abspath(\"{dir}/{dir_name}/*.{ext}\".format(dir=target_dir, dir_name=dir_name, ext=ext))\n","    file_paths = sorted(glob.glob(dir_path))\n","    # extract id\n","    machine_id_list = sorted(list(set(itertools.chain.from_iterable(\n","        [re.findall('id_[0-9][0-9]', ext_id) for ext_id in file_paths]))))\n","    return machine_id_list\n","\n","def test_file_list_generator(target_dir,\n","                             id_name,\n","                             dir_name=\"test\",\n","                             prefix_normal=\"normal\",\n","                             prefix_anomaly=\"anomaly\",\n","                             ext=\"wav\"):\n","  \n","    print(\"target_dir : {}\".format(target_dir+\"_\"+id_name))\n","\n","    normal_files = sorted(\n","    glob.glob(\"{dir}/{dir_name}/{prefix_normal}_{id_name}*.{ext}\".format(dir=target_dir,\n","                                                                                 dir_name=dir_name,\n","                                                                                 prefix_normal=prefix_normal,\n","                                                                                 id_name=id_name,\n","                                                                                 ext=ext)))\n","    normal_labels = numpy.zeros(len(normal_files))\n","    anomaly_files = sorted(\n","    glob.glob(\"{dir}/{dir_name}/{prefix_anomaly}_{id_name}*.{ext}\".format(dir=target_dir,\n","                                                                                  dir_name=dir_name,\n","                                                                                  prefix_anomaly=prefix_anomaly,\n","                                                                                  id_name=id_name,\n","                                                                                  ext=ext)))\n","    anomaly_labels = numpy.ones(len(anomaly_files))\n","    files = numpy.concatenate((normal_files, anomaly_files), axis=0)\n","    labels = numpy.concatenate((normal_labels, anomaly_labels), axis=0)\n","    print(\"test_file  num : {num}\".format(num=len(files)))\n","    if len(files) == 0:\n","        print(\"no_wav_file!!\")\n","    print(\"\\n========================================\")\n","\n","    return files, labels\n","\n","target_dir = \"/content/drive/MyDrive/test/valve\"\n","\n","performance = []\n","machine_type = os.path.split(target_dir)[1]\n","print(\"============== MODEL LOAD ==============\")\n","# set model path\n","model_file = \"/content/drive/MyDrive/models/valve/model_valve.h5\"\n","\n","# load model file\n","if not os.path.exists(model_file):\n","  print(\"{} model not found \".format(machine_type))\n","  sys.exit(-1)\n","model = tensorflow.keras.models.load_model(model_file, custom_objects={'CustomModel': CustomModel, 'mse':mse_metric, 'lr': lr_metric})\n","# model.summary()\n","\n","machine_id_list = get_machine_id_list_for_test(target_dir)\n","\n","# initialize lines in csv for AUC and pAUC\n","csv_lines = []\n","\n","csv_lines.append([machine_type])\n","csv_lines.append([\"id\", \"AUC\", \"pAUC\"])\n","performance = []\n","\n","for id_str in machine_id_list:\n","  # load test file\n","\n","  id_num = id_str.split(\"_\")[1]\n","\n","  # Definizione della label \"match\" da utilizzare in fase di testing e del min e max da utilizzare per la normalizzazione\n","  # i min e max sono stati calcolati a partire dai dati di training.\n","  if id_num == \"00\":\n","    match_labels = numpy.asarray([1,0,0,0])\n","    mean = mean_00\n","    std = std_00\n","  elif id_num == \"02\":\n","    match_labels = numpy.asarray([0,1,0,0])\n","    mean = mean_02\n","    std = std_02\n","  elif id_num == \"04\":\n","    match_labels = numpy.asarray([0,0,1,0])\n","    mean = mean_04\n","    std = std_04\n","  elif id_num == \"06\": \n","    match_labels = numpy.asarray([0,0,0,1])\n","    mean = mean_06\n","    std = std_06\n","\n","  test_files, y_true = test_file_list_generator(target_dir, id_str)\n","  #print(\"\\n====== True Labels ======\")\n","  #print(y_true)\n","  #print(\"==> ====== Match ID Labels ======\")\n","  #print(match_labels.shape)\n","  #print(\"=================================\\n\")\n","\n","  # setup anomaly score file path\n","  anomaly_score_csv = \"/content/drive/MyDrive/models/valve/anomaly_score_{machine_type}_{id_str}.csv\".format(machine_type=machine_type, id_str=id_str)\n","  anomaly_score_list = []\n","\n","  print(\"\\n============== BEGIN TEST FOR A MACHINE ID {id} ==============\".format(id=id_num))\n","\n","  y_pred = [0. for k in test_files]\n","\n","  for file_idx, file_path in tqdm(enumerate(test_files), total=len(test_files)):\n","\n","    # Estrazione spettrogramma audio test\n","    data = file_to_vector_array(file_path, n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH, power=POWER)\n","\n","    # Normalizzazione spettrogramma di test\n","    data = ( data - mean ) / (std)\n","\n","    # Estrazione delle frame 128x10\n","    data_splitted = numpy.zeros((61, 128, 10))\n","    index = 0\n","    i = 0\n","    while i < 303:\n","      vector_i = numpy.zeros((128,10))\n","      for j in range(0,128):\n","        vector_i[j] = data[j][i:i+10]\n","      data_splitted[index] = vector_i\n","      index += 1\n","      i = i+5\n","\n","    # Calcolo dell'errore medio sulle frame estratte dallo spettrogramma\n","    elem_error = []\n","    for elem in data_splitted:\n","      predicted = model.predict([elem.reshape(1,128,10), match_labels.reshape(1,4)])\n","\n","      errors = numpy.mean(numpy.square(elem - predicted), axis=1)\n","      elem_error.append(numpy.mean(errors))\n","    # Log dell'errore associato all'istanza di test\n","    y_pred[file_idx] = numpy.mean(elem_error)\n","    anomaly_score_list.append([os.path.basename(file_path), y_pred[file_idx]])\n","  \n","  save_csv(save_file_path=anomaly_score_csv, save_data=anomaly_score_list)\n","\n"," # Calcolo AUC e pAUC per i dati con un certo ID_0x\n","  auc = metrics.roc_auc_score(y_true, y_pred)\n","  p_auc = metrics.roc_auc_score(y_true, y_pred, max_fpr=0.1)\n","  csv_lines.append([id_str.split(\"_\", 1)[1], auc, p_auc])\n","  performance.append([auc, p_auc])\n","  print(\"AUC : {}\".format(auc))\n","  print(\"pAUC : {}\".format(p_auc))\n","\n","  print(\"\\n============ END OF TEST FOR A MACHINE ID ============\")\n","\n","# Stampa di AUC e pAUC medi su tutti i dati di test (media di AUC e pAUC sui vari ID).\n","print(\"\\n============ AVERAGE PERFORMANCES ============\")\n","averaged_performance = numpy.mean(numpy.array(performance, dtype=float), axis=0)\n","csv_lines.append([\"Average\"] + list(averaged_performance))\n","print(averaged_performance)\n","\n","result_path = \"/content/drive/MyDrive/models/valve/anomaly_score_avg.csv\"\n","save_csv(save_file_path=result_path, save_data=csv_lines)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["============== MODEL LOAD ==============\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 0/219 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["target_dir : /content/drive/MyDrive/test/valve_id_00\n","test_file  num : 219\n","\n","========================================\n","\n","============== BEGIN TEST FOR A MACHINE ID 00 ==============\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 219/219 [12:32<00:00,  3.44s/it]\n","  0%|          | 0/220 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["AUC : 0.8091596638655462\n","pAUC : 0.5811587793011942\n","\n","============ END OF TEST FOR A MACHINE ID ============\n","target_dir : /content/drive/MyDrive/test/valve_id_02\n","test_file  num : 220\n","\n","========================================\n","\n","============== BEGIN TEST FOR A MACHINE ID 02 ==============\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 220/220 [12:46<00:00,  3.49s/it]\n","  0%|          | 0/220 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["AUC : 0.6439166666666668\n","pAUC : 0.5087719298245614\n","\n","============ END OF TEST FOR A MACHINE ID ============\n","target_dir : /content/drive/MyDrive/test/valve_id_04\n","test_file  num : 220\n","\n","========================================\n","\n","============== BEGIN TEST FOR A MACHINE ID 04 ==============\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 220/220 [12:48<00:00,  3.49s/it]\n","  0%|          | 0/220 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["AUC : 0.68575\n","pAUC : 0.5416666666666666\n","\n","============ END OF TEST FOR A MACHINE ID ============\n","target_dir : /content/drive/MyDrive/test/valve_id_06\n","test_file  num : 220\n","\n","========================================\n","\n","============== BEGIN TEST FOR A MACHINE ID 06 ==============\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 220/220 [12:27<00:00,  3.40s/it]\n"],"name":"stderr"},{"output_type":"stream","text":["AUC : 0.7164166666666667\n","pAUC : 0.5640350877192982\n","\n","============ END OF TEST FOR A MACHINE ID ============\n","\n","============ AVERAGE PERFORMANCES ============\n","[0.71381075 0.54890812]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aQWFuCYywfJf"},"source":["!wget -O /content/drive/MyDrive/dev_data_valve.zip https://zenodo.org/record/3678171/files/dev_data_valve.zip?download=1\n","!unzip /content/drive/MyDrive/dev_data_valve.zip -d /content/drive/MyDrive/"],"execution_count":null,"outputs":[]}]}