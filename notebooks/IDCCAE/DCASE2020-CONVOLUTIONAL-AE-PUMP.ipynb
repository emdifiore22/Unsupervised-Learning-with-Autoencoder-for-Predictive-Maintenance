{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "DCASE2020-CONVOLUTIONAL-AE-PUMP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shGJCT1ze5JD"
      },
      "source": [
        "# IMPORT AND DATA LOADING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVXNaYh2hqNf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93e6db65-d8c6-4e29-8b7f-6d2bf10340e6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "sK9FvLade5JG"
      },
      "source": [
        "# import necessari\n",
        "import librosa\n",
        "import numpy\n",
        "import sys\n",
        "import os\n",
        "import glob\n",
        "import itertools\n",
        "import re\n",
        "import pickle\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.models\n",
        "import tensorflow.keras.backend as K\n",
        "import keras.optimizers\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, BatchNormalization, Activation, Flatten, Multiply, Add, Reshape\n",
        "from tqdm import tqdm\n",
        "from itertools import groupby\n",
        "from keras.utils import to_categorical\n",
        "from sklearn import metrics\n",
        "\n",
        "# costanti \n",
        "ALPHA = 0.75\n",
        "N_MELS = 128\n",
        "HOP_LENGTH = 512\n",
        "N_FFT = 1024\n",
        "POWER = 2.0\n",
        "FRAME_NUMS = 313\n",
        "FRAMES = 10\n",
        "\n",
        "# FEATURES EXTRACTION\n",
        "\n",
        "# Loading da Google Drive\n",
        "train_data = numpy.load(\"/content/drive/MyDrive/DCASE_DATA_EXTRACTED/train/training_pump.npy\")\n",
        "grouped_list_by_machine_id = pickle.load( open( \"/content/drive/MyDrive/DCASE_DATA_EXTRACTED/train/training_pump_grouped_list.npy\", \"rb\" ) )"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "9FMJuYSMe5JH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0305d533-e970-4a70-cb25-f86060fd16a1"
      },
      "source": [
        "# GENERAZIONE DELLE LABELS\n",
        "# One-hot encoding\n",
        "label = []\n",
        "choices = []\n",
        "for i in range(0, len(grouped_list_by_machine_id)):\n",
        "  for j in range(0, len(grouped_list_by_machine_id[i])):\n",
        "    machine_id = grouped_list_by_machine_id[i][j].split('/')[7].split('_')[2]\n",
        "    #print(grouped_list_by_machine_id[i][j].split('/')[7])\n",
        "    random_choice = numpy.random.choice([\"match\", \"non_match\"], p = [ALPHA, 1-ALPHA]) \n",
        "\n",
        "    if machine_id == '00':\n",
        "      if random_choice == \"match\":\n",
        "        to_append = [1,0,0,0]\n",
        "      else: \n",
        "        not_match_label = numpy.random.choice([1, 2, 3]) \n",
        "        if not_match_label == 1:\n",
        "          to_append = [0,1,0,0]\n",
        "        elif not_match_label == 2:\n",
        "          to_append = [0,0,1,0]\n",
        "        else: \n",
        "          to_append = [0,0,0,1]\n",
        "\n",
        "    elif machine_id == '02': \n",
        "      if random_choice == \"match\":\n",
        "        to_append = [0,1,0,0]\n",
        "      else: \n",
        "        not_match_label = numpy.random.choice( [ 1, 2, 3] ) \n",
        "        if not_match_label == 1:\n",
        "          to_append = [1,0,0,0]\n",
        "        elif not_match_label == 2:\n",
        "          to_append = [0,0,1,0]\n",
        "        else: \n",
        "          to_append = [0,0,0,1]\n",
        "\n",
        "    elif machine_id == \"04\":\n",
        "      if random_choice == \"match\":\n",
        "        to_append = [0,0,1,0]\n",
        "      else: \n",
        "        not_match_label = numpy.random.choice( [ 1, 2, 3] ) \n",
        "        if not_match_label == 1:\n",
        "          to_append = [1,0,0,0]\n",
        "        elif not_match_label == 2:\n",
        "          to_append = [0,1,0,0]\n",
        "        else: \n",
        "          to_append = [0,0,0,1]\n",
        "\n",
        "    elif machine_id == \"06\":\n",
        "      if random_choice == \"match\":\n",
        "        to_append = [0,0,0,1]\n",
        "      else: \n",
        "        not_match_label = numpy.random.choice( [ 1, 2, 3] ) \n",
        "        if not_match_label == 1:\n",
        "          to_append = [1,0,0,0]\n",
        "        elif not_match_label == 2:\n",
        "          to_append = [0,1,0,0]\n",
        "        else: \n",
        "          to_append = [0,0,1,0]\n",
        "    \n",
        "    label.append(to_append) # Append della label associata a ciascuno spettrogramma\n",
        "    choices.append(random_choice) # Append della choice utilizzata per associare la label\n",
        "                                  # La choice sarà utile in fase di addestramento per capire che tipo di loss calcolare\n",
        "\n",
        "# Trasformazione in numpy.array     \n",
        "label = numpy.asarray(label)\n",
        "choices = numpy.asarray(choices)\n",
        "print(label.shape)\n",
        "print(choices.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3349, 4)\n",
            "(3349,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Y1ElEYume5JI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e40afdc9-44c9-4df5-efab-3f266b4a2471"
      },
      "source": [
        "print(len(grouped_list_by_machine_id[0]))\n",
        "print(len(grouped_list_by_machine_id[1]))\n",
        "print(len(grouped_list_by_machine_id[2]))\n",
        "print(len(grouped_list_by_machine_id[3]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "906\n",
            "905\n",
            "602\n",
            "936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "GwyJRVVme5JJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fe8e1ff-8e3f-4572-ee2b-103daaefcdc0"
      },
      "source": [
        "# Estrazione spettrogrammi divisi per ID\n",
        "id_00 = train_data[0:906]\n",
        "id_02 = train_data[906:1811]\n",
        "id_04 = train_data[1811:2413]\n",
        "id_06 = train_data[2413:3349]\n",
        "\n",
        "# Z-Score Normalization ID_00\n",
        "id_00_norm = numpy.empty_like(id_00)\n",
        "mean_00 = numpy.mean(id_00)\n",
        "std_00 = numpy.std(id_00)\n",
        "id_00_norm = (id_00 - mean_00) / std_00\n",
        "\n",
        "# Z-Score Normalization ID_02\n",
        "id_02_norm = numpy.empty_like(id_02)\n",
        "mean_02 = numpy.mean(id_02)\n",
        "std_02 = numpy.std(id_02)\n",
        "id_02_norm = (id_02 - mean_02) / std_02\n",
        "\n",
        "# Z-Score Normalization ID_04\n",
        "id_04_norm = numpy.empty_like(id_04)\n",
        "mean_04 = numpy.mean(id_04)\n",
        "std_04 = numpy.std(id_04)\n",
        "id_04_norm = (id_04 - mean_04) / std_04\n",
        "\n",
        "# Z-Score Normalization ID_06\n",
        "id_06_norm = numpy.empty_like(id_06)\n",
        "mean_06 = numpy.mean(id_06)\n",
        "std_06 = numpy.std(id_06)\n",
        "id_06_norm = (id_06 - mean_06) / std_06\n",
        "\n",
        "print(\"Mean: {m}\".format(m=mean_00))\n",
        "print(\"Dev.Std: {d}\".format(d=std_00))\n",
        "print(id_00_norm.shape)\n",
        "\n",
        "print(\"Mean: {m}\".format(m=mean_02))\n",
        "print(\"Dev.Std: {d}\".format(d=std_02))\n",
        "print(id_02_norm.shape)\n",
        "\n",
        "print(\"Mean: {m}\".format(m=mean_04))\n",
        "print(\"Dev.Std: {d}\".format(d=std_04))\n",
        "print(id_04_norm.shape)\n",
        "\n",
        "print(\"Mean: {m}\".format(m=mean_06))\n",
        "print(\"Dev.Std: {d}\".format(d=std_06))\n",
        "print(id_06_norm.shape)\n",
        "\n",
        "train_data_norm = numpy.concatenate([id_00_norm, id_02_norm, id_04_norm, id_06_norm])\n",
        "\n",
        "training = numpy.zeros((len(train_data_norm)*15, 128, 32)) # Dataset utilizzato per il training\n",
        "index = 0\n",
        "for vector_array in train_data_norm:\n",
        "  i = 0\n",
        "  while (i+32) <= 313:\n",
        "    vector_i = numpy.zeros((128,32))\n",
        "    for j in range(0,128):\n",
        "      vector_i[j] = vector_array[j][i:i+32]\n",
        "    training[index] = vector_i\n",
        "    index += 1\n",
        "    i = i+20\n",
        "    \n",
        "\n",
        "# Associazione della label associata a ciascun spettrogramma a ciascuno dei frame estratto da esso.\n",
        "training_labels = []\n",
        "for elem in label:\n",
        "  if numpy.array_equal(elem, numpy.asarray([1,0,0,0])) :\n",
        "    for i in range(15):\n",
        "      training_labels.append([1,0,0,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,1,0,0])):\n",
        "    for i in range(15):\n",
        "      training_labels.append([0,1,0,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,0,1,0])):\n",
        "    for i in range(15):\n",
        "      training_labels.append([0,0,1,0])\n",
        "  elif numpy.array_equal(elem, numpy.asarray([0,0,0,1])):\n",
        "    for i in range(15):\n",
        "      training_labels.append([0,0,0,1])\n",
        "\n",
        "# Associazione della choice associata a ciascun spettrogramma a ciascuno dei frame estratto da esso. \n",
        "training_choices = []\n",
        "for elem in choices:\n",
        "  if numpy.array_equal(elem, numpy.asarray(\"match\")) :\n",
        "    for i in range(15):\n",
        "      training_choices.append(\"match\")\n",
        "  elif numpy.array_equal(elem, numpy.asarray(\"non_match\")):\n",
        "    for i in range(15):\n",
        "      training_choices.append(\"non_match\")\n",
        "\n",
        "training_labels = numpy.asarray(training_labels) # Dataset utilizzato per il training\n",
        "training_choices = numpy.asarray(training_choices) # Dataset utilizzato per il training\n",
        "\n",
        "\n",
        "# Shuffling\n",
        "split_validation = int(len(train_data_norm)*15*0.1)\n",
        "split_train = int(len(train_data_norm)*15 - split_validation)\n",
        "print(split_train)\n",
        "randomize = numpy.arange(len(training))\n",
        "numpy.random.shuffle(randomize)\n",
        "training_tot_shuffle = training[randomize]\n",
        "training_tot_labels_shuffle = training_labels[randomize]\n",
        "training_tot_choices_shuffle = training_choices[randomize]\n",
        "\n",
        "training_shuffle = training_tot_shuffle[:split_train]\n",
        "validation_shuffle = training_tot_shuffle[-split_validation:]\n",
        "\n",
        "training_labels_shuffle = training_tot_labels_shuffle[:split_train]\n",
        "validation_labels_shuffle = training_tot_labels_shuffle[- split_validation:]\n",
        "\n",
        "training_choices_shuffle = training_tot_choices_shuffle[:split_train]\n",
        "validation_choices_shuffle = training_tot_choices_shuffle[- split_validation:]\n",
        "\n",
        "print(training_shuffle.shape)\n",
        "print(training_labels_shuffle.shape)\n",
        "print(training_choices_shuffle.shape)\n",
        "print(validation_shuffle.shape)\n",
        "print(validation_labels_shuffle.shape)\n",
        "print(validation_choices_shuffle.shape)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean: -29.656117535853326\n",
            "Dev.Std: 8.065205243458877\n",
            "(906, 128, 313)\n",
            "Mean: -29.71556073662517\n",
            "Dev.Std: 8.29815727525677\n",
            "(905, 128, 313)\n",
            "Mean: -28.522272932687812\n",
            "Dev.Std: 7.003299452150415\n",
            "(602, 128, 313)\n",
            "Mean: -31.05105694934055\n",
            "Dev.Std: 9.429433266661944\n",
            "(936, 128, 313)\n",
            "45212\n",
            "(45212, 128, 32)\n",
            "(45212, 4)\n",
            "(45212,)\n",
            "(5023, 128, 32)\n",
            "(5023, 4)\n",
            "(5023,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsmnTUOWe5JK"
      },
      "source": [
        "# KERAS MODEL DEFINITION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "zMEt6TDWe5JL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31932b9c-892c-423c-c714-da53ad254318"
      },
      "source": [
        "# KERAS MODEL\n",
        "\n",
        "input_img = keras.Input(shape=(128, 32, 1))  # adapt this if using 'channels_first' image data format\n",
        "input_Label = keras.Input(shape = [4,])\n",
        "\n",
        "# encoder\n",
        "x = keras.layers.Conv2D(32, (5, 5),strides=(2,1), padding='same')(input_img)   #32x128 -> 32x64\n",
        "x = keras.layers.BatchNormalization()(x)\n",
        "x = keras.layers.Activation('relu')(x)\n",
        "x = keras.layers.Conv2D(64, (5, 5),strides=(2,1), padding='same')(x)           #32x32\n",
        "x = keras.layers.BatchNormalization()(x)\n",
        "x = keras.layers.Activation('relu')(x)\n",
        "x = keras.layers.Conv2D(128, (5, 5),strides=(2,2), padding='same')(x)          #16x16\n",
        "x = keras.layers.BatchNormalization()(x)\n",
        "x = keras.layers.Activation('relu')(x)\n",
        "x = keras.layers.Conv2D(256, (3, 3),strides=(2,2), padding='same')(x)          #8x8\n",
        "x = keras.layers.BatchNormalization()(x)\n",
        "x = keras.layers.Activation('relu')(x)\n",
        "x = keras.layers.Conv2D(512, (3, 3),strides=(2,2), padding='same')(x)          #4x4\n",
        "x = keras.layers.BatchNormalization()(x)\n",
        "x = keras.layers.Activation('relu')(x)\n",
        "\n",
        "volumeSize = keras.backend.int_shape(x)\n",
        "# at this point the representation size is latentDim i.e. latentDim-dimensional\n",
        "x = keras.layers.Conv2D(40, (4,4), strides=(1,1), padding='valid')(x)\n",
        "encoded = keras.layers.Flatten()(x)\n",
        "\n",
        "# Second Branch - Conditioning Feed Forward Neural Network\n",
        "c = keras.layers.Dense(40)(input_Label)\n",
        "c = keras.layers.Activation('sigmoid')(c)\n",
        "q = keras.layers.Dense(40)(input_Label)\n",
        "\n",
        "m = keras.layers.Multiply()([c,encoded])\n",
        "encoded_input_conditioned = keras.layers.Add()([q, m]) # Input da passare al decoder\n",
        "    \n",
        "# decoder\n",
        "x = keras.layers.Dense(volumeSize[1] * volumeSize[2] * volumeSize[3])(encoded_input_conditioned) \n",
        "x = keras.layers.Reshape((volumeSize[1], volumeSize[2], 512))(x)                #4x4\n",
        "\n",
        "x = keras.layers.Conv2DTranspose(256, (3, 3),strides=(2,2), padding='same')(x)  #8x8\n",
        "x = keras.layers.BatchNormalization()(x)\n",
        "x = keras.layers.Activation('relu')(x)\n",
        "x = keras.layers.Conv2DTranspose(128, (3, 3),strides=(2,2), padding='same')(x)  #16x16   \n",
        "x = keras.layers.BatchNormalization()(x)\n",
        "x = keras.layers.Activation('relu')(x)\n",
        "x = keras.layers.Conv2DTranspose(64, (5, 5),strides=(2,2), padding='same')(x)   #32x32\n",
        "x = keras.layers.BatchNormalization()(x)\n",
        "x = keras.layers.Activation('relu')(x)\n",
        "x = keras.layers.Conv2DTranspose(32, (5, 5),strides=(2,1), padding='same')(x)   #32x64\n",
        "x = keras.layers.BatchNormalization()(x)\n",
        "x = keras.layers.Activation('relu')(x)\n",
        "    \n",
        "decoded = keras.layers.Conv2DTranspose(1, (5, 5),strides=(2,1), padding='same')(x) \n",
        "decoded_reshaped = keras.layers.Reshape((1, 128, 32))(decoded)  \n",
        "\n",
        "loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "mse_metric = keras.metrics.MeanSquaredError(name=\"mse\")\n",
        "\n",
        "class CustomModel(keras.Model):\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [loss_tracker, mse_metric]\n",
        "\n",
        "    def test_step(self, data):\n",
        "        # Unpack the data\n",
        "        x, y = data\n",
        "        # Compute predictions\n",
        "        y_pred = self([x[0],x[1]], training=False)\n",
        "        # Indici match\n",
        "        match = tf.where ( tf.equal(x[2][:], \"match\") )\n",
        "        # Dati match\n",
        "        data_match = K.gather(y, match)\n",
        "        # Separazione dei dati PREDETTI sulla base degli indici relativi a match/non_match\n",
        "        # Dati match\n",
        "        pred_match = K.gather(y_pred, match)\n",
        "\n",
        "        # Update metrica\n",
        "        mse_metric.update_state(data_match, pred_match)\n",
        "\n",
        "        return {\"mse\": mse_metric.result()}\n",
        "    \n",
        "    def train_step(self, data):\n",
        "          # Unpack the data. Its structure depends on your model and on what you pass to `fit()`.\n",
        "          x, y = data\n",
        "\n",
        "          # Vettore C utilizzato per il calcolo della loss in caso di non_match\n",
        "          C = 5 \n",
        "          # Valore di probabilità utilizzato come peso\n",
        "          ALPHA = 0.75 \n",
        "\n",
        "          # Indici match\n",
        "          match = tf.where ( tf.equal(x[2][:], \"match\") )\n",
        "\n",
        "          # Indici non_match\n",
        "          not_match = tf.where ( tf.equal(x[2][:], \"non_match\") )\n",
        "\n",
        "          # Dati match\n",
        "          data_match = K.gather(y, match)\n",
        "\n",
        "          with tf.GradientTape() as tape:\n",
        "              y_pred = self([x[0],x[1]], training=True)  # Forward pass\n",
        "\n",
        "              # Separazione dei dati PREDETTI sulla base degli indici relativi a match/non_match\n",
        "              # Dati match\n",
        "              pred_match = K.gather(y_pred, match)\n",
        "              # Dati non match\n",
        "              pred_not_match = K.gather(y_pred, not_match) \n",
        "\n",
        "              loss_m = K.mean(keras.losses.mean_squared_error(data_match, pred_match)) + 1e-6  # Calcolo Loss Match\n",
        "              loss_nm = K.mean(keras.losses.mean_squared_error(C,pred_not_match)) + 1e-6     # Calcolo Loss Non_Match\n",
        "\n",
        "              loss = ALPHA * loss_m + (1 - ALPHA) * loss_nm     # loss utilizzata per l'update dei pesi\n",
        "\n",
        "          # Compute gradients\n",
        "          trainable_vars = self.trainable_variables\n",
        "          gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "          # Update weights\n",
        "          self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "          # Compute our own metrics\n",
        "          loss_tracker.update_state(loss)\n",
        "          mse_metric.update_state(y, y_pred)\n",
        "          return {\"loss\": loss_tracker.result(), \"mse\": mse_metric.result()}\n",
        "\n",
        "\n",
        "autoencoder = CustomModel(inputs=(input_img, input_Label), outputs = decoded_reshaped)\n",
        "\n",
        "def get_lr_metric(optimizer):\n",
        "    def lr(y_true, y_pred):\n",
        "        return optimizer._decayed_lr(tf.float32) # I use ._decayed_lr method instead of .lr\n",
        "    return lr\n",
        "\n",
        "opt = keras.optimizers.Adam(\n",
        "    learning_rate = 0.0001,\n",
        "    beta_1=0.95,\n",
        "    beta_2=0.999\n",
        ")\n",
        "\n",
        "lr_metric = get_lr_metric(opt)\n",
        "autoencoder.compile(optimizer = opt, metrics=[\"mse\", lr_metric])\n",
        "autoencoder.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"custom_model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 128, 32, 1)] 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 64, 32, 32)   832         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 64, 32, 32)   128         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 64, 32, 32)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 64)   51264       activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 64)   256         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 64)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 16, 16, 128)  204928      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 16, 16, 128)  512         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 16, 16, 128)  0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 8, 8, 256)    295168      activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 8, 8, 256)    1024        conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 8, 8, 256)    0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 4, 4, 512)    1180160     activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 4, 4, 512)    2048        conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 4)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 4, 4, 512)    0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 40)           200         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 1, 1, 40)     327720      activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 40)           0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 40)           0           conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 40)           200         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "multiply (Multiply)             (None, 40)           0           activation_5[0][0]               \n",
            "                                                                 flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 40)           0           dense_1[0][0]                    \n",
            "                                                                 multiply[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 8192)         335872      add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 4, 4, 512)    0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose (Conv2DTranspo (None, 8, 8, 256)    1179904     reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 8, 8, 256)    1024        conv2d_transpose[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 8, 8, 256)    0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTrans (None, 16, 16, 128)  295040      activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 16, 16, 128)  512         conv2d_transpose_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 16, 16, 128)  0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTrans (None, 32, 32, 64)   204864      activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 64)   256         conv2d_transpose_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 32, 32, 64)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTrans (None, 64, 32, 32)   51232       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 64, 32, 32)   128         conv2d_transpose_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 64, 32, 32)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_4 (Conv2DTrans (None, 128, 32, 1)   801         activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 1, 128, 32)   0           conv2d_transpose_4[0][0]         \n",
            "==================================================================================================\n",
            "Total params: 4,134,073\n",
            "Trainable params: 4,131,129\n",
            "Non-trainable params: 2,944\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "tCHx_rpTe5JM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a8c1345-524c-4528-d56c-132510d39489"
      },
      "source": [
        "history = autoencoder.fit([training_shuffle, training_labels_shuffle, training_choices_shuffle], \n",
        "                          training_shuffle, \n",
        "                          epochs=100,\n",
        "                          batch_size=256, \n",
        "                          validation_data=([validation_shuffle, validation_labels_shuffle, validation_choices_shuffle], validation_shuffle), shuffle=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "177/177 [==============================] - 88s 304ms/step - loss: 4.3913 - mse: 2.3492 - val_mse: 27.1225\n",
            "Epoch 2/100\n",
            "177/177 [==============================] - 53s 297ms/step - loss: 0.8303 - mse: 5.9729 - val_mse: 22.0857\n",
            "Epoch 3/100\n",
            "177/177 [==============================] - 52s 296ms/step - loss: 0.3263 - mse: 6.5489 - val_mse: 10.0775\n",
            "Epoch 4/100\n",
            "177/177 [==============================] - 52s 296ms/step - loss: 0.2898 - mse: 6.5841 - val_mse: 0.4217\n",
            "Epoch 5/100\n",
            "177/177 [==============================] - 52s 295ms/step - loss: 0.2809 - mse: 6.5882 - val_mse: 0.3532\n",
            "Epoch 6/100\n",
            "177/177 [==============================] - 52s 296ms/step - loss: 0.2783 - mse: 6.5975 - val_mse: 0.3528\n",
            "Epoch 7/100\n",
            "177/177 [==============================] - 52s 296ms/step - loss: 0.2712 - mse: 6.6048 - val_mse: 0.3465\n",
            "Epoch 8/100\n",
            "177/177 [==============================] - 52s 296ms/step - loss: 0.2666 - mse: 6.6152 - val_mse: 0.3579\n",
            "Epoch 9/100\n",
            "177/177 [==============================] - 52s 295ms/step - loss: 0.2608 - mse: 6.6292 - val_mse: 0.3521\n",
            "Epoch 10/100\n",
            "177/177 [==============================] - 52s 295ms/step - loss: 0.2637 - mse: 6.6165 - val_mse: 0.3536\n",
            "Epoch 11/100\n",
            "177/177 [==============================] - 52s 296ms/step - loss: 0.2611 - mse: 6.6173 - val_mse: 0.3601\n",
            "Epoch 12/100\n",
            "177/177 [==============================] - 52s 295ms/step - loss: 0.2570 - mse: 6.6516 - val_mse: 0.3621\n",
            "Epoch 13/100\n",
            "177/177 [==============================] - 52s 295ms/step - loss: 0.2554 - mse: 6.6532 - val_mse: 0.3531\n",
            "Epoch 14/100\n",
            "177/177 [==============================] - 52s 296ms/step - loss: 0.2574 - mse: 6.6488 - val_mse: 0.3460\n",
            "Epoch 15/100\n",
            "177/177 [==============================] - 52s 295ms/step - loss: 0.3274 - mse: 6.5716 - val_mse: 0.6033\n",
            "Epoch 16/100\n",
            "177/177 [==============================] - 52s 296ms/step - loss: 0.3185 - mse: 6.5826 - val_mse: 0.3810\n",
            "Epoch 17/100\n",
            "177/177 [==============================] - 52s 295ms/step - loss: 0.2703 - mse: 6.6426 - val_mse: 0.3614\n",
            "Epoch 18/100\n",
            "177/177 [==============================] - 52s 295ms/step - loss: 0.2564 - mse: 6.6588 - val_mse: 0.3631\n",
            "Epoch 19/100\n",
            "177/177 [==============================] - 52s 295ms/step - loss: 0.2549 - mse: 6.6636 - val_mse: 0.3446\n",
            "Epoch 20/100\n",
            "177/177 [==============================] - 52s 296ms/step - loss: 0.2522 - mse: 6.6739 - val_mse: 0.3508\n",
            "Epoch 21/100\n",
            "177/177 [==============================] - 52s 296ms/step - loss: 0.2550 - mse: 6.6724 - val_mse: 0.3519\n",
            "Epoch 22/100\n",
            "177/177 [==============================] - 52s 295ms/step - loss: 0.2518 - mse: 6.6746 - val_mse: 0.3422\n",
            "Epoch 23/100\n",
            "177/177 [==============================] - 52s 295ms/step - loss: 0.2511 - mse: 6.6780 - val_mse: 0.3402\n",
            "Epoch 24/100\n",
            "177/177 [==============================] - 52s 295ms/step - loss: 0.2510 - mse: 6.6789 - val_mse: 0.3408\n",
            "Epoch 25/100\n",
            "177/177 [==============================] - 52s 295ms/step - loss: 0.2505 - mse: 6.6842 - val_mse: 0.3419\n",
            "Epoch 26/100\n",
            "177/177 [==============================] - 52s 295ms/step - loss: 0.2504 - mse: 6.6820 - val_mse: 0.3399\n",
            "Epoch 27/100\n",
            "177/177 [==============================] - 52s 295ms/step - loss: 0.2505 - mse: 6.6827 - val_mse: 0.3402\n",
            "Epoch 28/100\n",
            "177/177 [==============================] - 52s 295ms/step - loss: 0.2503 - mse: 6.6794 - val_mse: 0.3405\n",
            "Epoch 29/100\n",
            "177/177 [==============================] - 52s 295ms/step - loss: 0.2501 - mse: 6.6833 - val_mse: 0.3425\n",
            "Epoch 30/100\n",
            "177/177 [==============================] - 52s 295ms/step - loss: 0.2503 - mse: 6.6853 - val_mse: 0.3407\n",
            "Epoch 31/100\n",
            "177/177 [==============================] - 52s 295ms/step - loss: 0.2500 - mse: 6.6833 - val_mse: 0.3422\n",
            "Epoch 32/100\n",
            "177/177 [==============================] - 52s 296ms/step - loss: 0.2502 - mse: 6.6870 - val_mse: 0.3411\n",
            "Epoch 33/100\n",
            "177/177 [==============================] - 52s 296ms/step - loss: 0.2501 - mse: 6.6820 - val_mse: 0.3405\n",
            "Epoch 34/100\n",
            "177/177 [==============================] - 52s 296ms/step - loss: 0.2500 - mse: 6.6830 - val_mse: 0.3422\n",
            "Epoch 35/100\n",
            "177/177 [==============================] - 52s 296ms/step - loss: 0.2500 - mse: 6.6833 - val_mse: 0.3438\n",
            "Epoch 36/100\n",
            "177/177 [==============================] - 52s 295ms/step - loss: 0.2499 - mse: 6.6837 - val_mse: 0.3406\n",
            "Epoch 37/100\n",
            "177/177 [==============================] - 52s 295ms/step - loss: 0.2499 - mse: 6.6863 - val_mse: 0.3410\n",
            "Epoch 38/100\n",
            "177/177 [==============================] - 52s 295ms/step - loss: 0.2498 - mse: 6.6863 - val_mse: 0.3407\n",
            "Epoch 39/100\n",
            "177/177 [==============================] - 52s 295ms/step - loss: 0.2500 - mse: 6.6848 - val_mse: 0.3422\n",
            "Epoch 40/100\n",
            "177/177 [==============================] - 52s 295ms/step - loss: 0.2497 - mse: 6.6855 - val_mse: 0.3408\n",
            "Epoch 41/100\n",
            "177/177 [==============================] - 52s 296ms/step - loss: 0.2497 - mse: 6.6844 - val_mse: 0.3400\n",
            "Epoch 42/100\n",
            "177/177 [==============================] - 52s 295ms/step - loss: 0.2498 - mse: 6.6854 - val_mse: 0.3410\n",
            "Epoch 43/100\n",
            "177/177 [==============================] - 52s 295ms/step - loss: 0.2503 - mse: 6.6847 - val_mse: 0.3405\n",
            "Epoch 44/100\n",
            "177/177 [==============================] - 52s 295ms/step - loss: 0.2499 - mse: 6.6844 - val_mse: 0.3406\n",
            "Epoch 45/100\n",
            "177/177 [==============================] - 52s 295ms/step - loss: 0.2500 - mse: 6.6847 - val_mse: 0.3478\n",
            "Epoch 46/100\n",
            "177/177 [==============================] - 52s 295ms/step - loss: 0.3111 - mse: 6.6177 - val_mse: 0.3888\n",
            "Epoch 47/100\n",
            "177/177 [==============================] - 52s 295ms/step - loss: 0.2771 - mse: 6.6441 - val_mse: 0.4495\n",
            "Epoch 48/100\n",
            "177/177 [==============================] - 52s 295ms/step - loss: 0.2584 - mse: 6.6749 - val_mse: 0.4028\n",
            "Epoch 49/100\n",
            "177/177 [==============================] - 52s 295ms/step - loss: 0.2574 - mse: 6.6751 - val_mse: 0.3452\n",
            "Epoch 50/100\n",
            "177/177 [==============================] - 52s 295ms/step - loss: 0.2503 - mse: 6.6838 - val_mse: 0.3404\n",
            "Epoch 51/100\n",
            "177/177 [==============================] - 52s 295ms/step - loss: 0.2561 - mse: 6.6776 - val_mse: 0.3459\n",
            "Epoch 52/100\n",
            "177/177 [==============================] - 52s 295ms/step - loss: 0.2527 - mse: 6.6756 - val_mse: 0.3959\n",
            "Epoch 53/100\n",
            "177/177 [==============================] - 52s 296ms/step - loss: 0.2500 - mse: 6.6842 - val_mse: 0.3450\n",
            "Epoch 54/100\n",
            "177/177 [==============================] - 52s 296ms/step - loss: 0.2549 - mse: 6.6792 - val_mse: 0.4543\n",
            "Epoch 55/100\n",
            "177/177 [==============================] - 52s 296ms/step - loss: 0.2657 - mse: 6.6627 - val_mse: 0.3592\n",
            "Epoch 56/100\n",
            "177/177 [==============================] - 52s 296ms/step - loss: 0.2523 - mse: 6.6830 - val_mse: 0.3521\n",
            "Epoch 57/100\n",
            "177/177 [==============================] - 52s 296ms/step - loss: 0.2497 - mse: 6.6882 - val_mse: 0.3402\n",
            "Epoch 58/100\n",
            "177/177 [==============================] - 52s 296ms/step - loss: 0.2498 - mse: 6.6859 - val_mse: 0.3383\n",
            "Epoch 59/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2496 - mse: 6.6822 - val_mse: 0.3397\n",
            "Epoch 60/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2497 - mse: 6.6860 - val_mse: 0.3412\n",
            "Epoch 61/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2496 - mse: 6.6864 - val_mse: 0.3402\n",
            "Epoch 62/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2496 - mse: 6.6844 - val_mse: 0.3379\n",
            "Epoch 63/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2495 - mse: 6.6842 - val_mse: 0.3403\n",
            "Epoch 64/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2497 - mse: 6.6824 - val_mse: 0.3397\n",
            "Epoch 65/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2495 - mse: 6.6858 - val_mse: 0.3389\n",
            "Epoch 66/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2496 - mse: 6.6843 - val_mse: 0.3401\n",
            "Epoch 67/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2495 - mse: 6.6847 - val_mse: 0.3405\n",
            "Epoch 68/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2496 - mse: 6.6881 - val_mse: 0.3407\n",
            "Epoch 69/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2496 - mse: 6.6863 - val_mse: 0.3382\n",
            "Epoch 70/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2495 - mse: 6.6842 - val_mse: 0.3415\n",
            "Epoch 71/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2495 - mse: 6.6841 - val_mse: 0.3397\n",
            "Epoch 72/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2497 - mse: 6.6846 - val_mse: 0.3410\n",
            "Epoch 73/100\n",
            "177/177 [==============================] - 52s 293ms/step - loss: 0.2495 - mse: 6.6859 - val_mse: 0.3396\n",
            "Epoch 74/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2495 - mse: 6.6838 - val_mse: 0.3402\n",
            "Epoch 75/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2495 - mse: 6.6858 - val_mse: 0.3404\n",
            "Epoch 76/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2495 - mse: 6.6843 - val_mse: 0.3403\n",
            "Epoch 77/100\n",
            "177/177 [==============================] - 52s 295ms/step - loss: 0.2495 - mse: 6.6848 - val_mse: 0.3383\n",
            "Epoch 78/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2495 - mse: 6.6876 - val_mse: 0.3398\n",
            "Epoch 79/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2493 - mse: 6.6820 - val_mse: 0.3382\n",
            "Epoch 80/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2495 - mse: 6.6868 - val_mse: 0.3395\n",
            "Epoch 81/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2494 - mse: 6.6871 - val_mse: 0.3415\n",
            "Epoch 82/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2494 - mse: 6.6856 - val_mse: 0.3418\n",
            "Epoch 83/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2495 - mse: 6.6854 - val_mse: 0.3382\n",
            "Epoch 84/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2495 - mse: 6.6862 - val_mse: 0.3371\n",
            "Epoch 85/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2495 - mse: 6.6861 - val_mse: 0.3396\n",
            "Epoch 86/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2494 - mse: 6.6879 - val_mse: 0.3394\n",
            "Epoch 87/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2495 - mse: 6.6860 - val_mse: 0.3418\n",
            "Epoch 88/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2494 - mse: 6.6871 - val_mse: 0.3390\n",
            "Epoch 89/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2495 - mse: 6.6857 - val_mse: 0.3400\n",
            "Epoch 90/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2495 - mse: 6.6881 - val_mse: 0.3385\n",
            "Epoch 91/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2496 - mse: 6.6834 - val_mse: 0.3384\n",
            "Epoch 92/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2524 - mse: 6.6754 - val_mse: 0.5040\n",
            "Epoch 93/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2943 - mse: 6.6380 - val_mse: 0.5627\n",
            "Epoch 94/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2549 - mse: 6.6768 - val_mse: 0.3355\n",
            "Epoch 95/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2534 - mse: 6.6815 - val_mse: 0.4685\n",
            "Epoch 96/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2533 - mse: 6.6759 - val_mse: 0.3448\n",
            "Epoch 97/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2502 - mse: 6.6859 - val_mse: 0.3552\n",
            "Epoch 98/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2499 - mse: 6.6854 - val_mse: 0.3360\n",
            "Epoch 99/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2494 - mse: 6.6876 - val_mse: 0.3371\n",
            "Epoch 100/100\n",
            "177/177 [==============================] - 52s 294ms/step - loss: 0.2494 - mse: 6.6853 - val_mse: 0.3414\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "xVgrPAV2e5JM"
      },
      "source": [
        "# Salvataggio del modello\n",
        "autoencoder.save('/content/drive/MyDrive/models/IDCCAE/pump/2/model_pump.h5')\n",
        "\n",
        "# Salvataggio history di apprendimento\n",
        "with open('/content/drive/MyDrive/models/IDCCAE/pump/2/trainHistoryDict', 'wb') as file_pi:\n",
        "    pickle.dump(history.history, file_pi)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Llr3vaUZe5JN"
      },
      "source": [
        "# TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "MZXcYCCae5JN"
      },
      "source": [
        "import csv\n",
        "\n",
        "def save_csv(save_file_path,\n",
        "             save_data):\n",
        "    with open(save_file_path, \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f, lineterminator='\\n')\n",
        "        writer.writerows(save_data)\n",
        "\n",
        "\n",
        "# load dataset\n",
        "def select_dirs(path):\n",
        "    dir_path = os.path.abspath(path)\n",
        "    dirs = sorted(glob.glob(dir_path))\n",
        "    return dirs\n",
        "\n",
        "def file_load(wav_name, mono=False):\n",
        "    try:\n",
        "        return librosa.load(wav_name, sr=None, mono=mono)\n",
        "    except:\n",
        "        logger.error(\"file_broken or not exists!! : {}\".format(wav_name))\n",
        "\n",
        "def file_list_generator(target_dir, dir_name=\"train\", ext=\"wav\"):\n",
        "    print(\"target_dir : {}\".format(target_dir))\n",
        "\n",
        "    # generate training list\n",
        "    training_list_path = os.path.abspath(\"{dir}/{dir_name}/*.{ext}\".format(dir=target_dir, dir_name=dir_name, ext=ext))\n",
        "    files = sorted(glob.glob(training_list_path))\n",
        "    if len(files) == 0:\n",
        "      print(\"errore\")\n",
        "    return files\n",
        "\n",
        "\n",
        "def file_to_vector_array(file_name, n_mels=64, n_fft=1024, hop_length=512, power=2.0):\n",
        "    # 02 generate melspectrogram using librosa\n",
        "    y, sr = file_load(file_name)\n",
        "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, power=power)\n",
        "\n",
        "    # 03 convert melspectrogram to log mel energy\n",
        "    log_mel_spectrogram = 20.0 / power * numpy.log10(mel_spectrogram + sys.float_info.epsilon)\n",
        "\n",
        "    return log_mel_spectrogram\n",
        "\n",
        "  \n",
        "def list_to_vector_array(file_list, msg=\"calc...\", n_mels=64, n_fft=1024, hop_length=512, power=2.0, frames=10):\n",
        "    # iterate file_to_vector_array()\n",
        "    for idx in tqdm(range(len(file_list)), desc=msg):\n",
        "        vector_array = file_to_vector_array(file_list[idx], n_mels=n_mels, n_fft=n_fft, hop_length=hop_length, power=power)\n",
        "\n",
        "        if idx == 0:\n",
        "            dataset = numpy.zeros((len(file_list), n_mels, frames), float)\n",
        "        dataset[vector_array.shape[0] * idx: vector_array.shape[0] * (idx + 1), :] = vector_array\n",
        "    return dataset\n",
        "\n",
        "def key_by_id(item):\n",
        "  path_splitted = item.split(\"/\")\n",
        "  file_name = path_splitted[ len(path_splitted) - 1 ]\n",
        "  file_name_splitted = file_name.split(\"_\")\n",
        "  machine_id = file_name_splitted = file_name_splitted[2]\n",
        "  return machine_id\n",
        "\n",
        "def get_machine_id_list_for_test(target_dir,\n",
        "                                 dir_name=\"test\",\n",
        "                                 ext=\"wav\"):\n",
        "\n",
        "    # create test files\n",
        "    dir_path = os.path.abspath(\"{dir}/{dir_name}/*.{ext}\".format(dir=target_dir, dir_name=dir_name, ext=ext))\n",
        "    file_paths = sorted(glob.glob(dir_path))\n",
        "    # extract id\n",
        "    machine_id_list = sorted(list(set(itertools.chain.from_iterable(\n",
        "        [re.findall('id_[0-9][0-9]', ext_id) for ext_id in file_paths]))))\n",
        "    return machine_id_list\n",
        "\n",
        "def test_file_list_generator(target_dir,\n",
        "                             id_name,\n",
        "                             dir_name=\"test\",\n",
        "                             prefix_normal=\"normal\",\n",
        "                             prefix_anomaly=\"anomaly\",\n",
        "                             ext=\"wav\"):\n",
        "  \n",
        "    print(\"target_dir : {}\".format(target_dir+\"_\"+id_name))\n",
        "\n",
        "    normal_files = sorted(\n",
        "    glob.glob(\"{dir}/{dir_name}/{prefix_normal}_{id_name}*.{ext}\".format(dir=target_dir,\n",
        "                                                                                 dir_name=dir_name,\n",
        "                                                                                 prefix_normal=prefix_normal,\n",
        "                                                                                 id_name=id_name,\n",
        "                                                                                 ext=ext)))\n",
        "    normal_labels = numpy.zeros(len(normal_files))\n",
        "    anomaly_files = sorted(\n",
        "    glob.glob(\"{dir}/{dir_name}/{prefix_anomaly}_{id_name}*.{ext}\".format(dir=target_dir,\n",
        "                                                                                  dir_name=dir_name,\n",
        "                                                                                  prefix_anomaly=prefix_anomaly,\n",
        "                                                                                  id_name=id_name,\n",
        "                                                                                  ext=ext)))\n",
        "    anomaly_labels = numpy.ones(len(anomaly_files))\n",
        "    files = numpy.concatenate((normal_files, anomaly_files), axis=0)\n",
        "    labels = numpy.concatenate((normal_labels, anomaly_labels), axis=0)\n",
        "    print(\"test_file  num : {num}\".format(num=len(files)))\n",
        "    if len(files) == 0:\n",
        "        print(\"no_wav_file!!\")\n",
        "    print(\"\\n========================================\")\n",
        "\n",
        "    return files, labels"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "c6d20c4fe5JO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8e2f28e-e2b3-4cda-f58e-021482a87c81"
      },
      "source": [
        "target_dir = \"/content/drive/MyDrive/test/pump\"\n",
        "\n",
        "machine_type = os.path.split(target_dir)[1]\n",
        "print(\"============== MODEL LOAD ==============\")\n",
        "# set model path\n",
        "model_file = \"/content/drive/MyDrive/models/IDCCAE/pump/2/model_pump.h5\"\n",
        "\n",
        "# load model file\n",
        "if not os.path.exists(model_file):\n",
        "  print(\"{} model not found \".format(machine_type))\n",
        "  sys.exit(-1)\n",
        "model = keras.models.load_model(model_file, custom_objects={'CustomModel': CustomModel, 'mse':mse_metric, 'lr': lr_metric})\n",
        "# model.summary()\n",
        "\n",
        "machine_id_list = get_machine_id_list_for_test(target_dir)\n",
        "\n",
        "# initialize lines in csv for AUC and pAUC\n",
        "csv_lines = []\n",
        "\n",
        "csv_lines.append([machine_type])\n",
        "csv_lines.append([\"id\", \"AUC\", \"pAUC\"])\n",
        "performance = []\n",
        "\n",
        "for id_str in machine_id_list:\n",
        "  # load test file\n",
        "\n",
        "  id_num = id_str.split(\"_\")[1]\n",
        "\n",
        "  # Definizione della label \"match\" da utilizzare in fase di testing e del min e max da utilizzare per la normalizzazione\n",
        "  # i min e max sono stati calcolati a partire dai dati di training.\n",
        "  if id_num == \"00\":\n",
        "    match_labels = numpy.asarray([1,0,0,0])\n",
        "    mean = mean_00\n",
        "    std = std_00\n",
        "  if id_num == \"02\":\n",
        "    match_labels = numpy.asarray([0,1,0,0])\n",
        "    mean = mean_02\n",
        "    std = std_02\n",
        "  if id_num == \"04\":\n",
        "    match_labels = numpy.asarray([0,0,1,0])\n",
        "    mean = mean_04\n",
        "    std = std_04\n",
        "  if id_num == \"06\":\n",
        "    match_labels = numpy.asarray([0,0,0,1])\n",
        "    mean = mean_06\n",
        "    std = std_06\n",
        "\n",
        "  test_files, y_true = test_file_list_generator(target_dir, id_str)\n",
        "  #print(\"\\n====== True Labels ======\")\n",
        "  #print(y_true)\n",
        "  #print(\"==> ====== Match ID Labels ======\")\n",
        "  #print(match_labels.shape)\n",
        "  #print(\"=================================\\n\")\n",
        "\n",
        "  # setup anomaly score file path\n",
        "  anomaly_score_csv = \"/content/drive/MyDrive/models/IDCCAE/pump/2/anomaly_score_{machine_type}_{id_str}.csv\".format(machine_type=machine_type, id_str=id_str)\n",
        "  anomaly_score_list = []\n",
        "\n",
        "  print(\"\\n============== BEGIN TEST FOR A MACHINE ID {id} ==============\".format(id=id_num))\n",
        "\n",
        "  y_pred = [0. for k in test_files]\n",
        "\n",
        "\n",
        "  for file_idx, file_path in tqdm(enumerate(test_files), total=len(test_files)):\n",
        "\n",
        "    # Estrazione spettrogramma audio test\n",
        "    data = file_to_vector_array(file_path, n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH, power=POWER)\n",
        "\n",
        "    # Normalizzazione spettrogramma di test\n",
        "    data = ( data - mean ) / std\n",
        "\n",
        "    # Estrazione delle frame 128x32\n",
        "    data_splitted = numpy.zeros((15, 128, 32))\n",
        "    index = 0\n",
        "    i = 0\n",
        "    while (i+32) <= 313:\n",
        "      vector_i = numpy.zeros((128,32))\n",
        "      for j in range(0,128):\n",
        "        vector_i[j] = data[j][i:i+32]\n",
        "      data_splitted[index] = vector_i\n",
        "      index += 1\n",
        "      i = i+20\n",
        "\n",
        "    # Calcolo dell'errore medio sulle frame estratte dallo spettrogramma\n",
        "    elem_error = []\n",
        "    for elem in data_splitted:\n",
        "      predicted = model.predict([elem.reshape(1,128,32), match_labels.reshape((1,4))])\n",
        "      errors = numpy.mean(numpy.square(elem - predicted.reshape(1,128,32)), axis=1)\n",
        "      elem_error.append(numpy.mean(errors))\n",
        "\n",
        "    # Log dell'errore associato all'istanza di test\n",
        "    y_pred[file_idx] = numpy.mean(elem_error)\n",
        "    anomaly_score_list.append([os.path.basename(file_path), y_pred[file_idx]])\n",
        "  \n",
        "\n",
        "  save_csv(save_file_path=anomaly_score_csv, save_data=anomaly_score_list)\n",
        "    \n",
        "  # Calcolo AUC e pAUC per i dati con un certo ID_0x\n",
        "  auc = metrics.roc_auc_score(y_true,y_pred)\n",
        "  p_auc = metrics.roc_auc_score(y_true, y_pred, max_fpr=0.1)\n",
        "  csv_lines.append([id_str.split(\"_\", 1)[1], auc, p_auc])\n",
        "  performance.append([auc, p_auc])\n",
        "  print(\"AUC : {}\".format(auc))\n",
        "  print(\"pAUC : {}\".format(p_auc))\n",
        "\n",
        "  print(\"\\n============ END OF TEST FOR A MACHINE ID ============\")\n",
        "\n",
        "# Stampa di AUC e pAUC medi su tutti i dati di test (media di AUC e pAUC sui vari ID).\n",
        "print(\"\\n============ AVERAGE PERFORMANCES ============\")\n",
        "averaged_performance = numpy.mean(numpy.array(performance, dtype=float), axis=0)\n",
        "csv_lines.append([\"Average\"] + list(averaged_performance))\n",
        "csv_lines.append([])\n",
        "print(averaged_performance)\n",
        "\n",
        "result_path = \"/content/drive/MyDrive/models/IDCCAE/pump/2/anomaly_score_avg_pump.csv\"\n",
        "save_csv(save_file_path=result_path, save_data=csv_lines)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============== MODEL LOAD ==============\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/243 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "target_dir : /content/drive/MyDrive/test/pump_id_00\n",
            "test_file  num : 243\n",
            "\n",
            "========================================\n",
            "\n",
            "============== BEGIN TEST FOR A MACHINE ID 00 ==============\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 243/243 [04:14<00:00,  1.05s/it]\n",
            "  0%|          | 0/211 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AUC : 0.7787412587412588\n",
            "pAUC : 0.5590725064409275\n",
            "\n",
            "============ END OF TEST FOR A MACHINE ID ============\n",
            "target_dir : /content/drive/MyDrive/test/pump_id_02\n",
            "test_file  num : 211\n",
            "\n",
            "========================================\n",
            "\n",
            "============== BEGIN TEST FOR A MACHINE ID 02 ==============\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 211/211 [03:36<00:00,  1.03s/it]\n",
            "  0%|          | 0/200 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AUC : 0.7471171171171171\n",
            "pAUC : 0.7088667614983404\n",
            "\n",
            "============ END OF TEST FOR A MACHINE ID ============\n",
            "target_dir : /content/drive/MyDrive/test/pump_id_04\n",
            "test_file  num : 200\n",
            "\n",
            "========================================\n",
            "\n",
            "============== BEGIN TEST FOR A MACHINE ID 04 ==============\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [03:43<00:00,  1.12s/it]\n",
            "  0%|          | 0/202 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AUC : 0.9963\n",
            "pAUC : 0.9942105263157895\n",
            "\n",
            "============ END OF TEST FOR A MACHINE ID ============\n",
            "target_dir : /content/drive/MyDrive/test/pump_id_06\n",
            "test_file  num : 202\n",
            "\n",
            "========================================\n",
            "\n",
            "============== BEGIN TEST FOR A MACHINE ID 06 ==============\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 202/202 [03:35<00:00,  1.07s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AUC : 0.5823529411764706\n",
            "pAUC : 0.5103199174406605\n",
            "\n",
            "============ END OF TEST FOR A MACHINE ID ============\n",
            "\n",
            "============ AVERAGE PERFORMANCES ============\n",
            "[0.77612783 0.69311743]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}