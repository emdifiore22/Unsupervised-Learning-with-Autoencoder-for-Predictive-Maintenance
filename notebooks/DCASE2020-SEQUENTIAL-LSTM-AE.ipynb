{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCASE2020-SEQUENTIAL-LSTM-AE.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9uFgr4mYC5v"
      },
      "source": [
        "# import necessari\n",
        "import librosa\n",
        "import numpy\n",
        "import sys\n",
        "import os\n",
        "import glob\n",
        "import itertools\n",
        "import re\n",
        "import pickle\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.models\n",
        "import tensorflow.keras.backend as K\n",
        "import keras.optimizers\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, BatchNormalization, Activation, Flatten, Multiply, Add, Reshape\n",
        "from tqdm import tqdm\n",
        "from itertools import groupby\n",
        "from keras.utils import to_categorical\n",
        "from sklearn import metrics\n",
        "\n",
        "# costanti \n",
        "ALPHA = 0.75\n",
        "N_MELS = 128\n",
        "HOP_LENGTH = 512\n",
        "N_FFT = 1024\n",
        "POWER = 2.0\n",
        "FRAME_NUMS = 313\n",
        "NUM_FILES = 3349\n",
        "FRAMES = 10\n",
        "\n",
        "# FEATURES EXTRACTION\n",
        "\n",
        "# Loading da Google Drive\n",
        "train_data = numpy.load(\"/content/drive/MyDrive/DCASE_DATA_EXTRACTED/train/training_pump.npy\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJf6dHTQYjsN"
      },
      "source": [
        "# Estrazione spettrogrammi divisi per ID\n",
        "id_00 = train_data[0:906]\n",
        "id_02 = train_data[906:1811]\n",
        "id_04 = train_data[1811:2413]\n",
        "id_06 = train_data[2413:3349]\n",
        "\n",
        "# Z-Score Normalization ID_00\n",
        "id_00_norm = numpy.empty_like(id_00)\n",
        "max_00 = numpy.max(id_00)\n",
        "min_00 = numpy.min(id_00)\n",
        "id_00_norm = (id_00 - min_00) / (max_00 - min_00)\n",
        "\n",
        "# Z-Score Normalization ID_02\n",
        "id_02_norm = numpy.empty_like(id_02)\n",
        "max_02 = numpy.max(id_02)\n",
        "min_02 = numpy.min(id_02)\n",
        "id_02_norm = (id_02 - min_02) / (max_02 - min_02)\n",
        "\n",
        "# Z-Score Normalization ID_04\n",
        "id_04_norm = numpy.empty_like(id_04)\n",
        "max_04 = numpy.max(id_04)\n",
        "min_04 = numpy.min(id_04)\n",
        "id_04_norm = (id_04 - max_04) / (max_02 - min_02)\n",
        "\n",
        "# Z-Score Normalization ID_06\n",
        "id_06_norm = numpy.empty_like(id_06)\n",
        "max_06 = numpy.max(id_06)\n",
        "min_06 = numpy.min(id_06)\n",
        "id_06_norm = (id_06 - max_06) / (max_06 - min_06)\n",
        "'''\n",
        "print(\"Mean: {m}\".format(m=mean_00))\n",
        "print(\"Dev.Std: {d}\".format(d=std_00))\n",
        "print(id_00_norm.shape)\n",
        "\n",
        "print(\"Mean: {m}\".format(m=mean_02))\n",
        "print(\"Dev.Std: {d}\".format(d=std_02))\n",
        "print(id_02_norm.shape)\n",
        "\n",
        "print(\"Mean: {m}\".format(m=mean_04))\n",
        "print(\"Dev.Std: {d}\".format(d=std_04))\n",
        "print(id_04_norm.shape)\n",
        "\n",
        "print(\"Mean: {m}\".format(m=mean_06))\n",
        "print(\"Dev.Std: {d}\".format(d=std_06))\n",
        "print(id_06_norm.shape)\n",
        "'''\n",
        "train_data_norm = numpy.concatenate([id_00_norm, id_02_norm, id_04_norm, id_06_norm])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PE3BY_KZKji"
      },
      "source": [
        "# Estrazione frame 128x10 da ciascun spettrogramma\n",
        "training_splitted = numpy.zeros((len(train_data_norm)*30, 128, 10)) # Dataset utilizzato per il training\n",
        "index = 0\n",
        "for vector_array in train_data_norm:\n",
        "  i = 0\n",
        "  while (i+5) < 303:\n",
        "    vector_i = numpy.zeros((128,10))\n",
        "    for j in range(0,128):\n",
        "      vector_i[j] = vector_array[j][i:i+10]\n",
        "    training_splitted[index] = vector_i\n",
        "    index += 1\n",
        "    i = i+10"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ew7oO2pAjWyV",
        "outputId": "6da65bd3-fa89-4cf6-8b80-0ec97f32c77a"
      },
      "source": [
        "training_splitted.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200940, 128, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0gCnCy-Za9J",
        "outputId": "51db8eaa-977b-475e-81ee-dad75f871556"
      },
      "source": [
        "training = numpy.zeros((len(training_splitted), 10, 128))\n",
        "index = 0\n",
        "for elem in training_splitted:\n",
        "  training[index] = elem.T\n",
        "  index += 1\n",
        "training.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100470, 10, 128)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfQ27Q2Rue5F",
        "outputId": "008d47e1-c73b-4810-e3fd-72e554d33000"
      },
      "source": [
        "# Shuffling\n",
        "randomize = numpy.arange(len(training))\n",
        "numpy.random.shuffle(randomize)\n",
        "training_tot_shuffle = training[randomize]\n",
        "print(training_tot_shuffle.shape)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100470, 10, 128)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISWY6OjXbZNd",
        "outputId": "b074306f-b112-4c8e-9255-3fa35367f735"
      },
      "source": [
        "timesteps = 10\n",
        "num_features = 128\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.LSTM(64, kernel_initializer='he_uniform', batch_input_shape=(None, timesteps, num_features), return_sequences=True, name='encoder_1'))\n",
        "model.add(keras.layers.LSTM(32, kernel_initializer='he_uniform', return_sequences=True, name='encoder_3'))\n",
        "model.add(keras.layers.LSTM(16, kernel_initializer='he_uniform', return_sequences=False, name='encoder_4'))\n",
        "model.add(keras.layers.RepeatVector(timesteps, name='encoder_decoder_bridge'))\n",
        "model.add(keras.layers.LSTM(16, kernel_initializer='he_uniform', return_sequences=True, name='decoder_1'))\n",
        "model.add(keras.layers.LSTM(32, kernel_initializer='he_uniform', return_sequences=True, name='decoder_2'))\n",
        "model.add(keras.layers.LSTM(64, kernel_initializer='he_uniform', return_sequences=True, name='decoder_3'))\n",
        "model.add(keras.layers.TimeDistributed(keras.layers.Dense(num_features)))\n",
        "model.summary()\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "encoder_1 (LSTM)             (None, 10, 64)            49408     \n",
            "_________________________________________________________________\n",
            "encoder_3 (LSTM)             (None, 10, 32)            12416     \n",
            "_________________________________________________________________\n",
            "encoder_4 (LSTM)             (None, 16)                3136      \n",
            "_________________________________________________________________\n",
            "encoder_decoder_bridge (Repe (None, 10, 16)            0         \n",
            "_________________________________________________________________\n",
            "decoder_1 (LSTM)             (None, 10, 16)            2112      \n",
            "_________________________________________________________________\n",
            "decoder_2 (LSTM)             (None, 10, 32)            6272      \n",
            "_________________________________________________________________\n",
            "decoder_3 (LSTM)             (None, 10, 64)            24832     \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 10, 128)           8320      \n",
            "=================================================================\n",
            "Total params: 106,496\n",
            "Trainable params: 106,496\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbvCY_H6hDZA",
        "outputId": "42bcb351-df4e-44db-d418-1645e3485c21"
      },
      "source": [
        "opt = keras.optimizers.Adam(\n",
        "    learning_rate = 0.0001,\n",
        "    beta_1=0.95,\n",
        "    beta_2=0.999\n",
        ")\n",
        "\n",
        "model.compile(loss=\"mse\",optimizer = opt)\n",
        "model.fit(training_tot_shuffle, training_tot_shuffle, epochs=50, validation_split=0.1, batch_size=64, shuffle=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1413/1413 [==============================] - 76s 27ms/step - loss: 0.0945 - val_loss: 0.0083\n",
            "Epoch 2/50\n",
            "1413/1413 [==============================] - 36s 25ms/step - loss: 0.0067 - val_loss: 0.0049\n",
            "Epoch 3/50\n",
            "1413/1413 [==============================] - 36s 25ms/step - loss: 0.0048 - val_loss: 0.0045\n",
            "Epoch 4/50\n",
            "1413/1413 [==============================] - 35s 25ms/step - loss: 0.0045 - val_loss: 0.0040\n",
            "Epoch 5/50\n",
            "1413/1413 [==============================] - 35s 25ms/step - loss: 0.0039 - val_loss: 0.0036\n",
            "Epoch 6/50\n",
            "1413/1413 [==============================] - 36s 25ms/step - loss: 0.0035 - val_loss: 0.0034\n",
            "Epoch 7/50\n",
            "1413/1413 [==============================] - 36s 25ms/step - loss: 0.0034 - val_loss: 0.0034\n",
            "Epoch 8/50\n",
            "1413/1413 [==============================] - 36s 25ms/step - loss: 0.0034 - val_loss: 0.0033\n",
            "Epoch 9/50\n",
            "1413/1413 [==============================] - 36s 25ms/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 10/50\n",
            "1413/1413 [==============================] - 36s 25ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 11/50\n",
            "1413/1413 [==============================] - 35s 25ms/step - loss: 0.0031 - val_loss: 0.0031\n",
            "Epoch 12/50\n",
            "1413/1413 [==============================] - 35s 24ms/step - loss: 0.0031 - val_loss: 0.0031\n",
            "Epoch 13/50\n",
            "1413/1413 [==============================] - 35s 25ms/step - loss: 0.0031 - val_loss: 0.0031\n",
            "Epoch 14/50\n",
            "1413/1413 [==============================] - 35s 25ms/step - loss: 0.0031 - val_loss: 0.0031\n",
            "Epoch 15/50\n",
            "1413/1413 [==============================] - 35s 25ms/step - loss: 0.0031 - val_loss: 0.0030\n",
            "Epoch 16/50\n",
            "1413/1413 [==============================] - 35s 25ms/step - loss: 0.0030 - val_loss: 0.0030\n",
            "Epoch 17/50\n",
            "1413/1413 [==============================] - 36s 26ms/step - loss: 0.0030 - val_loss: 0.0030\n",
            "Epoch 18/50\n",
            "1413/1413 [==============================] - 36s 26ms/step - loss: 0.0030 - val_loss: 0.0030\n",
            "Epoch 19/50\n",
            "1413/1413 [==============================] - 36s 26ms/step - loss: 0.0029 - val_loss: 0.0029\n",
            "Epoch 20/50\n",
            "1413/1413 [==============================] - 36s 26ms/step - loss: 0.0029 - val_loss: 0.0029\n",
            "Epoch 21/50\n",
            "1413/1413 [==============================] - 36s 26ms/step - loss: 0.0029 - val_loss: 0.0029\n",
            "Epoch 22/50\n",
            "1413/1413 [==============================] - 36s 25ms/step - loss: 0.0029 - val_loss: 0.0029\n",
            "Epoch 23/50\n",
            "1413/1413 [==============================] - 35s 25ms/step - loss: 0.0029 - val_loss: 0.0029\n",
            "Epoch 24/50\n",
            "1413/1413 [==============================] - 35s 25ms/step - loss: 0.0028 - val_loss: 0.0028\n",
            "Epoch 25/50\n",
            "1413/1413 [==============================] - 35s 25ms/step - loss: 0.0028 - val_loss: 0.0028\n",
            "Epoch 26/50\n",
            "1413/1413 [==============================] - 35s 25ms/step - loss: 0.0028 - val_loss: 0.0028\n",
            "Epoch 27/50\n",
            "1413/1413 [==============================] - 34s 24ms/step - loss: 0.0028 - val_loss: 0.0028\n",
            "Epoch 28/50\n",
            "1413/1413 [==============================] - 34s 24ms/step - loss: 0.0028 - val_loss: 0.0028\n",
            "Epoch 29/50\n",
            "1413/1413 [==============================] - 35s 24ms/step - loss: 0.0028 - val_loss: 0.0028\n",
            "Epoch 30/50\n",
            "1413/1413 [==============================] - 35s 25ms/step - loss: 0.0028 - val_loss: 0.0028\n",
            "Epoch 31/50\n",
            "1413/1413 [==============================] - 35s 24ms/step - loss: 0.0028 - val_loss: 0.0028\n",
            "Epoch 32/50\n",
            "1413/1413 [==============================] - 35s 24ms/step - loss: 0.0028 - val_loss: 0.0028\n",
            "Epoch 33/50\n",
            "1413/1413 [==============================] - 35s 24ms/step - loss: 0.0027 - val_loss: 0.0027\n",
            "Epoch 34/50\n",
            "1413/1413 [==============================] - 35s 25ms/step - loss: 0.0027 - val_loss: 0.0027\n",
            "Epoch 35/50\n",
            "1413/1413 [==============================] - 35s 25ms/step - loss: 0.0027 - val_loss: 0.0027\n",
            "Epoch 36/50\n",
            "1413/1413 [==============================] - 35s 25ms/step - loss: 0.0027 - val_loss: 0.0027\n",
            "Epoch 37/50\n",
            "1413/1413 [==============================] - 36s 25ms/step - loss: 0.0027 - val_loss: 0.0027\n",
            "Epoch 38/50\n",
            "1413/1413 [==============================] - 36s 25ms/step - loss: 0.0027 - val_loss: 0.0027\n",
            "Epoch 39/50\n",
            "1413/1413 [==============================] - 36s 26ms/step - loss: 0.0027 - val_loss: 0.0027\n",
            "Epoch 40/50\n",
            "1413/1413 [==============================] - 36s 25ms/step - loss: 0.0027 - val_loss: 0.0027\n",
            "Epoch 41/50\n",
            "1413/1413 [==============================] - 36s 26ms/step - loss: 0.0026 - val_loss: 0.0027\n",
            "Epoch 42/50\n",
            "1413/1413 [==============================] - 36s 25ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 43/50\n",
            "1413/1413 [==============================] - 35s 25ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 44/50\n",
            "1413/1413 [==============================] - 35s 25ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 45/50\n",
            "1413/1413 [==============================] - 35s 25ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 46/50\n",
            "1413/1413 [==============================] - 35s 25ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 47/50\n",
            "1413/1413 [==============================] - 35s 25ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 48/50\n",
            "1413/1413 [==============================] - 35s 25ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 49/50\n",
            "1413/1413 [==============================] - 35s 25ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 50/50\n",
            "1413/1413 [==============================] - 35s 25ms/step - loss: 0.0026 - val_loss: 0.0026\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f51f031c3d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZ8P46b6tq11"
      },
      "source": [
        "model.save('/content/drive/MyDrive/models/LSTM-4/model_pump.h5')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y42D3ejetbrN"
      },
      "source": [
        "import csv\n",
        "\n",
        "def save_csv(save_file_path,\n",
        "             save_data):\n",
        "    with open(save_file_path, \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f, lineterminator='\\n')\n",
        "        writer.writerows(save_data)\n",
        "\n",
        "\n",
        "# load dataset\n",
        "def select_dirs(path):\n",
        "    dir_path = os.path.abspath(path)\n",
        "    dirs = sorted(glob.glob(dir_path))\n",
        "    return dirs\n",
        "\n",
        "def file_load(wav_name, mono=False):\n",
        "    try:\n",
        "        return librosa.load(wav_name, sr=None, mono=mono)\n",
        "    except:\n",
        "        logger.error(\"file_broken or not exists!! : {}\".format(wav_name))\n",
        "\n",
        "def file_list_generator(target_dir, dir_name=\"train\", ext=\"wav\"):\n",
        "    print(\"target_dir : {}\".format(target_dir))\n",
        "\n",
        "    # generate training list\n",
        "    training_list_path = os.path.abspath(\"{dir}/{dir_name}/*.{ext}\".format(dir=target_dir, dir_name=dir_name, ext=ext))\n",
        "    files = sorted(glob.glob(training_list_path))\n",
        "    if len(files) == 0:\n",
        "      print(\"errore\")\n",
        "    return files\n",
        "\n",
        "\n",
        "def file_to_vector_array(file_name, n_mels=64, n_fft=1024, hop_length=512, power=2.0):\n",
        "    # 02 generate melspectrogram using librosa\n",
        "    y, sr = file_load(file_name)\n",
        "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, power=power)\n",
        "\n",
        "    # 03 convert melspectrogram to log mel energy\n",
        "    log_mel_spectrogram = 20.0 / power * numpy.log10(mel_spectrogram + sys.float_info.epsilon)\n",
        "\n",
        "    return log_mel_spectrogram\n",
        "\n",
        "  \n",
        "def list_to_vector_array(file_list, msg=\"calc...\", n_mels=64, n_fft=1024, hop_length=512, power=2.0, frames=10):\n",
        "    # iterate file_to_vector_array()\n",
        "    for idx in tqdm(range(len(file_list)), desc=msg):\n",
        "        vector_array = file_to_vector_array(file_list[idx], n_mels=n_mels, n_fft=n_fft, hop_length=hop_length, power=power)\n",
        "\n",
        "        if idx == 0:\n",
        "            dataset = numpy.zeros((len(file_list), n_mels, frames), float)\n",
        "        dataset[vector_array.shape[0] * idx: vector_array.shape[0] * (idx + 1), :] = vector_array\n",
        "    return dataset\n",
        "\n",
        "def key_by_id(item):\n",
        "  path_splitted = item.split(\"/\")\n",
        "  file_name = path_splitted[ len(path_splitted) - 1 ]\n",
        "  file_name_splitted = file_name.split(\"_\")\n",
        "  machine_id = file_name_splitted = file_name_splitted[2]\n",
        "  return machine_id\n",
        "\n",
        "def get_machine_id_list_for_test(target_dir,\n",
        "                                 dir_name=\"test\",\n",
        "                                 ext=\"wav\"):\n",
        "\n",
        "    # create test files\n",
        "    dir_path = os.path.abspath(\"{dir}/{dir_name}/*.{ext}\".format(dir=target_dir, dir_name=dir_name, ext=ext))\n",
        "    file_paths = sorted(glob.glob(dir_path))\n",
        "    # extract id\n",
        "    machine_id_list = sorted(list(set(itertools.chain.from_iterable(\n",
        "        [re.findall('id_[0-9][0-9]', ext_id) for ext_id in file_paths]))))\n",
        "    return machine_id_list\n",
        "\n",
        "def test_file_list_generator(target_dir,\n",
        "                             id_name,\n",
        "                             dir_name=\"test\",\n",
        "                             prefix_normal=\"normal\",\n",
        "                             prefix_anomaly=\"anomaly\",\n",
        "                             ext=\"wav\"):\n",
        "  \n",
        "    print(\"target_dir : {}\".format(target_dir+\"_\"+id_name))\n",
        "\n",
        "    normal_files = sorted(\n",
        "    glob.glob(\"{dir}/{dir_name}/{prefix_normal}_{id_name}*.{ext}\".format(dir=target_dir,\n",
        "                                                                                 dir_name=dir_name,\n",
        "                                                                                 prefix_normal=prefix_normal,\n",
        "                                                                                 id_name=id_name,\n",
        "                                                                                 ext=ext)))\n",
        "    normal_labels = numpy.zeros(len(normal_files))\n",
        "    anomaly_files = sorted(\n",
        "    glob.glob(\"{dir}/{dir_name}/{prefix_anomaly}_{id_name}*.{ext}\".format(dir=target_dir,\n",
        "                                                                                  dir_name=dir_name,\n",
        "                                                                                  prefix_anomaly=prefix_anomaly,\n",
        "                                                                                  id_name=id_name,\n",
        "                                                                                  ext=ext)))\n",
        "    anomaly_labels = numpy.ones(len(anomaly_files))\n",
        "    files = numpy.concatenate((normal_files, anomaly_files), axis=0)\n",
        "    labels = numpy.concatenate((normal_labels, anomaly_labels), axis=0)\n",
        "    print(\"test_file  num : {num}\".format(num=len(files)))\n",
        "    if len(files) == 0:\n",
        "        print(\"no_wav_file!!\")\n",
        "    print(\"\\n========================================\")\n",
        "\n",
        "    return files, labels"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQLl5W5n7_T5",
        "outputId": "65c565fc-84ea-4f5c-beff-2cf168deca9a"
      },
      "source": [
        "target_dir = \"/content/drive/MyDrive/pump\"\n",
        "\n",
        "machine_type = os.path.split(target_dir)[1]\n",
        "print(\"============== MODEL LOAD ==============\")\n",
        "# set model path\n",
        "model_file = \"/content/drive/MyDrive/models/LSTM-4/model_pump.h5\"\n",
        "\n",
        "# load model file\n",
        "if not os.path.exists(model_file):\n",
        "  print(\"{} model not found \".format(machine_type))\n",
        "  sys.exit(-1)\n",
        "model = keras.models.load_model(model_file)\n",
        "# model.summary()\n",
        "\n",
        "machine_id_list = get_machine_id_list_for_test(target_dir)\n",
        "\n",
        "# initialize lines in csv for AUC and pAUC\n",
        "csv_lines = []\n",
        "\n",
        "csv_lines.append([machine_type])\n",
        "csv_lines.append([\"id\", \"AUC\", \"pAUC\"])\n",
        "performance = []\n",
        "\n",
        "for id_str in machine_id_list:\n",
        "  # load test file\n",
        "\n",
        "  id_num = id_str.split(\"_\")[1]\n",
        "\n",
        "  # Definizione della label \"match\" da utilizzare in fase di testing e del min e max da utilizzare per la normalizzazione\n",
        "  # i min e max sono stati calcolati a partire dai dati di training.\n",
        "  if id_num == \"00\":\n",
        "    max = max_00\n",
        "    min = min_00\n",
        "  if id_num == \"02\":\n",
        "    max = max_02\n",
        "    min = min_02\n",
        "  if id_num == \"04\":\n",
        "    max = max_04\n",
        "    min = min_04\n",
        "  if id_num == \"06\":\n",
        "    max = max_06\n",
        "    min = min_06\n",
        "\n",
        "  test_files, y_true = test_file_list_generator(target_dir, id_str)\n",
        "\n",
        "  # setup anomaly score file path\n",
        "  anomaly_score_csv = \"/content/drive/MyDrive/models/LSTM-4/anomaly_score_{machine_type}_{id_str}.csv\".format(machine_type=machine_type, id_str=id_str)\n",
        "  anomaly_score_list = []\n",
        "\n",
        "  print(\"\\n============== BEGIN TEST FOR A MACHINE ID {id} ==============\".format(id=id_num))\n",
        "\n",
        "  y_pred = [0. for k in test_files]\n",
        "\n",
        "\n",
        "  for file_idx, file_path in tqdm(enumerate(test_files), total=len(test_files)):\n",
        "\n",
        "    # Estrazione spettrogramma audio test\n",
        "    data = file_to_vector_array(file_path, n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH, power=POWER)\n",
        "\n",
        "    # Normalizzazione spettrogramma di test\n",
        "    data = ( data - min ) / (max-min)\n",
        "\n",
        "    # Estrazione delle frame 128x10\n",
        "    data_splitted = numpy.zeros((30, 128, 10))\n",
        "    index = 0\n",
        "    i = 0\n",
        "    while (i+5) < 303:\n",
        "      vector_i = numpy.zeros((128,10))\n",
        "      for j in range(0,128):\n",
        "        vector_i[j] = data[j][i:i+10]\n",
        "      data_splitted[index] = vector_i\n",
        "      index += 1\n",
        "      i = i+10\n",
        "\n",
        "    elem_error = []\n",
        "    for elem in data_splitted:\n",
        "      predicted = model.predict(elem.reshape(1,10,128))\n",
        "      errors = numpy.mean( numpy.square( elem.T - predicted.reshape((1,10,128)) ) , axis=1)\n",
        "      elem_error.append(numpy.mean(errors))\n",
        "\n",
        "    # Log dell'errore associato all'istanza di test\n",
        "    y_pred[file_idx] = numpy.mean(elem_error)\n",
        "    anomaly_score_list.append([os.path.basename(file_path), y_pred[file_idx]])\n",
        "  \n",
        "  save_csv(save_file_path=anomaly_score_csv, save_data=anomaly_score_list)\n",
        "    \n",
        "  # Calcolo AUC e pAUC per i dati con un certo ID_0x\n",
        "  auc = metrics.roc_auc_score(y_true,y_pred)\n",
        "  p_auc = metrics.roc_auc_score(y_true, y_pred, max_fpr=0.1)\n",
        "  csv_lines.append([id_str.split(\"_\", 1)[1], auc, p_auc])\n",
        "  performance.append([auc, p_auc])\n",
        "  print(\"AUC : {}\".format(auc))\n",
        "  print(\"pAUC : {}\".format(p_auc))\n",
        "\n",
        "  print(\"\\n============ END OF TEST FOR A MACHINE ID ============\")\n",
        "\n",
        "# Stampa di AUC e pAUC medi su tutti i dati di test (media di AUC e pAUC sui vari ID).\n",
        "print(\"\\n============ AVERAGE PERFORMANCES ============\")\n",
        "averaged_performance = numpy.mean(numpy.array(performance, dtype=float), axis=0)\n",
        "csv_lines.append([\"Average\"] + list(averaged_performance))\n",
        "csv_lines.append([])\n",
        "print(averaged_performance)\n",
        "\n",
        "result_path = \"/content/drive/MyDrive/models/LSTM-4/anomaly_score_avg.csv\"\n",
        "save_csv(save_file_path=result_path, save_data=csv_lines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============== MODEL LOAD ==============\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/243 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "target_dir : /content/drive/MyDrive/pump_id_00\n",
            "test_file  num : 243\n",
            "\n",
            "========================================\n",
            "\n",
            "============== BEGIN TEST FOR A MACHINE ID 00 ==============\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 59%|█████▉    | 144/243 [05:17<03:37,  2.20s/it]"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}